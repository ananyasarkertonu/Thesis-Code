{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.035789,
     "end_time": "2020-10-20T18:14:20.657495",
     "exception": false,
     "start_time": "2020-10-20T18:14:20.621706",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2020-10-20T18:14:20.762540Z",
     "iopub.status.busy": "2020-10-20T18:14:20.746764Z",
     "iopub.status.idle": "2020-10-20T18:19:10.472390Z",
     "shell.execute_reply": "2020-10-20T18:19:10.473064Z"
    },
    "papermill": {
     "duration": 289.780179,
     "end_time": "2020-10-20T18:19:10.473289",
     "exception": false,
     "start_time": "2020-10-20T18:14:20.693110",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def words(text): return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "WORDS = Counter(words(open('../input/twitterdataset2/big.txt').read()))\n",
    "\n",
    "def P(word, N=sum(WORDS.values())): \n",
    "    \"Probability of `word`.\"\n",
    "    return WORDS[word] / N\n",
    "\n",
    "def correction(word): \n",
    "    \"Most probable spelling correction for word.\"\n",
    "    return max(candidates(word), key=P)\n",
    "\n",
    "def candidates(word): \n",
    "    \"Generate possible spelling corrections for word.\"\n",
    "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "\n",
    "def known(words): \n",
    "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "    return set(w for w in words if w in WORDS)\n",
    "\n",
    "def edits1(word):\n",
    "    \"All edits that are one edit away from `word`.\"\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def edits2(word): \n",
    "    \"All edits that are two edits away from `word`.\"\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Naive Bayes\n",
    "from nltk.corpus import words as wsss\n",
    "import pandas as pd \n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from nltk.corpus import stopwords\n",
    "import nltk.classify.util\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from nltk.classify import SklearnClassifier\n",
    "from nltk.corpus import names\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import *\n",
    "\n",
    "wordss=[]\n",
    "for i in wsss.words():\n",
    "    wordss.append(i)\n",
    "\n",
    "start = time.time()\n",
    "num=0\n",
    "df = read_csv(\"../input/twitterdataset/traindata.csv\")\n",
    "df1 = df['tag']\n",
    "df2 = df['content']\n",
    "tok = WordPunctTokenizer()\n",
    "pat1 = r'@[A-Za-z0-9]+'\n",
    "pat2 = r'https?://[A-Za-z0-9./]+'\n",
    "combined_pat = r'|'.join((pat1, pat2))\n",
    "\n",
    "f= open(\"processed_dataset.csv\",\"w\")\n",
    "\n",
    "def clean(s):\n",
    "    def tweet_cleaner(text):\n",
    "        soup = BeautifulSoup(text, 'lxml')\n",
    "        souped = soup.get_text()\n",
    "        stripped = re.sub(combined_pat, '', souped)\n",
    "        try:\n",
    "            clean = stripped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
    "        except:\n",
    "            clean = stripped\n",
    "        letters_only = re.sub(\"[^a-zA-Z]\", \" \", clean)\n",
    "        lower_case = letters_only.lower()\n",
    "        words = tok.tokenize(lower_case)\n",
    "        return (\" \".join(words)).strip()\n",
    "    \n",
    "    stopWords = set(stopwords.words('english'))\n",
    "    ps = PorterStemmer()\n",
    "    li=[]\n",
    "    test_result = []\n",
    "    wordsFiltered = []\n",
    "    b=tweet_cleaner(s)\n",
    "    test_result.append(b)\n",
    "    words = word_tokenize(tweet_cleaner(s))\n",
    "    #print(words)\n",
    "    \n",
    "    wordsFiltered=[]\n",
    "    \n",
    "    for w in words:\n",
    "        if w not in stopWords:\n",
    "            wordsFiltered.append(w)\n",
    "\n",
    "    for word in wordsFiltered:\n",
    "        l=ps.stem(word)\n",
    "        li.append(l)\n",
    "    \n",
    "    tmp=[]\n",
    "    for i in li:\n",
    "        if i not in tmp:\n",
    "            if i!='b':\n",
    "                if len(i)==1:\n",
    "                    continue\n",
    "                elif len(i)<3:\n",
    "                    if i in wordss:\n",
    "                        tmp.append(correction(i))\n",
    "                else:\n",
    "                    tmp.append(correction(i))\n",
    "    \n",
    "    f.write(',')\n",
    "    for i in tmp:\n",
    "        f.write(i+' ')\n",
    "    f.write('\\n')\n",
    "    \n",
    "    #print(li)\n",
    "    return tmp\n",
    "    \n",
    "    \n",
    "    #return words\n",
    "\n",
    "\n",
    "p=[]\n",
    "s=[]\n",
    "n=[]\n",
    "c=[]\n",
    "counterz = 0\n",
    "for t in range(0,len(df2)):\n",
    "    if counterz%100==0:\n",
    "        print(counterz)\n",
    "    counterz=counterz+1\n",
    "    if df1[t]=='politics':\n",
    "        sen=df2[t]\n",
    "        f.write('0')\n",
    "        p=p+clean(sen)\n",
    "    elif df1[t]=='sports':\n",
    "        sen=df2[t]\n",
    "        f.write('1')\n",
    "        s=s+clean(sen)\n",
    "    elif df1[t]=='natural':\n",
    "        sen=df2[t]\n",
    "        f.write('2')\n",
    "        n=n+clean(sen)\n",
    "    elif df1[t]=='crime':\n",
    "        sen=df2[t]\n",
    "        f.write('3')\n",
    "        c=c+clean(sen)\n",
    "        \n",
    "f.close()\n",
    "        \n",
    "#print(p)\n",
    "#print(s)\n",
    "#print(n)\n",
    "#print(c)\n",
    "\n",
    "normal_vocab=[]\n",
    "def word_feats(words):\n",
    "    return dict([(words, True)]) \n",
    "\n",
    "\n",
    "for a in p:\n",
    "    if a in p and a in s and a in n and a in c:\n",
    "        normal_vocab=normal_vocab+list(a);\n",
    "        p.remove(a)\n",
    "        s.remove(a)\n",
    "        n.remove(a)\n",
    "        c.remove(a)\n",
    "    if a in p and a in s:\n",
    "        normal_vocab=normal_vocab+list(a);\n",
    "        p.remove(a)\n",
    "        s.remove(a)\n",
    "    if a in s and a in n:\n",
    "        normal_vocab=normal_vocab+list(a);\n",
    "        n.remove(a)\n",
    "        s.remove(a)\n",
    "    if a in n and a in c:\n",
    "        normal_vocab=normal_vocab+list(a);\n",
    "        n.remove(a)\n",
    "        c.remove(a)\n",
    "    if a in p and a in c:\n",
    "        normal_vocab=normal_vocab+list(a);\n",
    "        p.remove(a)\n",
    "        c.remove(a)\n",
    "    if a in s and a in c:\n",
    "        normal_vocab=normal_vocab+list(a);\n",
    "        c.remove(a)\n",
    "        s.remove(a)\n",
    "    if a in p and a in n:\n",
    "        normal_vocab=normal_vocab+list(a);\n",
    "        p.remove(a)\n",
    "        n.remove(a)\n",
    "    if a in p and a in s and a in n:\n",
    "        normal_vocab=normal_vocab+list(a);\n",
    "        p.remove(a)\n",
    "        s.remove(a)\n",
    "        n.remove(a)\n",
    "    if a in s and a in n and a in c:\n",
    "        normal_vocab=normal_vocab+list(a);\n",
    "        c.remove(a)\n",
    "        s.remove(a)\n",
    "        n.remove(a)\n",
    "    if a in p and a in n and a in c:\n",
    "        normal_vocab=normal_vocab+list(a);\n",
    "        p.remove(a)\n",
    "        c.remove(a)\n",
    "        n.remove(a)\n",
    "    if a in p and a in s and a in c:\n",
    "        normal_vocab=normal_vocab+list(a);\n",
    "        p.remove(a)\n",
    "        s.remove(a)\n",
    "        c.remove(a)\n",
    "\n",
    "        \n",
    "politics_vocab=p\n",
    "#print(politics_vocab)\n",
    "sports_vocab=s\n",
    "natural_vocab=n\n",
    "crime_vocab=c\n",
    " \n",
    "politics_features = [(word_feats(pol), 'pol') for pol in politics_vocab]\n",
    "sports_features = [(word_feats(spo), 'spo') for spo in sports_vocab]\n",
    "natural_features = [(word_feats(nat), 'nat') for nat in natural_vocab]\n",
    "crime_features = [(word_feats(cri), 'cri') for cri in crime_vocab]\n",
    "normal_features = [(word_feats(nor), 'nor') for nor in normal_vocab]\n",
    "train_set = politics_features + sports_features + natural_features+crime_features\n",
    "#print(train_set)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.03628,
     "end_time": "2020-10-20T18:19:10.546678",
     "exception": false,
     "start_time": "2020-10-20T18:19:10.510398",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2020-10-20T18:19:10.636846Z",
     "iopub.status.busy": "2020-10-20T18:19:10.631528Z",
     "iopub.status.idle": "2020-10-20T18:20:13.946452Z",
     "shell.execute_reply": "2020-10-20T18:20:13.947406Z"
    },
    "papermill": {
     "duration": 63.36361,
     "end_time": "2020-10-20T18:20:13.947682",
     "exception": false,
     "start_time": "2020-10-20T18:19:10.584072",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "120\n",
      "140\n",
      "160\n",
      "180\n",
      "[2, 0, 1, 3, 1, 2, 2, 2, 2, 1, 1, 1, 2, 1, 0, 2, 0, 0, 2, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 0, 0, 1, 2, 1, 0, 3, 1, 2, 1, 1, 1, 2, 1, 2, 1, 0, 1, 2, 0, 1, 1, 1, 1, 3, 0, 1, 2, 1, 2, 2, 0, 1, 3, 1, 0, 1, 0, 1, 2, 1, 0, 1, 0, 1, 2, 1, 2, 1, 0, 1, 2, 1, 2, 2, 2, 0, 0, 2, 2, 2, 2, 0, 2, 2, 1, 1, 1, 0, 1, 1, 2, 2, 2, 2, 1, 2, 2, 0, 2, 2, 3, 2, 2, 0, 0, 3, 2, 1, 3, 0, 3, 1, 2, 3, 0, 2, 0, 1, 0, 3, 2, 2, 1, 2, 1, 2, 1, 3, 0, 0, 2, 1, 0, 1, 2, 1, 1, 2, 0, 1, 3, 3, 2, 0, 0, 0, 3, 1, 2, 3, 2, 3, 1, 1, 2, 0, 2, 3, 1, 2, 3, 3, 2, 2, 1, 1, 1, 1, 2, 2, 2, 2, 1, 0, 2, 1, 1, 2, 1, 1, 1, 2, 1, 1, 2, 3, 2, 2, 1, 1]\n",
      "[2, 0, 1, 3, 1, 0, 2, 2, 0, 1, 1, 1, 2, 1, 0, 2, 2, 0, 2, 1, 1, 1, 2, 2, 2, 1, 1, 2, 2, 0, 0, 1, 2, 1, 0, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 0, 1, 2, 0, 1, 1, 1, 1, 3, 0, 1, 2, 1, 2, 2, 0, 1, 3, 1, 0, 1, 0, 1, 2, 1, 0, 1, 0, 1, 2, 1, 1, 1, 0, 1, 2, 1, 2, 2, 2, 0, 0, 2, 2, 2, 2, 0, 2, 2, 1, 1, 2, 0, 1, 1, 2, 2, 2, 2, 1, 2, 2, 0, 2, 2, 2, 2, 2, 0, 2, 2, 2, 1, 2, 0, 2, 1, 2, 2, 0, 2, 0, 1, 0, 1, 3, 2, 1, 2, 1, 2, 1, 2, 2, 0, 2, 1, 0, 1, 1, 2, 1, 2, 0, 1, 3, 0, 2, 0, 2, 0, 2, 1, 1, 2, 2, 2, 1, 1, 2, 0, 2, 0, 1, 2, 0, 0, 2, 2, 1, 1, 1, 1, 2, 2, 2, 1, 1, 0, 2, 1, 1, 2, 2, 1, 1, 2, 1, 1, 2, 2, 2, 2, 1, 1]\n",
      "Politics Count: 36\n",
      "Sports Count: 75\n",
      "Natural Count: 69\n",
      "Crime Count: 20\n",
      "\n",
      "Politics Predicted: 38\n",
      "Sports Predicted: 73\n",
      "Natural Predicted: 84\n",
      "Crime Predicted: 5\n",
      "\n",
      "Politics Correct: 32\n",
      "Sports Correct: 68\n",
      "Natural Correct: 62\n",
      "Crime Correct: 4\n",
      "\n",
      "Test Data: 200\n",
      "Accuracy: 0.83\n",
      "Run Time: 350.6646511554718\n"
     ]
    }
   ],
   "source": [
    "classifier = NaiveBayesClassifier.train(train_set)\n",
    "##classifier = SklearnClassifier(LinearSVC(random_state=0), sparse=False).train(train_set) \n",
    "#classifier = SklearnClassifier(LinearSVC(tol=1e-6, max_iter=20000, class_weight='balanced', C=100, multi_class='ovr', random_state=0, loss='hinge'), sparse=False).train(train_set) \n",
    "#classifier = SklearnClassifier(LogisticRegression(), sparse=False).train(train_set)\n",
    "#classifier = SklearnClassifier(DecisionTreeClassifier(), sparse=False).train(train_set)\n",
    "#classifier = SklearnClassifier(RandomForestClassifier(n_estimators=5, max_depth=10, random_state=0, criterion=\"entropy\"), sparse=False).train(train_set) \n",
    "#classifier = SklearnClassifier(KNeighborsClassifier(), sparse=False).train(train_set) \n",
    "\n",
    "cols = ['text']\n",
    "dt = pd.read_csv(\"../input/twitterdataset/test.csv\",header=None, names=cols)\n",
    "dtm = read_csv(\"../input/twitterdataset/testmatch2.csv\")\n",
    "dtm1 = dtm['tag']\n",
    "dtm2 = dtm['content']\n",
    "#print(classifier)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Predict\n",
    "tok = WordPunctTokenizer()\n",
    "pat1 = r'@[A-Za-z0-9]+'\n",
    "pat2 = r'https?://[A-Za-z0-9./]+'\n",
    "combined_pat = r'|'.join((pat1, pat2))\n",
    "\n",
    "\n",
    "def tweet_cleaner(text):\n",
    "    soup = BeautifulSoup(text, 'lxml')\n",
    "    souped = soup.get_text()\n",
    "    stripped = re.sub(combined_pat, '', souped)\n",
    "    try:\n",
    "        clean = stripped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
    "    except:\n",
    "        clean = stripped\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", clean)\n",
    "    lower_case = letters_only.lower()\n",
    "    words = tok.tokenize(lower_case)\n",
    "    return (\" \".join(words)).strip()\n",
    "\n",
    "\n",
    "testing = dt.text[:]\n",
    "stopWords = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "li=[]\n",
    "test_result = []\n",
    "wordsFiltered = []\n",
    "\n",
    "c=0\n",
    "n=0\n",
    "po=0\n",
    "sp=0\n",
    "cr=0\n",
    "na=0\n",
    "poli=0\n",
    "spor=0\n",
    "natu=0\n",
    "crim=0\n",
    "o=0\n",
    "\n",
    "counterz=0\n",
    "\n",
    "import numpy as np\n",
    "actual = []\n",
    "predicted = []\n",
    "\n",
    "for t in testing:\n",
    "    if counterz%20==0:\n",
    "        print(counterz)\n",
    "    counterz=counterz+1\n",
    "    pol=0\n",
    "    cri=0\n",
    "    nat=0\n",
    "    spo=0\n",
    "    pol2=0\n",
    "    cri2=0\n",
    "    nat2=0\n",
    "    spo2=0\n",
    "    #print(t)\n",
    "    \n",
    "    #print(wd)\n",
    "    b=tweet_cleaner(t)\n",
    "    #print(b)\n",
    "    wd = word_tokenize(b)\n",
    "    test_result.append(b)\n",
    "    words = word_tokenize(tweet_cleaner(t))\n",
    "    #print(words)\n",
    "    \n",
    "    for w in words:\n",
    "        if w not in stopWords:\n",
    "            wordsFiltered.append(w)\n",
    "    #print(wordsFiltered)\n",
    "    for word in wordsFiltered:\n",
    "        l=ps.stem(word)\n",
    "        li.append(l)\n",
    "    #print(li)\n",
    "    \n",
    "    tmp=[]\n",
    "    for i in li:\n",
    "        if i not in tmp:\n",
    "            if i!='b':\n",
    "                if len(i)==1:\n",
    "                    continue\n",
    "                elif len(i)<3:\n",
    "                    if i in wordss:\n",
    "                        tmp.append(correction(i))\n",
    "                else:\n",
    "                    tmp.append(correction(i))\n",
    "    \n",
    "    sentence = tmp\n",
    "    #print(sentence)\n",
    "    #sentence = sentence.lower()\n",
    "    #words = sentence.split(' ')\n",
    "    w=sentence\n",
    "    wordsFiltered=[]\n",
    "    for word in w:\n",
    "        classResult = classifier.classify( word_feats(word))\n",
    "        if classResult == 'pol':\n",
    "            pol = pol + 1\n",
    "        if classResult == 'spo':\n",
    "            spo = spo + 1\n",
    "        if classResult == 'nat':\n",
    "            nat = nat + 1\n",
    "        if classResult == 'cri':\n",
    "            cri = cri + 1\n",
    "    #print(neg)\n",
    "    #print(pos)\n",
    "    #print(li)\n",
    "    #print(len(words))\n",
    "\n",
    "    pol2=float(pol)/(len(wd))\n",
    "    spo2=float(spo)/(len(wd))\n",
    "    nat2=float(nat)/(len(wd))\n",
    "    cri2=float(cri)/(len(wd))\n",
    "\n",
    "    #print('Politics: ' + str(pol2))\n",
    "    #print('Sports: ' + str(spo2))\n",
    "    #print('Natural: ' + str(nat2))\n",
    "    #print('Crime: ' + str(cri2))\n",
    "    \n",
    "    \n",
    "    if dtm1[o]=='politics':\n",
    "        actual.append(0)\n",
    "    elif dtm1[o]=='sports':\n",
    "        actual.append(1)\n",
    "    elif dtm1[o]=='natural':\n",
    "        actual.append(2)\n",
    "    elif dtm1[o]=='crime':\n",
    "        actual.append(3)\n",
    "    \n",
    "    if pol2>=spo2 and pol2>=nat2 and pol2>=cri2:\n",
    "        po=po+1\n",
    "        predicted.append(0)\n",
    "        if dtm1[o]=='politics':\n",
    "            poli=poli+1\n",
    "            #print('politics')\n",
    "    elif spo2>=pol2 and spo2>=nat2 and spo2>=cri2:\n",
    "        sp=sp+1\n",
    "        predicted.append(1)\n",
    "        if dtm1[o]=='sports':\n",
    "            spor=spor+1\n",
    "            #print('sports')\n",
    "    elif nat2>=spo2 and nat2>=pol2 and nat2>=cri2:\n",
    "        na=na+1\n",
    "        predicted.append(2)\n",
    "        if dtm1[o]=='natural':\n",
    "            natu=natu+1\n",
    "            #print('natural')\n",
    "    elif cri2>=spo2 and cri2>=nat2 and cri2>=pol2:\n",
    "        cr=cr+1\n",
    "        predicted.append(3)\n",
    "        if dtm1[o]=='crime':\n",
    "            crim=crim+1\n",
    "            #print('crime')\n",
    "    \n",
    "    #print(li)\n",
    "    while len(li)>0:\n",
    "        li.pop()\n",
    "    \n",
    "    c=c+float(pol2+spo2+nat2+cri2)\n",
    "    n=n+1\n",
    "    o=o+1\n",
    "\n",
    "\n",
    "\n",
    "#np.save(\"actual.npy\", np.array(actual))\n",
    "#np.save(\"SVMpredicted.npy\", np.array(predicted))\n",
    "\n",
    "print(actual)\n",
    "print(predicted)\n",
    "\n",
    "politics_count=0\n",
    "sports_count=0\n",
    "natural_count=0\n",
    "crime_count=0\n",
    "for i in range(0, len(testing)):\n",
    "    if dtm1[i]=='politics':\n",
    "        politics_count=politics_count+1\n",
    "    elif dtm1[i]=='sports':\n",
    "        sports_count=sports_count+1\n",
    "    elif dtm1[i]=='natural':\n",
    "        natural_count=natural_count+1\n",
    "    elif dtm1[i]=='crime':\n",
    "        crime_count=crime_count+1\n",
    "print('Politics Count: ' + str(politics_count))\n",
    "print('Sports Count: ' + str(sports_count))\n",
    "print('Natural Count: ' + str(natural_count))\n",
    "print('Crime Count: ' + str(crime_count))\n",
    "print()\n",
    "\n",
    "\n",
    "\n",
    "ac=poli+spor+crim+natu\n",
    "print('Politics Predicted: ' + str(po))\n",
    "print('Sports Predicted: ' + str(sp))\n",
    "print('Natural Predicted: ' + str(na))\n",
    "print('Crime Predicted: ' + str(cr))\n",
    "\n",
    "print()\n",
    "print('Politics Correct: ' + str(poli))\n",
    "print('Sports Correct: ' + str(spor))\n",
    "print('Natural Correct: ' + str(natu))\n",
    "print('Crime Correct: ' + str(crim))\n",
    "print()\n",
    "\n",
    "print('Test Data: ' + str(n))\n",
    "print('Accuracy: ' + str(ac/n))\n",
    "end = time.time()\n",
    "print('Run Time: ' +str(end - start))# -*- coding: utf-8 -*-\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.041154,
     "end_time": "2020-10-20T18:20:14.033702",
     "exception": false,
     "start_time": "2020-10-20T18:20:13.992548",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-20T18:20:14.136891Z",
     "iopub.status.busy": "2020-10-20T18:20:14.126345Z",
     "iopub.status.idle": "2020-10-20T18:21:24.916082Z",
     "shell.execute_reply": "2020-10-20T18:21:24.917034Z"
    },
    "papermill": {
     "duration": 70.842065,
     "end_time": "2020-10-20T18:21:24.917290",
     "exception": false,
     "start_time": "2020-10-20T18:20:14.075225",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "120\n",
      "140\n",
      "160\n",
      "180\n",
      "[2, 0, 1, 3, 1, 2, 2, 2, 2, 1, 0, 0, 3, 2, 3, 2, 1, 0, 2, 0, 1, 0, 2, 0, 2, 0, 2, 0, 1, 0, 1, 2, 1, 2, 1, 2, 1, 0, 1, 2, 1, 2, 1, 2, 1, 2, 0, 2, 0, 2, 1, 2, 1, 2, 1, 2, 1, 0, 2, 1, 2, 1, 2, 1, 2, 0, 2, 0, 3, 1, 2, 3, 0, 1, 2, 0, 2, 1, 2, 1, 2, 1, 0, 1, 0, 2, 0, 1, 0, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 0, 1, 0, 1, 0, 3, 0, 1, 0, 1, 0, 1, 3, 1, 2, 1, 2, 1, 0, 1, 2, 1, 0, 1, 2, 1, 0, 1, 2, 1, 2, 1, 2, 1, 0, 1, 0, 1, 2, 1, 0, 1, 0, 1, 2, 1, 2, 1, 0, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 0, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 0, 1, 0, 2, 1, 2, 1, 2, 1, 2, 1, 0, 1, 2, 1, 2, 0, 2, 0]\n",
      "[2, 0, 1, 3, 1, 0, 2, 2, 0, 1, 0, 0, 3, 2, 3, 2, 1, 0, 2, 0, 1, 0, 2, 0, 2, 0, 2, 0, 1, 0, 1, 2, 1, 2, 1, 2, 1, 0, 1, 2, 1, 2, 1, 2, 1, 2, 0, 1, 0, 2, 1, 2, 1, 2, 1, 2, 1, 0, 2, 1, 2, 1, 2, 1, 2, 0, 2, 0, 3, 1, 2, 3, 0, 1, 2, 0, 2, 1, 2, 1, 1, 1, 0, 1, 0, 2, 0, 1, 0, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 0, 1, 0, 3, 0, 1, 0, 1, 0, 1, 3, 1, 2, 1, 2, 1, 0, 1, 2, 1, 0, 1, 2, 1, 0, 1, 2, 1, 2, 1, 2, 1, 0, 1, 0, 1, 2, 1, 0, 1, 0, 1, 2, 1, 1, 1, 0, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 0, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 0, 1, 0, 2, 1, 2, 1, 2, 1, 2, 1, 0, 1, 2, 1, 2, 0, 2, 0]\n",
      "Politics Count: 42\n",
      "Sports Count: 80\n",
      "Natural Count: 71\n",
      "Crime Count: 7\n",
      "\n",
      "Politics Predicted: 43\n",
      "Sports Predicted: 83\n",
      "Natural Predicted: 67\n",
      "Crime Predicted: 7\n",
      "\n",
      "Politics Correct: 41\n",
      "Sports Correct: 80\n",
      "Natural Correct: 66\n",
      "Crime Correct: 7\n",
      "\n",
      "Test Data: 200\n",
      "Accuracy: 0.97\n",
      "Run Time: 421.6292841434479\n"
     ]
    }
   ],
   "source": [
    "#classifier = NaiveBayesClassifier.train(train_set)\n",
    "##classifier = SklearnClassifier(LinearSVC(random_state=0), sparse=False).train(train_set) \n",
    "classifier = SklearnClassifier(LinearSVC(tol=1e-6, max_iter=20000, class_weight='balanced', C=100, multi_class='ovr', random_state=0, loss='hinge'), sparse=False).train(train_set) \n",
    "#classifier = SklearnClassifier(LogisticRegression(), sparse=False).train(train_set)\n",
    "#classifier = SklearnClassifier(DecisionTreeClassifier(), sparse=False).train(train_set)\n",
    "#classifier = SklearnClassifier(RandomForestClassifier(n_estimators=5, max_depth=10, random_state=0, criterion=\"entropy\"), sparse=False).train(train_set) \n",
    "#classifier = SklearnClassifier(KNeighborsClassifier(), sparse=False).train(train_set) \n",
    "\n",
    "cols = ['text']\n",
    "dt = pd.read_csv(\"../input/twitterdataset/testv2.csv\",header=None, names=cols)\n",
    "dtm = read_csv(\"../input/twitterdataset/testmatch2v2.csv\")\n",
    "dtm1 = dtm['tag']\n",
    "dtm2 = dtm['content']\n",
    "#print(classifier)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Predict\n",
    "tok = WordPunctTokenizer()\n",
    "pat1 = r'@[A-Za-z0-9]+'\n",
    "pat2 = r'https?://[A-Za-z0-9./]+'\n",
    "combined_pat = r'|'.join((pat1, pat2))\n",
    "\n",
    "\n",
    "def tweet_cleaner(text):\n",
    "    soup = BeautifulSoup(text, 'lxml')\n",
    "    souped = soup.get_text()\n",
    "    stripped = re.sub(combined_pat, '', souped)\n",
    "    try:\n",
    "        clean = stripped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
    "    except:\n",
    "        clean = stripped\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", clean)\n",
    "    lower_case = letters_only.lower()\n",
    "    words = tok.tokenize(lower_case)\n",
    "    return (\" \".join(words)).strip()\n",
    "\n",
    "\n",
    "testing = dt.text[:]\n",
    "stopWords = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "li=[]\n",
    "test_result = []\n",
    "wordsFiltered = []\n",
    "\n",
    "c=0\n",
    "n=0\n",
    "po=0\n",
    "sp=0\n",
    "cr=0\n",
    "na=0\n",
    "poli=0\n",
    "spor=0\n",
    "natu=0\n",
    "crim=0\n",
    "o=0\n",
    "\n",
    "counterz=0\n",
    "\n",
    "import numpy as np\n",
    "actual = []\n",
    "predicted = []\n",
    "\n",
    "for t in testing:\n",
    "    if counterz%20==0:\n",
    "        print(counterz)\n",
    "    counterz=counterz+1\n",
    "    pol=0\n",
    "    cri=0\n",
    "    nat=0\n",
    "    spo=0\n",
    "    pol2=0\n",
    "    cri2=0\n",
    "    nat2=0\n",
    "    spo2=0\n",
    "    #print(t)\n",
    "    \n",
    "    #print(wd)\n",
    "    b=tweet_cleaner(t)\n",
    "    #print(b)\n",
    "    wd = word_tokenize(b)\n",
    "    test_result.append(b)\n",
    "    words = word_tokenize(tweet_cleaner(t))\n",
    "    #print(words)\n",
    "    \n",
    "    for w in words:\n",
    "        if w not in stopWords:\n",
    "            wordsFiltered.append(w)\n",
    "    #print(wordsFiltered)\n",
    "    for word in wordsFiltered:\n",
    "        l=ps.stem(word)\n",
    "        li.append(l)\n",
    "    #print(li)\n",
    "    \n",
    "    tmp=[]\n",
    "    for i in li:\n",
    "        if i not in tmp:\n",
    "            if i!='b':\n",
    "                if len(i)==1:\n",
    "                    continue\n",
    "                elif len(i)<3:\n",
    "                    if i in wordss:\n",
    "                        tmp.append(correction(i))\n",
    "                else:\n",
    "                    tmp.append(correction(i))\n",
    "    \n",
    "    sentence = tmp\n",
    "    #print(sentence)\n",
    "    #sentence = sentence.lower()\n",
    "    #words = sentence.split(' ')\n",
    "    w=sentence\n",
    "    wordsFiltered=[]\n",
    "    for word in w:\n",
    "        classResult = classifier.classify( word_feats(word))\n",
    "        if classResult == 'pol':\n",
    "            pol = pol + 1\n",
    "        if classResult == 'spo':\n",
    "            spo = spo + 1\n",
    "        if classResult == 'nat':\n",
    "            nat = nat + 1\n",
    "        if classResult == 'cri':\n",
    "            cri = cri + 1\n",
    "    #print(neg)\n",
    "    #print(pos)\n",
    "    #print(li)\n",
    "    #print(len(words))\n",
    "\n",
    "    pol2=float(pol)/(len(wd))\n",
    "    spo2=float(spo)/(len(wd))\n",
    "    nat2=float(nat)/(len(wd))\n",
    "    cri2=float(cri)/(len(wd))\n",
    "\n",
    "    #print('Politics: ' + str(pol2))\n",
    "    #print('Sports: ' + str(spo2))\n",
    "    #print('Natural: ' + str(nat2))\n",
    "    #print('Crime: ' + str(cri2))\n",
    "    \n",
    "    \n",
    "    if dtm1[o]=='politics':\n",
    "        actual.append(0)\n",
    "    elif dtm1[o]=='sports':\n",
    "        actual.append(1)\n",
    "    elif dtm1[o]=='natural':\n",
    "        actual.append(2)\n",
    "    elif dtm1[o]=='crime':\n",
    "        actual.append(3)\n",
    "    \n",
    "    if pol2>=spo2 and pol2>=nat2 and pol2>=cri2:\n",
    "        po=po+1\n",
    "        predicted.append(0)\n",
    "        if dtm1[o]=='politics':\n",
    "            poli=poli+1\n",
    "            #print('politics')\n",
    "    elif spo2>=pol2 and spo2>=nat2 and spo2>=cri2:\n",
    "        sp=sp+1\n",
    "        predicted.append(1)\n",
    "        if dtm1[o]=='sports':\n",
    "            spor=spor+1\n",
    "            #print('sports')\n",
    "    elif nat2>=spo2 and nat2>=pol2 and nat2>=cri2:\n",
    "        na=na+1\n",
    "        predicted.append(2)\n",
    "        if dtm1[o]=='natural':\n",
    "            natu=natu+1\n",
    "            #print('natural')\n",
    "    elif cri2>=spo2 and cri2>=nat2 and cri2>=pol2:\n",
    "        cr=cr+1\n",
    "        predicted.append(3)\n",
    "        if dtm1[o]=='crime':\n",
    "            crim=crim+1\n",
    "            #print('crime')\n",
    "    \n",
    "    #print(li)\n",
    "    while len(li)>0:\n",
    "        li.pop()\n",
    "    \n",
    "    c=c+float(pol2+spo2+nat2+cri2)\n",
    "    n=n+1\n",
    "    o=o+1\n",
    "\n",
    "\n",
    "\n",
    "#np.save(\"actual.npy\", np.array(actual))\n",
    "#np.save(\"SVMpredicted.npy\", np.array(predicted))\n",
    "\n",
    "print(actual)\n",
    "print(predicted)\n",
    "\n",
    "politics_count=0\n",
    "sports_count=0\n",
    "natural_count=0\n",
    "crime_count=0\n",
    "for i in range(0, len(testing)):\n",
    "    if dtm1[i]=='politics':\n",
    "        politics_count=politics_count+1\n",
    "    elif dtm1[i]=='sports':\n",
    "        sports_count=sports_count+1\n",
    "    elif dtm1[i]=='natural':\n",
    "        natural_count=natural_count+1\n",
    "    elif dtm1[i]=='crime':\n",
    "        crime_count=crime_count+1\n",
    "print('Politics Count: ' + str(politics_count))\n",
    "print('Sports Count: ' + str(sports_count))\n",
    "print('Natural Count: ' + str(natural_count))\n",
    "print('Crime Count: ' + str(crime_count))\n",
    "print()\n",
    "\n",
    "\n",
    "\n",
    "ac=poli+spor+crim+natu\n",
    "print('Politics Predicted: ' + str(po))\n",
    "print('Sports Predicted: ' + str(sp))\n",
    "print('Natural Predicted: ' + str(na))\n",
    "print('Crime Predicted: ' + str(cr))\n",
    "\n",
    "print()\n",
    "print('Politics Correct: ' + str(poli))\n",
    "print('Sports Correct: ' + str(spor))\n",
    "print('Natural Correct: ' + str(natu))\n",
    "print('Crime Correct: ' + str(crim))\n",
    "print()\n",
    "\n",
    "print('Test Data: ' + str(n))\n",
    "print('Accuracy: ' + str(ac/n))\n",
    "end = time.time()\n",
    "print('Run Time: ' +str(end - start))# -*- coding: utf-8 -*-\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.046569,
     "end_time": "2020-10-20T18:21:25.013478",
     "exception": false,
     "start_time": "2020-10-20T18:21:24.966909",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# SVM, tol=10e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-20T18:21:25.123154Z",
     "iopub.status.busy": "2020-10-20T18:21:25.117941Z",
     "iopub.status.idle": "2020-10-20T18:22:37.263374Z",
     "shell.execute_reply": "2020-10-20T18:22:37.261960Z"
    },
    "papermill": {
     "duration": 72.202138,
     "end_time": "2020-10-20T18:22:37.263620",
     "exception": false,
     "start_time": "2020-10-20T18:21:25.061482",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "120\n",
      "140\n",
      "160\n",
      "180\n",
      "[2, 0, 1, 3, 1, 2, 2, 2, 2, 1, 0, 0, 3, 2, 3, 2, 1, 0, 2, 0, 1, 0, 2, 0, 2, 0, 2, 0, 1, 0, 1, 2, 1, 2, 1, 2, 1, 0, 1, 2, 1, 2, 1, 2, 1, 2, 0, 2, 0, 2, 1, 2, 1, 2, 1, 2, 1, 0, 2, 1, 2, 1, 2, 1, 2, 0, 2, 0, 3, 1, 2, 3, 0, 1, 2, 0, 2, 1, 2, 1, 2, 1, 0, 1, 0, 2, 0, 1, 0, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 0, 1, 0, 1, 0, 3, 0, 1, 0, 1, 0, 1, 3, 1, 2, 1, 2, 1, 0, 1, 2, 1, 0, 1, 2, 1, 0, 1, 2, 1, 2, 1, 2, 1, 0, 1, 0, 1, 2, 1, 0, 1, 0, 1, 2, 1, 2, 1, 0, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 0, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 0, 1, 0, 2, 1, 2, 1, 2, 1, 2, 1, 0, 1, 2, 1, 2, 0, 2, 0]\n",
      "[2, 0, 1, 3, 1, 0, 2, 2, 0, 1, 0, 0, 3, 2, 3, 2, 1, 0, 2, 0, 1, 0, 2, 0, 2, 0, 2, 0, 1, 0, 1, 2, 1, 2, 1, 2, 1, 0, 1, 2, 1, 2, 1, 2, 1, 2, 0, 1, 0, 2, 1, 2, 1, 2, 1, 2, 1, 0, 2, 1, 2, 1, 2, 1, 2, 0, 2, 0, 3, 1, 2, 3, 0, 1, 2, 0, 2, 1, 2, 1, 1, 1, 0, 1, 0, 2, 0, 1, 0, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 0, 1, 0, 3, 0, 1, 0, 1, 0, 1, 3, 1, 2, 1, 2, 1, 0, 1, 2, 1, 0, 1, 2, 1, 0, 1, 2, 1, 2, 1, 2, 1, 0, 1, 0, 1, 2, 1, 0, 1, 0, 1, 2, 1, 2, 1, 0, 1, 2, 2, 2, 1, 2, 1, 2, 1, 2, 1, 0, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 0, 1, 0, 2, 1, 2, 1, 2, 1, 2, 1, 0, 1, 2, 1, 2, 0, 2, 0]\n",
      "Politics Count: 42\n",
      "Sports Count: 80\n",
      "Natural Count: 71\n",
      "Crime Count: 7\n",
      "\n",
      "Politics Predicted: 43\n",
      "Sports Predicted: 81\n",
      "Natural Predicted: 69\n",
      "Crime Predicted: 7\n",
      "\n",
      "Politics Correct: 41\n",
      "Sports Correct: 79\n",
      "Natural Correct: 67\n",
      "Crime Correct: 7\n",
      "\n",
      "Test Data: 200\n",
      "Accuracy: 0.97\n",
      "Run Time: 494.02073907852173\n"
     ]
    }
   ],
   "source": [
    "#classifier = NaiveBayesClassifier.train(train_set)\n",
    "##classifier = SklearnClassifier(LinearSVC(random_state=0), sparse=False).train(train_set) \n",
    "classifier = SklearnClassifier(LinearSVC(tol=1e-5, max_iter=20000, class_weight='balanced', C=100, multi_class='ovr', random_state=0, loss='hinge'), sparse=False).train(train_set) \n",
    "#classifier = SklearnClassifier(LogisticRegression(), sparse=False).train(train_set)\n",
    "#classifier = SklearnClassifier(DecisionTreeClassifier(), sparse=False).train(train_set)\n",
    "#classifier = SklearnClassifier(RandomForestClassifier(n_estimators=5, max_depth=10, random_state=0, criterion=\"entropy\"), sparse=False).train(train_set) \n",
    "#classifier = SklearnClassifier(KNeighborsClassifier(), sparse=False).train(train_set) \n",
    "\n",
    "cols = ['text']\n",
    "dt = pd.read_csv(\"../input/twitterdataset/testv2.csv\",header=None, names=cols)\n",
    "dtm = read_csv(\"../input/twitterdataset/testmatch2v2.csv\")\n",
    "dtm1 = dtm['tag']\n",
    "dtm2 = dtm['content']\n",
    "#print(classifier)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Predict\n",
    "tok = WordPunctTokenizer()\n",
    "pat1 = r'@[A-Za-z0-9]+'\n",
    "pat2 = r'https?://[A-Za-z0-9./]+'\n",
    "combined_pat = r'|'.join((pat1, pat2))\n",
    "\n",
    "\n",
    "def tweet_cleaner(text):\n",
    "    soup = BeautifulSoup(text, 'lxml')\n",
    "    souped = soup.get_text()\n",
    "    stripped = re.sub(combined_pat, '', souped)\n",
    "    try:\n",
    "        clean = stripped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
    "    except:\n",
    "        clean = stripped\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", clean)\n",
    "    lower_case = letters_only.lower()\n",
    "    words = tok.tokenize(lower_case)\n",
    "    return (\" \".join(words)).strip()\n",
    "\n",
    "\n",
    "testing = dt.text[:]\n",
    "stopWords = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "li=[]\n",
    "test_result = []\n",
    "wordsFiltered = []\n",
    "\n",
    "c=0\n",
    "n=0\n",
    "po=0\n",
    "sp=0\n",
    "cr=0\n",
    "na=0\n",
    "poli=0\n",
    "spor=0\n",
    "natu=0\n",
    "crim=0\n",
    "o=0\n",
    "\n",
    "counterz=0\n",
    "\n",
    "import numpy as np\n",
    "actual = []\n",
    "predicted = []\n",
    "\n",
    "for t in testing:\n",
    "    if counterz%20==0:\n",
    "        print(counterz)\n",
    "    counterz=counterz+1\n",
    "    pol=0\n",
    "    cri=0\n",
    "    nat=0\n",
    "    spo=0\n",
    "    pol2=0\n",
    "    cri2=0\n",
    "    nat2=0\n",
    "    spo2=0\n",
    "    #print(t)\n",
    "    \n",
    "    #print(wd)\n",
    "    b=tweet_cleaner(t)\n",
    "    #print(b)\n",
    "    wd = word_tokenize(b)\n",
    "    test_result.append(b)\n",
    "    words = word_tokenize(tweet_cleaner(t))\n",
    "    #print(words)\n",
    "    \n",
    "    for w in words:\n",
    "        if w not in stopWords:\n",
    "            wordsFiltered.append(w)\n",
    "    #print(wordsFiltered)\n",
    "    for word in wordsFiltered:\n",
    "        l=ps.stem(word)\n",
    "        li.append(l)\n",
    "    #print(li)\n",
    "    \n",
    "    tmp=[]\n",
    "    for i in li:\n",
    "        if i not in tmp:\n",
    "            if i!='b':\n",
    "                if len(i)==1:\n",
    "                    continue\n",
    "                elif len(i)<3:\n",
    "                    if i in wordss:\n",
    "                        tmp.append(correction(i))\n",
    "                else:\n",
    "                    tmp.append(correction(i))\n",
    "    \n",
    "    sentence = tmp\n",
    "    #print(sentence)\n",
    "    #sentence = sentence.lower()\n",
    "    #words = sentence.split(' ')\n",
    "    w=sentence\n",
    "    wordsFiltered=[]\n",
    "    for word in w:\n",
    "        classResult = classifier.classify( word_feats(word))\n",
    "        if classResult == 'pol':\n",
    "            pol = pol + 1\n",
    "        if classResult == 'spo':\n",
    "            spo = spo + 1\n",
    "        if classResult == 'nat':\n",
    "            nat = nat + 1\n",
    "        if classResult == 'cri':\n",
    "            cri = cri + 1\n",
    "    #print(neg)\n",
    "    #print(pos)\n",
    "    #print(li)\n",
    "    #print(len(words))\n",
    "\n",
    "    pol2=float(pol)/(len(wd))\n",
    "    spo2=float(spo)/(len(wd))\n",
    "    nat2=float(nat)/(len(wd))\n",
    "    cri2=float(cri)/(len(wd))\n",
    "\n",
    "    #print('Politics: ' + str(pol2))\n",
    "    #print('Sports: ' + str(spo2))\n",
    "    #print('Natural: ' + str(nat2))\n",
    "    #print('Crime: ' + str(cri2))\n",
    "    \n",
    "    \n",
    "    if dtm1[o]=='politics':\n",
    "        actual.append(0)\n",
    "    elif dtm1[o]=='sports':\n",
    "        actual.append(1)\n",
    "    elif dtm1[o]=='natural':\n",
    "        actual.append(2)\n",
    "    elif dtm1[o]=='crime':\n",
    "        actual.append(3)\n",
    "    \n",
    "    if pol2>=spo2 and pol2>=nat2 and pol2>=cri2:\n",
    "        po=po+1\n",
    "        predicted.append(0)\n",
    "        if dtm1[o]=='politics':\n",
    "            poli=poli+1\n",
    "            #print('politics')\n",
    "    elif spo2>=pol2 and spo2>=nat2 and spo2>=cri2:\n",
    "        sp=sp+1\n",
    "        predicted.append(1)\n",
    "        if dtm1[o]=='sports':\n",
    "            spor=spor+1\n",
    "            #print('sports')\n",
    "    elif nat2>=spo2 and nat2>=pol2 and nat2>=cri2:\n",
    "        na=na+1\n",
    "        predicted.append(2)\n",
    "        if dtm1[o]=='natural':\n",
    "            natu=natu+1\n",
    "            #print('natural')\n",
    "    elif cri2>=spo2 and cri2>=nat2 and cri2>=pol2:\n",
    "        cr=cr+1\n",
    "        predicted.append(3)\n",
    "        if dtm1[o]=='crime':\n",
    "            crim=crim+1\n",
    "            #print('crime')\n",
    "    \n",
    "    #print(li)\n",
    "    while len(li)>0:\n",
    "        li.pop()\n",
    "    \n",
    "    c=c+float(pol2+spo2+nat2+cri2)\n",
    "    n=n+1\n",
    "    o=o+1\n",
    "\n",
    "\n",
    "\n",
    "#np.save(\"actual.npy\", np.array(actual))\n",
    "#np.save(\"SVMpredicted.npy\", np.array(predicted))\n",
    "\n",
    "print(actual)\n",
    "print(predicted)\n",
    "\n",
    "politics_count=0\n",
    "sports_count=0\n",
    "natural_count=0\n",
    "crime_count=0\n",
    "for i in range(0, len(testing)):\n",
    "    if dtm1[i]=='politics':\n",
    "        politics_count=politics_count+1\n",
    "    elif dtm1[i]=='sports':\n",
    "        sports_count=sports_count+1\n",
    "    elif dtm1[i]=='natural':\n",
    "        natural_count=natural_count+1\n",
    "    elif dtm1[i]=='crime':\n",
    "        crime_count=crime_count+1\n",
    "print('Politics Count: ' + str(politics_count))\n",
    "print('Sports Count: ' + str(sports_count))\n",
    "print('Natural Count: ' + str(natural_count))\n",
    "print('Crime Count: ' + str(crime_count))\n",
    "print()\n",
    "\n",
    "\n",
    "\n",
    "ac=poli+spor+crim+natu\n",
    "print('Politics Predicted: ' + str(po))\n",
    "print('Sports Predicted: ' + str(sp))\n",
    "print('Natural Predicted: ' + str(na))\n",
    "print('Crime Predicted: ' + str(cr))\n",
    "\n",
    "print()\n",
    "print('Politics Correct: ' + str(poli))\n",
    "print('Sports Correct: ' + str(spor))\n",
    "print('Natural Correct: ' + str(natu))\n",
    "print('Crime Correct: ' + str(crim))\n",
    "print()\n",
    "\n",
    "print('Test Data: ' + str(n))\n",
    "print('Accuracy: ' + str(ac/n))\n",
    "end = time.time()\n",
    "print('Run Time: ' +str(end - start))# -*- coding: utf-8 -*-\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.051455,
     "end_time": "2020-10-20T18:22:37.366803",
     "exception": false,
     "start_time": "2020-10-20T18:22:37.315348",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# SVM, tol=10e-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-20T18:22:37.493204Z",
     "iopub.status.busy": "2020-10-20T18:22:37.482698Z",
     "iopub.status.idle": "2020-10-20T18:23:48.422486Z",
     "shell.execute_reply": "2020-10-20T18:23:48.423158Z"
    },
    "papermill": {
     "duration": 71.005855,
     "end_time": "2020-10-20T18:23:48.423327",
     "exception": false,
     "start_time": "2020-10-20T18:22:37.417472",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "120\n",
      "140\n",
      "160\n",
      "180\n",
      "[2, 0, 1, 3, 1, 2, 2, 2, 2, 1, 0, 0, 3, 2, 3, 2, 1, 0, 2, 0, 1, 0, 2, 0, 2, 0, 2, 0, 1, 0, 1, 2, 1, 2, 1, 2, 1, 0, 1, 2, 1, 2, 1, 2, 1, 2, 0, 2, 0, 2, 1, 2, 1, 2, 1, 2, 1, 0, 2, 1, 2, 1, 2, 1, 2, 0, 2, 0, 3, 1, 2, 3, 0, 1, 2, 0, 2, 1, 2, 1, 2, 1, 0, 1, 0, 2, 0, 1, 0, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 0, 1, 0, 1, 0, 3, 0, 1, 0, 1, 0, 1, 3, 1, 2, 1, 2, 1, 0, 1, 2, 1, 0, 1, 2, 1, 0, 1, 2, 1, 2, 1, 2, 1, 0, 1, 0, 1, 2, 1, 0, 1, 0, 1, 2, 1, 2, 1, 0, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 0, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 0, 1, 0, 2, 1, 2, 1, 2, 1, 2, 1, 0, 1, 2, 1, 2, 0, 2, 0]\n",
      "[2, 0, 1, 3, 1, 0, 2, 2, 0, 1, 0, 0, 3, 2, 3, 2, 1, 0, 2, 0, 1, 0, 2, 0, 2, 0, 2, 0, 1, 0, 1, 2, 1, 2, 1, 2, 1, 0, 1, 2, 1, 2, 1, 2, 1, 2, 0, 1, 0, 2, 1, 2, 1, 2, 1, 2, 1, 0, 2, 1, 2, 1, 2, 1, 2, 0, 2, 0, 3, 1, 2, 3, 0, 1, 2, 0, 2, 1, 2, 1, 1, 1, 0, 1, 0, 2, 0, 1, 0, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 0, 1, 0, 3, 0, 1, 0, 1, 0, 1, 3, 1, 2, 1, 2, 1, 0, 1, 2, 1, 0, 1, 2, 1, 0, 1, 2, 1, 2, 1, 2, 1, 0, 1, 0, 1, 2, 1, 0, 1, 0, 1, 2, 1, 2, 1, 0, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 0, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 0, 1, 0, 2, 1, 2, 1, 2, 1, 2, 1, 0, 1, 2, 1, 2, 0, 2, 0]\n",
      "Politics Count: 42\n",
      "Sports Count: 80\n",
      "Natural Count: 71\n",
      "Crime Count: 7\n",
      "\n",
      "Politics Predicted: 43\n",
      "Sports Predicted: 82\n",
      "Natural Predicted: 68\n",
      "Crime Predicted: 7\n",
      "\n",
      "Politics Correct: 41\n",
      "Sports Correct: 80\n",
      "Natural Correct: 67\n",
      "Crime Correct: 7\n",
      "\n",
      "Test Data: 200\n",
      "Accuracy: 0.975\n",
      "Run Time: 565.1264889240265\n"
     ]
    }
   ],
   "source": [
    "#classifier = NaiveBayesClassifier.train(train_set)\n",
    "##classifier = SklearnClassifier(LinearSVC(random_state=0), sparse=False).train(train_set) \n",
    "classifier = SklearnClassifier(LinearSVC(tol=1e-7, max_iter=20000, class_weight='balanced', C=100, multi_class='ovr', random_state=0, loss='hinge'), sparse=False).train(train_set) \n",
    "#classifier = SklearnClassifier(LogisticRegression(), sparse=False).train(train_set)\n",
    "#classifier = SklearnClassifier(DecisionTreeClassifier(), sparse=False).train(train_set)\n",
    "#classifier = SklearnClassifier(RandomForestClassifier(n_estimators=5, max_depth=10, random_state=0, criterion=\"entropy\"), sparse=False).train(train_set) \n",
    "#classifier = SklearnClassifier(KNeighborsClassifier(), sparse=False).train(train_set) \n",
    "\n",
    "cols = ['text']\n",
    "dt = pd.read_csv(\"../input/twitterdataset/testv2.csv\",header=None, names=cols)\n",
    "dtm = read_csv(\"../input/twitterdataset/testmatch2v2.csv\")\n",
    "dtm1 = dtm['tag']\n",
    "dtm2 = dtm['content']\n",
    "#print(classifier)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Predict\n",
    "tok = WordPunctTokenizer()\n",
    "pat1 = r'@[A-Za-z0-9]+'\n",
    "pat2 = r'https?://[A-Za-z0-9./]+'\n",
    "combined_pat = r'|'.join((pat1, pat2))\n",
    "\n",
    "\n",
    "def tweet_cleaner(text):\n",
    "    soup = BeautifulSoup(text, 'lxml')\n",
    "    souped = soup.get_text()\n",
    "    stripped = re.sub(combined_pat, '', souped)\n",
    "    try:\n",
    "        clean = stripped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
    "    except:\n",
    "        clean = stripped\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", clean)\n",
    "    lower_case = letters_only.lower()\n",
    "    words = tok.tokenize(lower_case)\n",
    "    return (\" \".join(words)).strip()\n",
    "\n",
    "\n",
    "testing = dt.text[:]\n",
    "stopWords = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "li=[]\n",
    "test_result = []\n",
    "wordsFiltered = []\n",
    "\n",
    "c=0\n",
    "n=0\n",
    "po=0\n",
    "sp=0\n",
    "cr=0\n",
    "na=0\n",
    "poli=0\n",
    "spor=0\n",
    "natu=0\n",
    "crim=0\n",
    "o=0\n",
    "\n",
    "counterz=0\n",
    "\n",
    "import numpy as np\n",
    "actual = []\n",
    "predicted = []\n",
    "\n",
    "for t in testing:\n",
    "    if counterz%20==0:\n",
    "        print(counterz)\n",
    "    counterz=counterz+1\n",
    "    pol=0\n",
    "    cri=0\n",
    "    nat=0\n",
    "    spo=0\n",
    "    pol2=0\n",
    "    cri2=0\n",
    "    nat2=0\n",
    "    spo2=0\n",
    "    #print(t)\n",
    "    \n",
    "    #print(wd)\n",
    "    b=tweet_cleaner(t)\n",
    "    #print(b)\n",
    "    wd = word_tokenize(b)\n",
    "    test_result.append(b)\n",
    "    words = word_tokenize(tweet_cleaner(t))\n",
    "    #print(words)\n",
    "    \n",
    "    for w in words:\n",
    "        if w not in stopWords:\n",
    "            wordsFiltered.append(w)\n",
    "    #print(wordsFiltered)\n",
    "    for word in wordsFiltered:\n",
    "        l=ps.stem(word)\n",
    "        li.append(l)\n",
    "    #print(li)\n",
    "    \n",
    "    tmp=[]\n",
    "    for i in li:\n",
    "        if i not in tmp:\n",
    "            if i!='b':\n",
    "                if len(i)==1:\n",
    "                    continue\n",
    "                elif len(i)<3:\n",
    "                    if i in wordss:\n",
    "                        tmp.append(correction(i))\n",
    "                else:\n",
    "                    tmp.append(correction(i))\n",
    "    \n",
    "    sentence = tmp\n",
    "    #print(sentence)\n",
    "    #sentence = sentence.lower()\n",
    "    #words = sentence.split(' ')\n",
    "    w=sentence\n",
    "    wordsFiltered=[]\n",
    "    for word in w:\n",
    "        classResult = classifier.classify( word_feats(word))\n",
    "        if classResult == 'pol':\n",
    "            pol = pol + 1\n",
    "        if classResult == 'spo':\n",
    "            spo = spo + 1\n",
    "        if classResult == 'nat':\n",
    "            nat = nat + 1\n",
    "        if classResult == 'cri':\n",
    "            cri = cri + 1\n",
    "    #print(neg)\n",
    "    #print(pos)\n",
    "    #print(li)\n",
    "    #print(len(words))\n",
    "\n",
    "    pol2=float(pol)/(len(wd))\n",
    "    spo2=float(spo)/(len(wd))\n",
    "    nat2=float(nat)/(len(wd))\n",
    "    cri2=float(cri)/(len(wd))\n",
    "\n",
    "    #print('Politics: ' + str(pol2))\n",
    "    #print('Sports: ' + str(spo2))\n",
    "    #print('Natural: ' + str(nat2))\n",
    "    #print('Crime: ' + str(cri2))\n",
    "    \n",
    "    \n",
    "    if dtm1[o]=='politics':\n",
    "        actual.append(0)\n",
    "    elif dtm1[o]=='sports':\n",
    "        actual.append(1)\n",
    "    elif dtm1[o]=='natural':\n",
    "        actual.append(2)\n",
    "    elif dtm1[o]=='crime':\n",
    "        actual.append(3)\n",
    "    \n",
    "    if pol2>=spo2 and pol2>=nat2 and pol2>=cri2:\n",
    "        po=po+1\n",
    "        predicted.append(0)\n",
    "        if dtm1[o]=='politics':\n",
    "            poli=poli+1\n",
    "            #print('politics')\n",
    "    elif spo2>=pol2 and spo2>=nat2 and spo2>=cri2:\n",
    "        sp=sp+1\n",
    "        predicted.append(1)\n",
    "        if dtm1[o]=='sports':\n",
    "            spor=spor+1\n",
    "            #print('sports')\n",
    "    elif nat2>=spo2 and nat2>=pol2 and nat2>=cri2:\n",
    "        na=na+1\n",
    "        predicted.append(2)\n",
    "        if dtm1[o]=='natural':\n",
    "            natu=natu+1\n",
    "            #print('natural')\n",
    "    elif cri2>=spo2 and cri2>=nat2 and cri2>=pol2:\n",
    "        cr=cr+1\n",
    "        predicted.append(3)\n",
    "        if dtm1[o]=='crime':\n",
    "            crim=crim+1\n",
    "            #print('crime')\n",
    "    \n",
    "    #print(li)\n",
    "    while len(li)>0:\n",
    "        li.pop()\n",
    "    \n",
    "    c=c+float(pol2+spo2+nat2+cri2)\n",
    "    n=n+1\n",
    "    o=o+1\n",
    "\n",
    "\n",
    "\n",
    "#np.save(\"actual.npy\", np.array(actual))\n",
    "#np.save(\"SVMpredicted.npy\", np.array(predicted))\n",
    "\n",
    "print(actual)\n",
    "print(predicted)\n",
    "\n",
    "politics_count=0\n",
    "sports_count=0\n",
    "natural_count=0\n",
    "crime_count=0\n",
    "for i in range(0, len(testing)):\n",
    "    if dtm1[i]=='politics':\n",
    "        politics_count=politics_count+1\n",
    "    elif dtm1[i]=='sports':\n",
    "        sports_count=sports_count+1\n",
    "    elif dtm1[i]=='natural':\n",
    "        natural_count=natural_count+1\n",
    "    elif dtm1[i]=='crime':\n",
    "        crime_count=crime_count+1\n",
    "print('Politics Count: ' + str(politics_count))\n",
    "print('Sports Count: ' + str(sports_count))\n",
    "print('Natural Count: ' + str(natural_count))\n",
    "print('Crime Count: ' + str(crime_count))\n",
    "print()\n",
    "\n",
    "\n",
    "\n",
    "ac=poli+spor+crim+natu\n",
    "print('Politics Predicted: ' + str(po))\n",
    "print('Sports Predicted: ' + str(sp))\n",
    "print('Natural Predicted: ' + str(na))\n",
    "print('Crime Predicted: ' + str(cr))\n",
    "\n",
    "print()\n",
    "print('Politics Correct: ' + str(poli))\n",
    "print('Sports Correct: ' + str(spor))\n",
    "print('Natural Correct: ' + str(natu))\n",
    "print('Crime Correct: ' + str(crim))\n",
    "print()\n",
    "\n",
    "print('Test Data: ' + str(n))\n",
    "print('Accuracy: ' + str(ac/n))\n",
    "end = time.time()\n",
    "print('Run Time: ' +str(end - start))# -*- coding: utf-8 -*-\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.05492,
     "end_time": "2020-10-20T18:23:48.533586",
     "exception": false,
     "start_time": "2020-10-20T18:23:48.478666",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# SVM, max_iter=10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-20T18:23:48.669929Z",
     "iopub.status.busy": "2020-10-20T18:23:48.664678Z",
     "iopub.status.idle": "2020-10-20T18:24:58.925773Z",
     "shell.execute_reply": "2020-10-20T18:24:58.926693Z"
    },
    "papermill": {
     "duration": 70.338076,
     "end_time": "2020-10-20T18:24:58.926920",
     "exception": false,
     "start_time": "2020-10-20T18:23:48.588844",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "120\n",
      "140\n",
      "160\n",
      "180\n",
      "[2, 0, 1, 3, 1, 2, 2, 2, 2, 1, 0, 0, 3, 2, 3, 2, 1, 0, 2, 0, 1, 0, 2, 0, 2, 0, 2, 0, 1, 0, 1, 2, 1, 2, 1, 2, 1, 0, 1, 2, 1, 2, 1, 2, 1, 2, 0, 2, 0, 2, 1, 2, 1, 2, 1, 2, 1, 0, 2, 1, 2, 1, 2, 1, 2, 0, 2, 0, 3, 1, 2, 3, 0, 1, 2, 0, 2, 1, 2, 1, 2, 1, 0, 1, 0, 2, 0, 1, 0, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 0, 1, 0, 1, 0, 3, 0, 1, 0, 1, 0, 1, 3, 1, 2, 1, 2, 1, 0, 1, 2, 1, 0, 1, 2, 1, 0, 1, 2, 1, 2, 1, 2, 1, 0, 1, 0, 1, 2, 1, 0, 1, 0, 1, 2, 1, 2, 1, 0, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 0, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 0, 1, 0, 2, 1, 2, 1, 2, 1, 2, 1, 0, 1, 2, 1, 2, 0, 2, 0]\n",
      "[2, 0, 1, 3, 1, 0, 2, 2, 0, 1, 0, 0, 3, 2, 3, 2, 1, 0, 2, 0, 1, 0, 2, 0, 2, 0, 2, 0, 1, 0, 1, 2, 1, 2, 1, 2, 1, 0, 1, 2, 1, 2, 1, 2, 1, 2, 0, 1, 0, 2, 1, 2, 1, 2, 1, 2, 1, 0, 2, 1, 2, 1, 2, 1, 2, 0, 2, 0, 3, 1, 2, 3, 0, 1, 2, 0, 2, 1, 2, 1, 1, 1, 0, 1, 0, 2, 0, 1, 0, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 0, 1, 0, 3, 0, 1, 0, 1, 0, 1, 3, 1, 2, 1, 2, 1, 0, 1, 2, 1, 0, 1, 2, 1, 0, 1, 2, 1, 2, 1, 2, 1, 0, 1, 0, 1, 2, 1, 0, 1, 0, 1, 2, 1, 1, 1, 0, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 0, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 0, 1, 0, 2, 1, 2, 1, 2, 1, 2, 1, 0, 1, 2, 1, 2, 0, 2, 0]\n",
      "Politics Count: 42\n",
      "Sports Count: 80\n",
      "Natural Count: 71\n",
      "Crime Count: 7\n",
      "\n",
      "Politics Predicted: 43\n",
      "Sports Predicted: 83\n",
      "Natural Predicted: 67\n",
      "Crime Predicted: 7\n",
      "\n",
      "Politics Correct: 41\n",
      "Sports Correct: 80\n",
      "Natural Correct: 66\n",
      "Crime Correct: 7\n",
      "\n",
      "Test Data: 200\n",
      "Accuracy: 0.97\n",
      "Run Time: 635.6206789016724\n"
     ]
    }
   ],
   "source": [
    "#classifier = NaiveBayesClassifier.train(train_set)\n",
    "##classifier = SklearnClassifier(LinearSVC(random_state=0), sparse=False).train(train_set) \n",
    "classifier = SklearnClassifier(LinearSVC(tol=1e-6, max_iter=10000, class_weight='balanced', C=100, multi_class='ovr', random_state=0, loss='hinge'), sparse=False).train(train_set) \n",
    "#classifier = SklearnClassifier(LogisticRegression(), sparse=False).train(train_set)\n",
    "#classifier = SklearnClassifier(DecisionTreeClassifier(), sparse=False).train(train_set)\n",
    "#classifier = SklearnClassifier(RandomForestClassifier(n_estimators=5, max_depth=10, random_state=0, criterion=\"entropy\"), sparse=False).train(train_set) \n",
    "#classifier = SklearnClassifier(KNeighborsClassifier(), sparse=False).train(train_set) \n",
    "\n",
    "cols = ['text']\n",
    "dt = pd.read_csv(\"../input/twitterdataset/testv2.csv\",header=None, names=cols)\n",
    "dtm = read_csv(\"../input/twitterdataset/testmatch2v2.csv\")\n",
    "dtm1 = dtm['tag']\n",
    "dtm2 = dtm['content']\n",
    "#print(classifier)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Predict\n",
    "tok = WordPunctTokenizer()\n",
    "pat1 = r'@[A-Za-z0-9]+'\n",
    "pat2 = r'https?://[A-Za-z0-9./]+'\n",
    "combined_pat = r'|'.join((pat1, pat2))\n",
    "\n",
    "\n",
    "def tweet_cleaner(text):\n",
    "    soup = BeautifulSoup(text, 'lxml')\n",
    "    souped = soup.get_text()\n",
    "    stripped = re.sub(combined_pat, '', souped)\n",
    "    try:\n",
    "        clean = stripped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
    "    except:\n",
    "        clean = stripped\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", clean)\n",
    "    lower_case = letters_only.lower()\n",
    "    words = tok.tokenize(lower_case)\n",
    "    return (\" \".join(words)).strip()\n",
    "\n",
    "\n",
    "testing = dt.text[:]\n",
    "stopWords = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "li=[]\n",
    "test_result = []\n",
    "wordsFiltered = []\n",
    "\n",
    "c=0\n",
    "n=0\n",
    "po=0\n",
    "sp=0\n",
    "cr=0\n",
    "na=0\n",
    "poli=0\n",
    "spor=0\n",
    "natu=0\n",
    "crim=0\n",
    "o=0\n",
    "\n",
    "counterz=0\n",
    "\n",
    "import numpy as np\n",
    "actual = []\n",
    "predicted = []\n",
    "\n",
    "for t in testing:\n",
    "    if counterz%20==0:\n",
    "        print(counterz)\n",
    "    counterz=counterz+1\n",
    "    pol=0\n",
    "    cri=0\n",
    "    nat=0\n",
    "    spo=0\n",
    "    pol2=0\n",
    "    cri2=0\n",
    "    nat2=0\n",
    "    spo2=0\n",
    "    #print(t)\n",
    "    \n",
    "    #print(wd)\n",
    "    b=tweet_cleaner(t)\n",
    "    #print(b)\n",
    "    wd = word_tokenize(b)\n",
    "    test_result.append(b)\n",
    "    words = word_tokenize(tweet_cleaner(t))\n",
    "    #print(words)\n",
    "    \n",
    "    for w in words:\n",
    "        if w not in stopWords:\n",
    "            wordsFiltered.append(w)\n",
    "    #print(wordsFiltered)\n",
    "    for word in wordsFiltered:\n",
    "        l=ps.stem(word)\n",
    "        li.append(l)\n",
    "    #print(li)\n",
    "    \n",
    "    tmp=[]\n",
    "    for i in li:\n",
    "        if i not in tmp:\n",
    "            if i!='b':\n",
    "                if len(i)==1:\n",
    "                    continue\n",
    "                elif len(i)<3:\n",
    "                    if i in wordss:\n",
    "                        tmp.append(correction(i))\n",
    "                else:\n",
    "                    tmp.append(correction(i))\n",
    "    \n",
    "    sentence = tmp\n",
    "    #print(sentence)\n",
    "    #sentence = sentence.lower()\n",
    "    #words = sentence.split(' ')\n",
    "    w=sentence\n",
    "    wordsFiltered=[]\n",
    "    for word in w:\n",
    "        classResult = classifier.classify( word_feats(word))\n",
    "        if classResult == 'pol':\n",
    "            pol = pol + 1\n",
    "        if classResult == 'spo':\n",
    "            spo = spo + 1\n",
    "        if classResult == 'nat':\n",
    "            nat = nat + 1\n",
    "        if classResult == 'cri':\n",
    "            cri = cri + 1\n",
    "    #print(neg)\n",
    "    #print(pos)\n",
    "    #print(li)\n",
    "    #print(len(words))\n",
    "\n",
    "    pol2=float(pol)/(len(wd))\n",
    "    spo2=float(spo)/(len(wd))\n",
    "    nat2=float(nat)/(len(wd))\n",
    "    cri2=float(cri)/(len(wd))\n",
    "\n",
    "    #print('Politics: ' + str(pol2))\n",
    "    #print('Sports: ' + str(spo2))\n",
    "    #print('Natural: ' + str(nat2))\n",
    "    #print('Crime: ' + str(cri2))\n",
    "    \n",
    "    \n",
    "    if dtm1[o]=='politics':\n",
    "        actual.append(0)\n",
    "    elif dtm1[o]=='sports':\n",
    "        actual.append(1)\n",
    "    elif dtm1[o]=='natural':\n",
    "        actual.append(2)\n",
    "    elif dtm1[o]=='crime':\n",
    "        actual.append(3)\n",
    "    \n",
    "    if pol2>=spo2 and pol2>=nat2 and pol2>=cri2:\n",
    "        po=po+1\n",
    "        predicted.append(0)\n",
    "        if dtm1[o]=='politics':\n",
    "            poli=poli+1\n",
    "            #print('politics')\n",
    "    elif spo2>=pol2 and spo2>=nat2 and spo2>=cri2:\n",
    "        sp=sp+1\n",
    "        predicted.append(1)\n",
    "        if dtm1[o]=='sports':\n",
    "            spor=spor+1\n",
    "            #print('sports')\n",
    "    elif nat2>=spo2 and nat2>=pol2 and nat2>=cri2:\n",
    "        na=na+1\n",
    "        predicted.append(2)\n",
    "        if dtm1[o]=='natural':\n",
    "            natu=natu+1\n",
    "            #print('natural')\n",
    "    elif cri2>=spo2 and cri2>=nat2 and cri2>=pol2:\n",
    "        cr=cr+1\n",
    "        predicted.append(3)\n",
    "        if dtm1[o]=='crime':\n",
    "            crim=crim+1\n",
    "            #print('crime')\n",
    "    \n",
    "    #print(li)\n",
    "    while len(li)>0:\n",
    "        li.pop()\n",
    "    \n",
    "    c=c+float(pol2+spo2+nat2+cri2)\n",
    "    n=n+1\n",
    "    o=o+1\n",
    "\n",
    "\n",
    "\n",
    "#np.save(\"actual.npy\", np.array(actual))\n",
    "#np.save(\"SVMpredicted.npy\", np.array(predicted))\n",
    "\n",
    "print(actual)\n",
    "print(predicted)\n",
    "\n",
    "politics_count=0\n",
    "sports_count=0\n",
    "natural_count=0\n",
    "crime_count=0\n",
    "for i in range(0, len(testing)):\n",
    "    if dtm1[i]=='politics':\n",
    "        politics_count=politics_count+1\n",
    "    elif dtm1[i]=='sports':\n",
    "        sports_count=sports_count+1\n",
    "    elif dtm1[i]=='natural':\n",
    "        natural_count=natural_count+1\n",
    "    elif dtm1[i]=='crime':\n",
    "        crime_count=crime_count+1\n",
    "print('Politics Count: ' + str(politics_count))\n",
    "print('Sports Count: ' + str(sports_count))\n",
    "print('Natural Count: ' + str(natural_count))\n",
    "print('Crime Count: ' + str(crime_count))\n",
    "print()\n",
    "\n",
    "\n",
    "\n",
    "ac=poli+spor+crim+natu\n",
    "print('Politics Predicted: ' + str(po))\n",
    "print('Sports Predicted: ' + str(sp))\n",
    "print('Natural Predicted: ' + str(na))\n",
    "print('Crime Predicted: ' + str(cr))\n",
    "\n",
    "print()\n",
    "print('Politics Correct: ' + str(poli))\n",
    "print('Sports Correct: ' + str(spor))\n",
    "print('Natural Correct: ' + str(natu))\n",
    "print('Crime Correct: ' + str(crim))\n",
    "print()\n",
    "\n",
    "print('Test Data: ' + str(n))\n",
    "print('Accuracy: ' + str(ac/n))\n",
    "end = time.time()\n",
    "print('Run Time: ' +str(end - start))# -*- coding: utf-8 -*-\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.059175,
     "end_time": "2020-10-20T18:24:59.047731",
     "exception": false,
     "start_time": "2020-10-20T18:24:58.988556",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# SVM, max_iter=30000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-20T18:24:59.220134Z",
     "iopub.status.busy": "2020-10-20T18:24:59.209606Z",
     "iopub.status.idle": "2020-10-20T18:26:10.075665Z",
     "shell.execute_reply": "2020-10-20T18:26:10.076681Z"
    },
    "papermill": {
     "duration": 70.969993,
     "end_time": "2020-10-20T18:26:10.076940",
     "exception": false,
     "start_time": "2020-10-20T18:24:59.106947",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "120\n",
      "140\n",
      "160\n",
      "180\n",
      "[2, 0, 1, 3, 1, 2, 2, 2, 2, 1, 0, 0, 3, 2, 3, 2, 1, 0, 2, 0, 1, 0, 2, 0, 2, 0, 2, 0, 1, 0, 1, 2, 1, 2, 1, 2, 1, 0, 1, 2, 1, 2, 1, 2, 1, 2, 0, 2, 0, 2, 1, 2, 1, 2, 1, 2, 1, 0, 2, 1, 2, 1, 2, 1, 2, 0, 2, 0, 3, 1, 2, 3, 0, 1, 2, 0, 2, 1, 2, 1, 2, 1, 0, 1, 0, 2, 0, 1, 0, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 0, 1, 0, 1, 0, 3, 0, 1, 0, 1, 0, 1, 3, 1, 2, 1, 2, 1, 0, 1, 2, 1, 0, 1, 2, 1, 0, 1, 2, 1, 2, 1, 2, 1, 0, 1, 0, 1, 2, 1, 0, 1, 0, 1, 2, 1, 2, 1, 0, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 0, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 0, 1, 0, 2, 1, 2, 1, 2, 1, 2, 1, 0, 1, 2, 1, 2, 0, 2, 0]\n",
      "[2, 0, 1, 3, 1, 0, 2, 2, 0, 1, 0, 0, 3, 2, 3, 2, 1, 0, 2, 0, 1, 0, 2, 0, 2, 0, 2, 0, 1, 0, 1, 2, 1, 2, 1, 2, 1, 0, 1, 2, 1, 2, 1, 2, 1, 2, 0, 1, 0, 2, 1, 2, 1, 2, 1, 2, 1, 0, 2, 1, 2, 1, 2, 1, 2, 0, 2, 0, 3, 1, 2, 3, 0, 1, 2, 0, 2, 1, 2, 1, 1, 1, 0, 1, 0, 2, 0, 1, 0, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 0, 1, 0, 3, 0, 1, 0, 1, 0, 1, 3, 1, 2, 1, 2, 1, 0, 1, 2, 1, 0, 1, 2, 1, 0, 1, 2, 1, 2, 1, 2, 1, 0, 1, 0, 1, 2, 1, 0, 1, 0, 1, 2, 1, 1, 1, 0, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 0, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 0, 1, 0, 2, 1, 2, 1, 2, 1, 2, 1, 0, 1, 2, 1, 2, 0, 2, 0]\n",
      "Politics Count: 42\n",
      "Sports Count: 80\n",
      "Natural Count: 71\n",
      "Crime Count: 7\n",
      "\n",
      "Politics Predicted: 43\n",
      "Sports Predicted: 83\n",
      "Natural Predicted: 67\n",
      "Crime Predicted: 7\n",
      "\n",
      "Politics Correct: 41\n",
      "Sports Correct: 80\n",
      "Natural Correct: 66\n",
      "Crime Correct: 7\n",
      "\n",
      "Test Data: 200\n",
      "Accuracy: 0.97\n",
      "Run Time: 706.7681081295013\n"
     ]
    }
   ],
   "source": [
    "#classifier = NaiveBayesClassifier.train(train_set)\n",
    "##classifier = SklearnClassifier(LinearSVC(random_state=0), sparse=False).train(train_set) \n",
    "classifier = SklearnClassifier(LinearSVC(tol=1e-6, max_iter=30000, class_weight='balanced', C=100, multi_class='ovr', random_state=0, loss='hinge'), sparse=False).train(train_set) \n",
    "#classifier = SklearnClassifier(LogisticRegression(), sparse=False).train(train_set)\n",
    "#classifier = SklearnClassifier(DecisionTreeClassifier(), sparse=False).train(train_set)\n",
    "#classifier = SklearnClassifier(RandomForestClassifier(n_estimators=5, max_depth=10, random_state=0, criterion=\"entropy\"), sparse=False).train(train_set) \n",
    "#classifier = SklearnClassifier(KNeighborsClassifier(), sparse=False).train(train_set) \n",
    "\n",
    "cols = ['text']\n",
    "dt = pd.read_csv(\"../input/twitterdataset/testv2.csv\",header=None, names=cols)\n",
    "dtm = read_csv(\"../input/twitterdataset/testmatch2v2.csv\")\n",
    "dtm1 = dtm['tag']\n",
    "dtm2 = dtm['content']\n",
    "#print(classifier)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Predict\n",
    "tok = WordPunctTokenizer()\n",
    "pat1 = r'@[A-Za-z0-9]+'\n",
    "pat2 = r'https?://[A-Za-z0-9./]+'\n",
    "combined_pat = r'|'.join((pat1, pat2))\n",
    "\n",
    "\n",
    "def tweet_cleaner(text):\n",
    "    soup = BeautifulSoup(text, 'lxml')\n",
    "    souped = soup.get_text()\n",
    "    stripped = re.sub(combined_pat, '', souped)\n",
    "    try:\n",
    "        clean = stripped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
    "    except:\n",
    "        clean = stripped\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", clean)\n",
    "    lower_case = letters_only.lower()\n",
    "    words = tok.tokenize(lower_case)\n",
    "    return (\" \".join(words)).strip()\n",
    "\n",
    "\n",
    "testing = dt.text[:]\n",
    "stopWords = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "li=[]\n",
    "test_result = []\n",
    "wordsFiltered = []\n",
    "\n",
    "c=0\n",
    "n=0\n",
    "po=0\n",
    "sp=0\n",
    "cr=0\n",
    "na=0\n",
    "poli=0\n",
    "spor=0\n",
    "natu=0\n",
    "crim=0\n",
    "o=0\n",
    "\n",
    "counterz=0\n",
    "\n",
    "import numpy as np\n",
    "actual = []\n",
    "predicted = []\n",
    "\n",
    "for t in testing:\n",
    "    if counterz%20==0:\n",
    "        print(counterz)\n",
    "    counterz=counterz+1\n",
    "    pol=0\n",
    "    cri=0\n",
    "    nat=0\n",
    "    spo=0\n",
    "    pol2=0\n",
    "    cri2=0\n",
    "    nat2=0\n",
    "    spo2=0\n",
    "    #print(t)\n",
    "    \n",
    "    #print(wd)\n",
    "    b=tweet_cleaner(t)\n",
    "    #print(b)\n",
    "    wd = word_tokenize(b)\n",
    "    test_result.append(b)\n",
    "    words = word_tokenize(tweet_cleaner(t))\n",
    "    #print(words)\n",
    "    \n",
    "    for w in words:\n",
    "        if w not in stopWords:\n",
    "            wordsFiltered.append(w)\n",
    "    #print(wordsFiltered)\n",
    "    for word in wordsFiltered:\n",
    "        l=ps.stem(word)\n",
    "        li.append(l)\n",
    "    #print(li)\n",
    "    \n",
    "    tmp=[]\n",
    "    for i in li:\n",
    "        if i not in tmp:\n",
    "            if i!='b':\n",
    "                if len(i)==1:\n",
    "                    continue\n",
    "                elif len(i)<3:\n",
    "                    if i in wordss:\n",
    "                        tmp.append(correction(i))\n",
    "                else:\n",
    "                    tmp.append(correction(i))\n",
    "    \n",
    "    sentence = tmp\n",
    "    #print(sentence)\n",
    "    #sentence = sentence.lower()\n",
    "    #words = sentence.split(' ')\n",
    "    w=sentence\n",
    "    wordsFiltered=[]\n",
    "    for word in w:\n",
    "        classResult = classifier.classify( word_feats(word))\n",
    "        if classResult == 'pol':\n",
    "            pol = pol + 1\n",
    "        if classResult == 'spo':\n",
    "            spo = spo + 1\n",
    "        if classResult == 'nat':\n",
    "            nat = nat + 1\n",
    "        if classResult == 'cri':\n",
    "            cri = cri + 1\n",
    "    #print(neg)\n",
    "    #print(pos)\n",
    "    #print(li)\n",
    "    #print(len(words))\n",
    "\n",
    "    pol2=float(pol)/(len(wd))\n",
    "    spo2=float(spo)/(len(wd))\n",
    "    nat2=float(nat)/(len(wd))\n",
    "    cri2=float(cri)/(len(wd))\n",
    "\n",
    "    #print('Politics: ' + str(pol2))\n",
    "    #print('Sports: ' + str(spo2))\n",
    "    #print('Natural: ' + str(nat2))\n",
    "    #print('Crime: ' + str(cri2))\n",
    "    \n",
    "    \n",
    "    if dtm1[o]=='politics':\n",
    "        actual.append(0)\n",
    "    elif dtm1[o]=='sports':\n",
    "        actual.append(1)\n",
    "    elif dtm1[o]=='natural':\n",
    "        actual.append(2)\n",
    "    elif dtm1[o]=='crime':\n",
    "        actual.append(3)\n",
    "    \n",
    "    if pol2>=spo2 and pol2>=nat2 and pol2>=cri2:\n",
    "        po=po+1\n",
    "        predicted.append(0)\n",
    "        if dtm1[o]=='politics':\n",
    "            poli=poli+1\n",
    "            #print('politics')\n",
    "    elif spo2>=pol2 and spo2>=nat2 and spo2>=cri2:\n",
    "        sp=sp+1\n",
    "        predicted.append(1)\n",
    "        if dtm1[o]=='sports':\n",
    "            spor=spor+1\n",
    "            #print('sports')\n",
    "    elif nat2>=spo2 and nat2>=pol2 and nat2>=cri2:\n",
    "        na=na+1\n",
    "        predicted.append(2)\n",
    "        if dtm1[o]=='natural':\n",
    "            natu=natu+1\n",
    "            #print('natural')\n",
    "    elif cri2>=spo2 and cri2>=nat2 and cri2>=pol2:\n",
    "        cr=cr+1\n",
    "        predicted.append(3)\n",
    "        if dtm1[o]=='crime':\n",
    "            crim=crim+1\n",
    "            #print('crime')\n",
    "    \n",
    "    #print(li)\n",
    "    while len(li)>0:\n",
    "        li.pop()\n",
    "    \n",
    "    c=c+float(pol2+spo2+nat2+cri2)\n",
    "    n=n+1\n",
    "    o=o+1\n",
    "\n",
    "\n",
    "\n",
    "#np.save(\"actual.npy\", np.array(actual))\n",
    "#np.save(\"SVMpredicted.npy\", np.array(predicted))\n",
    "\n",
    "print(actual)\n",
    "print(predicted)\n",
    "\n",
    "politics_count=0\n",
    "sports_count=0\n",
    "natural_count=0\n",
    "crime_count=0\n",
    "for i in range(0, len(testing)):\n",
    "    if dtm1[i]=='politics':\n",
    "        politics_count=politics_count+1\n",
    "    elif dtm1[i]=='sports':\n",
    "        sports_count=sports_count+1\n",
    "    elif dtm1[i]=='natural':\n",
    "        natural_count=natural_count+1\n",
    "    elif dtm1[i]=='crime':\n",
    "        crime_count=crime_count+1\n",
    "print('Politics Count: ' + str(politics_count))\n",
    "print('Sports Count: ' + str(sports_count))\n",
    "print('Natural Count: ' + str(natural_count))\n",
    "print('Crime Count: ' + str(crime_count))\n",
    "print()\n",
    "\n",
    "\n",
    "\n",
    "ac=poli+spor+crim+natu\n",
    "print('Politics Predicted: ' + str(po))\n",
    "print('Sports Predicted: ' + str(sp))\n",
    "print('Natural Predicted: ' + str(na))\n",
    "print('Crime Predicted: ' + str(cr))\n",
    "\n",
    "print()\n",
    "print('Politics Correct: ' + str(poli))\n",
    "print('Sports Correct: ' + str(spor))\n",
    "print('Natural Correct: ' + str(natu))\n",
    "print('Crime Correct: ' + str(crim))\n",
    "print()\n",
    "\n",
    "print('Test Data: ' + str(n))\n",
    "print('Accuracy: ' + str(ac/n))\n",
    "end = time.time()\n",
    "print('Run Time: ' +str(end - start))# -*- coding: utf-8 -*-\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.065311,
     "end_time": "2020-10-20T18:26:10.208430",
     "exception": false,
     "start_time": "2020-10-20T18:26:10.143119",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# SVM, no class weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-20T18:26:10.378727Z",
     "iopub.status.busy": "2020-10-20T18:26:10.357757Z",
     "iopub.status.idle": "2020-10-20T18:27:21.358004Z",
     "shell.execute_reply": "2020-10-20T18:27:21.359091Z"
    },
    "papermill": {
     "duration": 71.086481,
     "end_time": "2020-10-20T18:27:21.359339",
     "exception": false,
     "start_time": "2020-10-20T18:26:10.272858",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "120\n",
      "140\n",
      "160\n",
      "180\n",
      "[2, 0, 1, 3, 1, 2, 2, 2, 2, 1, 0, 0, 3, 2, 3, 2, 1, 0, 2, 0, 1, 0, 2, 0, 2, 0, 2, 0, 1, 0, 1, 2, 1, 2, 1, 2, 1, 0, 1, 2, 1, 2, 1, 2, 1, 2, 0, 2, 0, 2, 1, 2, 1, 2, 1, 2, 1, 0, 2, 1, 2, 1, 2, 1, 2, 0, 2, 0, 3, 1, 2, 3, 0, 1, 2, 0, 2, 1, 2, 1, 2, 1, 0, 1, 0, 2, 0, 1, 0, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 0, 1, 0, 1, 0, 3, 0, 1, 0, 1, 0, 1, 3, 1, 2, 1, 2, 1, 0, 1, 2, 1, 0, 1, 2, 1, 0, 1, 2, 1, 2, 1, 2, 1, 0, 1, 0, 1, 2, 1, 0, 1, 0, 1, 2, 1, 2, 1, 0, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 0, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 0, 1, 0, 2, 1, 2, 1, 2, 1, 2, 1, 0, 1, 2, 1, 2, 0, 2, 0]\n",
      "[2, 0, 1, 3, 1, 0, 2, 2, 0, 1, 0, 0, 3, 2, 3, 2, 1, 0, 2, 0, 1, 0, 2, 0, 2, 0, 2, 0, 1, 0, 1, 2, 1, 2, 1, 2, 1, 0, 1, 2, 1, 2, 1, 2, 1, 2, 0, 1, 0, 2, 1, 2, 1, 2, 1, 2, 1, 0, 2, 1, 2, 1, 2, 1, 2, 0, 2, 1, 3, 1, 2, 3, 0, 1, 2, 0, 2, 1, 2, 1, 1, 1, 0, 1, 0, 2, 0, 1, 0, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 0, 1, 0, 3, 0, 1, 0, 1, 0, 1, 3, 1, 2, 1, 2, 1, 0, 1, 2, 1, 0, 1, 2, 1, 0, 1, 2, 1, 2, 1, 2, 1, 0, 1, 0, 1, 2, 1, 0, 1, 0, 1, 2, 1, 2, 1, 0, 1, 2, 2, 2, 1, 2, 1, 2, 1, 2, 1, 0, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 0, 1, 0, 2, 1, 2, 1, 2, 1, 2, 1, 0, 1, 2, 1, 2, 0, 2, 0]\n",
      "Politics Count: 42\n",
      "Sports Count: 80\n",
      "Natural Count: 71\n",
      "Crime Count: 7\n",
      "\n",
      "Politics Predicted: 42\n",
      "Sports Predicted: 82\n",
      "Natural Predicted: 69\n",
      "Crime Predicted: 7\n",
      "\n",
      "Politics Correct: 40\n",
      "Sports Correct: 79\n",
      "Natural Correct: 67\n",
      "Crime Correct: 7\n",
      "\n",
      "Test Data: 200\n",
      "Accuracy: 0.965\n",
      "Run Time: 778.0466940402985\n"
     ]
    }
   ],
   "source": [
    "#classifier = NaiveBayesClassifier.train(train_set)\n",
    "##classifier = SklearnClassifier(LinearSVC(random_state=0), sparse=False).train(train_set) \n",
    "classifier = SklearnClassifier(LinearSVC(tol=1e-6, max_iter=20000, C=100, multi_class='ovr', random_state=0, loss='hinge'), sparse=False).train(train_set) \n",
    "#classifier = SklearnClassifier(LogisticRegression(), sparse=False).train(train_set)\n",
    "#classifier = SklearnClassifier(DecisionTreeClassifier(), sparse=False).train(train_set)\n",
    "#classifier = SklearnClassifier(RandomForestClassifier(n_estimators=5, max_depth=10, random_state=0, criterion=\"entropy\"), sparse=False).train(train_set) \n",
    "#classifier = SklearnClassifier(KNeighborsClassifier(), sparse=False).train(train_set) \n",
    "\n",
    "cols = ['text']\n",
    "dt = pd.read_csv(\"../input/twitterdataset/testv2.csv\",header=None, names=cols)\n",
    "dtm = read_csv(\"../input/twitterdataset/testmatch2v2.csv\")\n",
    "dtm1 = dtm['tag']\n",
    "dtm2 = dtm['content']\n",
    "#print(classifier)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Predict\n",
    "tok = WordPunctTokenizer()\n",
    "pat1 = r'@[A-Za-z0-9]+'\n",
    "pat2 = r'https?://[A-Za-z0-9./]+'\n",
    "combined_pat = r'|'.join((pat1, pat2))\n",
    "\n",
    "\n",
    "def tweet_cleaner(text):\n",
    "    soup = BeautifulSoup(text, 'lxml')\n",
    "    souped = soup.get_text()\n",
    "    stripped = re.sub(combined_pat, '', souped)\n",
    "    try:\n",
    "        clean = stripped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
    "    except:\n",
    "        clean = stripped\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", clean)\n",
    "    lower_case = letters_only.lower()\n",
    "    words = tok.tokenize(lower_case)\n",
    "    return (\" \".join(words)).strip()\n",
    "\n",
    "\n",
    "testing = dt.text[:]\n",
    "stopWords = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "li=[]\n",
    "test_result = []\n",
    "wordsFiltered = []\n",
    "\n",
    "c=0\n",
    "n=0\n",
    "po=0\n",
    "sp=0\n",
    "cr=0\n",
    "na=0\n",
    "poli=0\n",
    "spor=0\n",
    "natu=0\n",
    "crim=0\n",
    "o=0\n",
    "\n",
    "counterz=0\n",
    "\n",
    "import numpy as np\n",
    "actual = []\n",
    "predicted = []\n",
    "\n",
    "for t in testing:\n",
    "    if counterz%20==0:\n",
    "        print(counterz)\n",
    "    counterz=counterz+1\n",
    "    pol=0\n",
    "    cri=0\n",
    "    nat=0\n",
    "    spo=0\n",
    "    pol2=0\n",
    "    cri2=0\n",
    "    nat2=0\n",
    "    spo2=0\n",
    "    #print(t)\n",
    "    \n",
    "    #print(wd)\n",
    "    b=tweet_cleaner(t)\n",
    "    #print(b)\n",
    "    wd = word_tokenize(b)\n",
    "    test_result.append(b)\n",
    "    words = word_tokenize(tweet_cleaner(t))\n",
    "    #print(words)\n",
    "    \n",
    "    for w in words:\n",
    "        if w not in stopWords:\n",
    "            wordsFiltered.append(w)\n",
    "    #print(wordsFiltered)\n",
    "    for word in wordsFiltered:\n",
    "        l=ps.stem(word)\n",
    "        li.append(l)\n",
    "    #print(li)\n",
    "    \n",
    "    tmp=[]\n",
    "    for i in li:\n",
    "        if i not in tmp:\n",
    "            if i!='b':\n",
    "                if len(i)==1:\n",
    "                    continue\n",
    "                elif len(i)<3:\n",
    "                    if i in wordss:\n",
    "                        tmp.append(correction(i))\n",
    "                else:\n",
    "                    tmp.append(correction(i))\n",
    "    \n",
    "    sentence = tmp\n",
    "    #print(sentence)\n",
    "    #sentence = sentence.lower()\n",
    "    #words = sentence.split(' ')\n",
    "    w=sentence\n",
    "    wordsFiltered=[]\n",
    "    for word in w:\n",
    "        classResult = classifier.classify( word_feats(word))\n",
    "        if classResult == 'pol':\n",
    "            pol = pol + 1\n",
    "        if classResult == 'spo':\n",
    "            spo = spo + 1\n",
    "        if classResult == 'nat':\n",
    "            nat = nat + 1\n",
    "        if classResult == 'cri':\n",
    "            cri = cri + 1\n",
    "    #print(neg)\n",
    "    #print(pos)\n",
    "    #print(li)\n",
    "    #print(len(words))\n",
    "\n",
    "    pol2=float(pol)/(len(wd))\n",
    "    spo2=float(spo)/(len(wd))\n",
    "    nat2=float(nat)/(len(wd))\n",
    "    cri2=float(cri)/(len(wd))\n",
    "\n",
    "    #print('Politics: ' + str(pol2))\n",
    "    #print('Sports: ' + str(spo2))\n",
    "    #print('Natural: ' + str(nat2))\n",
    "    #print('Crime: ' + str(cri2))\n",
    "    \n",
    "    \n",
    "    if dtm1[o]=='politics':\n",
    "        actual.append(0)\n",
    "    elif dtm1[o]=='sports':\n",
    "        actual.append(1)\n",
    "    elif dtm1[o]=='natural':\n",
    "        actual.append(2)\n",
    "    elif dtm1[o]=='crime':\n",
    "        actual.append(3)\n",
    "    \n",
    "    if pol2>=spo2 and pol2>=nat2 and pol2>=cri2:\n",
    "        po=po+1\n",
    "        predicted.append(0)\n",
    "        if dtm1[o]=='politics':\n",
    "            poli=poli+1\n",
    "            #print('politics')\n",
    "    elif spo2>=pol2 and spo2>=nat2 and spo2>=cri2:\n",
    "        sp=sp+1\n",
    "        predicted.append(1)\n",
    "        if dtm1[o]=='sports':\n",
    "            spor=spor+1\n",
    "            #print('sports')\n",
    "    elif nat2>=spo2 and nat2>=pol2 and nat2>=cri2:\n",
    "        na=na+1\n",
    "        predicted.append(2)\n",
    "        if dtm1[o]=='natural':\n",
    "            natu=natu+1\n",
    "            #print('natural')\n",
    "    elif cri2>=spo2 and cri2>=nat2 and cri2>=pol2:\n",
    "        cr=cr+1\n",
    "        predicted.append(3)\n",
    "        if dtm1[o]=='crime':\n",
    "            crim=crim+1\n",
    "            #print('crime')\n",
    "    \n",
    "    #print(li)\n",
    "    while len(li)>0:\n",
    "        li.pop()\n",
    "    \n",
    "    c=c+float(pol2+spo2+nat2+cri2)\n",
    "    n=n+1\n",
    "    o=o+1\n",
    "\n",
    "\n",
    "\n",
    "#np.save(\"actual.npy\", np.array(actual))\n",
    "#np.save(\"SVMpredicted.npy\", np.array(predicted))\n",
    "\n",
    "print(actual)\n",
    "print(predicted)\n",
    "\n",
    "politics_count=0\n",
    "sports_count=0\n",
    "natural_count=0\n",
    "crime_count=0\n",
    "for i in range(0, len(testing)):\n",
    "    if dtm1[i]=='politics':\n",
    "        politics_count=politics_count+1\n",
    "    elif dtm1[i]=='sports':\n",
    "        sports_count=sports_count+1\n",
    "    elif dtm1[i]=='natural':\n",
    "        natural_count=natural_count+1\n",
    "    elif dtm1[i]=='crime':\n",
    "        crime_count=crime_count+1\n",
    "print('Politics Count: ' + str(politics_count))\n",
    "print('Sports Count: ' + str(sports_count))\n",
    "print('Natural Count: ' + str(natural_count))\n",
    "print('Crime Count: ' + str(crime_count))\n",
    "print()\n",
    "\n",
    "\n",
    "\n",
    "ac=poli+spor+crim+natu\n",
    "print('Politics Predicted: ' + str(po))\n",
    "print('Sports Predicted: ' + str(sp))\n",
    "print('Natural Predicted: ' + str(na))\n",
    "print('Crime Predicted: ' + str(cr))\n",
    "\n",
    "print()\n",
    "print('Politics Correct: ' + str(poli))\n",
    "print('Sports Correct: ' + str(spor))\n",
    "print('Natural Correct: ' + str(natu))\n",
    "print('Crime Correct: ' + str(crim))\n",
    "print()\n",
    "\n",
    "print('Test Data: ' + str(n))\n",
    "print('Accuracy: ' + str(ac/n))\n",
    "end = time.time()\n",
    "print('Run Time: ' +str(end - start))# -*- coding: utf-8 -*-\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.069253,
     "end_time": "2020-10-20T18:27:21.499041",
     "exception": false,
     "start_time": "2020-10-20T18:27:21.429788",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# SVM, C=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-20T18:27:21.706541Z",
     "iopub.status.busy": "2020-10-20T18:27:21.671257Z",
     "iopub.status.idle": "2020-10-20T18:28:32.297669Z",
     "shell.execute_reply": "2020-10-20T18:28:32.298358Z"
    },
    "papermill": {
     "duration": 70.731151,
     "end_time": "2020-10-20T18:28:32.298536",
     "exception": false,
     "start_time": "2020-10-20T18:27:21.567385",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "120\n",
      "140\n",
      "160\n",
      "180\n",
      "[2, 0, 1, 3, 1, 2, 2, 2, 2, 1, 0, 0, 3, 2, 3, 2, 1, 0, 2, 0, 1, 0, 2, 0, 2, 0, 2, 0, 1, 0, 1, 2, 1, 2, 1, 2, 1, 0, 1, 2, 1, 2, 1, 2, 1, 2, 0, 2, 0, 2, 1, 2, 1, 2, 1, 2, 1, 0, 2, 1, 2, 1, 2, 1, 2, 0, 2, 0, 3, 1, 2, 3, 0, 1, 2, 0, 2, 1, 2, 1, 2, 1, 0, 1, 0, 2, 0, 1, 0, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 0, 1, 0, 1, 0, 3, 0, 1, 0, 1, 0, 1, 3, 1, 2, 1, 2, 1, 0, 1, 2, 1, 0, 1, 2, 1, 0, 1, 2, 1, 2, 1, 2, 1, 0, 1, 0, 1, 2, 1, 0, 1, 0, 1, 2, 1, 2, 1, 0, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 0, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 0, 1, 0, 2, 1, 2, 1, 2, 1, 2, 1, 0, 1, 2, 1, 2, 0, 2, 0]\n",
      "[2, 0, 1, 3, 1, 0, 2, 2, 0, 1, 0, 0, 3, 2, 3, 2, 1, 0, 2, 0, 1, 0, 2, 0, 2, 0, 2, 0, 1, 0, 1, 2, 1, 2, 1, 2, 1, 0, 1, 2, 1, 2, 1, 2, 1, 2, 0, 1, 0, 2, 1, 2, 1, 2, 1, 2, 1, 0, 2, 1, 2, 1, 2, 1, 2, 0, 2, 0, 3, 1, 2, 3, 0, 1, 2, 0, 2, 1, 2, 1, 1, 1, 0, 1, 0, 2, 0, 1, 0, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 0, 1, 0, 3, 0, 1, 0, 1, 0, 1, 3, 1, 2, 1, 2, 1, 0, 1, 2, 1, 0, 1, 2, 1, 0, 1, 2, 1, 2, 1, 2, 1, 0, 1, 0, 1, 2, 1, 0, 1, 0, 1, 2, 1, 1, 1, 0, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 0, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 0, 1, 0, 2, 1, 2, 1, 2, 1, 2, 1, 0, 1, 2, 1, 2, 0, 2, 0]\n",
      "Politics Count: 42\n",
      "Sports Count: 80\n",
      "Natural Count: 71\n",
      "Crime Count: 7\n",
      "\n",
      "Politics Predicted: 43\n",
      "Sports Predicted: 83\n",
      "Natural Predicted: 67\n",
      "Crime Predicted: 7\n",
      "\n",
      "Politics Correct: 41\n",
      "Sports Correct: 80\n",
      "Natural Correct: 66\n",
      "Crime Correct: 7\n",
      "\n",
      "Test Data: 200\n",
      "Accuracy: 0.97\n",
      "Run Time: 848.9683871269226\n"
     ]
    }
   ],
   "source": [
    "#classifier = NaiveBayesClassifier.train(train_set)\n",
    "##classifier = SklearnClassifier(LinearSVC(random_state=0), sparse=False).train(train_set) \n",
    "classifier = SklearnClassifier(LinearSVC(tol=1e-6, max_iter=20000, class_weight='balanced', C=10, multi_class='ovr', random_state=0, loss='hinge'), sparse=False).train(train_set) \n",
    "#classifier = SklearnClassifier(LogisticRegression(), sparse=False).train(train_set)\n",
    "#classifier = SklearnClassifier(DecisionTreeClassifier(), sparse=False).train(train_set)\n",
    "#classifier = SklearnClassifier(RandomForestClassifier(n_estimators=5, max_depth=10, random_state=0, criterion=\"entropy\"), sparse=False).train(train_set) \n",
    "#classifier = SklearnClassifier(KNeighborsClassifier(), sparse=False).train(train_set) \n",
    "\n",
    "cols = ['text']\n",
    "dt = pd.read_csv(\"../input/twitterdataset/testv2.csv\",header=None, names=cols)\n",
    "dtm = read_csv(\"../input/twitterdataset/testmatch2v2.csv\")\n",
    "dtm1 = dtm['tag']\n",
    "dtm2 = dtm['content']\n",
    "#print(classifier)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Predict\n",
    "tok = WordPunctTokenizer()\n",
    "pat1 = r'@[A-Za-z0-9]+'\n",
    "pat2 = r'https?://[A-Za-z0-9./]+'\n",
    "combined_pat = r'|'.join((pat1, pat2))\n",
    "\n",
    "\n",
    "def tweet_cleaner(text):\n",
    "    soup = BeautifulSoup(text, 'lxml')\n",
    "    souped = soup.get_text()\n",
    "    stripped = re.sub(combined_pat, '', souped)\n",
    "    try:\n",
    "        clean = stripped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
    "    except:\n",
    "        clean = stripped\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", clean)\n",
    "    lower_case = letters_only.lower()\n",
    "    words = tok.tokenize(lower_case)\n",
    "    return (\" \".join(words)).strip()\n",
    "\n",
    "\n",
    "testing = dt.text[:]\n",
    "stopWords = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "li=[]\n",
    "test_result = []\n",
    "wordsFiltered = []\n",
    "\n",
    "c=0\n",
    "n=0\n",
    "po=0\n",
    "sp=0\n",
    "cr=0\n",
    "na=0\n",
    "poli=0\n",
    "spor=0\n",
    "natu=0\n",
    "crim=0\n",
    "o=0\n",
    "\n",
    "counterz=0\n",
    "\n",
    "import numpy as np\n",
    "actual = []\n",
    "predicted = []\n",
    "\n",
    "for t in testing:\n",
    "    if counterz%20==0:\n",
    "        print(counterz)\n",
    "    counterz=counterz+1\n",
    "    pol=0\n",
    "    cri=0\n",
    "    nat=0\n",
    "    spo=0\n",
    "    pol2=0\n",
    "    cri2=0\n",
    "    nat2=0\n",
    "    spo2=0\n",
    "    #print(t)\n",
    "    \n",
    "    #print(wd)\n",
    "    b=tweet_cleaner(t)\n",
    "    #print(b)\n",
    "    wd = word_tokenize(b)\n",
    "    test_result.append(b)\n",
    "    words = word_tokenize(tweet_cleaner(t))\n",
    "    #print(words)\n",
    "    \n",
    "    for w in words:\n",
    "        if w not in stopWords:\n",
    "            wordsFiltered.append(w)\n",
    "    #print(wordsFiltered)\n",
    "    for word in wordsFiltered:\n",
    "        l=ps.stem(word)\n",
    "        li.append(l)\n",
    "    #print(li)\n",
    "    \n",
    "    tmp=[]\n",
    "    for i in li:\n",
    "        if i not in tmp:\n",
    "            if i!='b':\n",
    "                if len(i)==1:\n",
    "                    continue\n",
    "                elif len(i)<3:\n",
    "                    if i in wordss:\n",
    "                        tmp.append(correction(i))\n",
    "                else:\n",
    "                    tmp.append(correction(i))\n",
    "    \n",
    "    sentence = tmp\n",
    "    #print(sentence)\n",
    "    #sentence = sentence.lower()\n",
    "    #words = sentence.split(' ')\n",
    "    w=sentence\n",
    "    wordsFiltered=[]\n",
    "    for word in w:\n",
    "        classResult = classifier.classify( word_feats(word))\n",
    "        if classResult == 'pol':\n",
    "            pol = pol + 1\n",
    "        if classResult == 'spo':\n",
    "            spo = spo + 1\n",
    "        if classResult == 'nat':\n",
    "            nat = nat + 1\n",
    "        if classResult == 'cri':\n",
    "            cri = cri + 1\n",
    "    #print(neg)\n",
    "    #print(pos)\n",
    "    #print(li)\n",
    "    #print(len(words))\n",
    "\n",
    "    pol2=float(pol)/(len(wd))\n",
    "    spo2=float(spo)/(len(wd))\n",
    "    nat2=float(nat)/(len(wd))\n",
    "    cri2=float(cri)/(len(wd))\n",
    "\n",
    "    #print('Politics: ' + str(pol2))\n",
    "    #print('Sports: ' + str(spo2))\n",
    "    #print('Natural: ' + str(nat2))\n",
    "    #print('Crime: ' + str(cri2))\n",
    "    \n",
    "    \n",
    "    if dtm1[o]=='politics':\n",
    "        actual.append(0)\n",
    "    elif dtm1[o]=='sports':\n",
    "        actual.append(1)\n",
    "    elif dtm1[o]=='natural':\n",
    "        actual.append(2)\n",
    "    elif dtm1[o]=='crime':\n",
    "        actual.append(3)\n",
    "    \n",
    "    if pol2>=spo2 and pol2>=nat2 and pol2>=cri2:\n",
    "        po=po+1\n",
    "        predicted.append(0)\n",
    "        if dtm1[o]=='politics':\n",
    "            poli=poli+1\n",
    "            #print('politics')\n",
    "    elif spo2>=pol2 and spo2>=nat2 and spo2>=cri2:\n",
    "        sp=sp+1\n",
    "        predicted.append(1)\n",
    "        if dtm1[o]=='sports':\n",
    "            spor=spor+1\n",
    "            #print('sports')\n",
    "    elif nat2>=spo2 and nat2>=pol2 and nat2>=cri2:\n",
    "        na=na+1\n",
    "        predicted.append(2)\n",
    "        if dtm1[o]=='natural':\n",
    "            natu=natu+1\n",
    "            #print('natural')\n",
    "    elif cri2>=spo2 and cri2>=nat2 and cri2>=pol2:\n",
    "        cr=cr+1\n",
    "        predicted.append(3)\n",
    "        if dtm1[o]=='crime':\n",
    "            crim=crim+1\n",
    "            #print('crime')\n",
    "    \n",
    "    #print(li)\n",
    "    while len(li)>0:\n",
    "        li.pop()\n",
    "    \n",
    "    c=c+float(pol2+spo2+nat2+cri2)\n",
    "    n=n+1\n",
    "    o=o+1\n",
    "\n",
    "\n",
    "\n",
    "#np.save(\"actual.npy\", np.array(actual))\n",
    "#np.save(\"SVMpredicted.npy\", np.array(predicted))\n",
    "\n",
    "print(actual)\n",
    "print(predicted)\n",
    "\n",
    "politics_count=0\n",
    "sports_count=0\n",
    "natural_count=0\n",
    "crime_count=0\n",
    "for i in range(0, len(testing)):\n",
    "    if dtm1[i]=='politics':\n",
    "        politics_count=politics_count+1\n",
    "    elif dtm1[i]=='sports':\n",
    "        sports_count=sports_count+1\n",
    "    elif dtm1[i]=='natural':\n",
    "        natural_count=natural_count+1\n",
    "    elif dtm1[i]=='crime':\n",
    "        crime_count=crime_count+1\n",
    "print('Politics Count: ' + str(politics_count))\n",
    "print('Sports Count: ' + str(sports_count))\n",
    "print('Natural Count: ' + str(natural_count))\n",
    "print('Crime Count: ' + str(crime_count))\n",
    "print()\n",
    "\n",
    "\n",
    "\n",
    "ac=poli+spor+crim+natu\n",
    "print('Politics Predicted: ' + str(po))\n",
    "print('Sports Predicted: ' + str(sp))\n",
    "print('Natural Predicted: ' + str(na))\n",
    "print('Crime Predicted: ' + str(cr))\n",
    "\n",
    "print()\n",
    "print('Politics Correct: ' + str(poli))\n",
    "print('Sports Correct: ' + str(spor))\n",
    "print('Natural Correct: ' + str(natu))\n",
    "print('Crime Correct: ' + str(crim))\n",
    "print()\n",
    "\n",
    "print('Test Data: ' + str(n))\n",
    "print('Accuracy: ' + str(ac/n))\n",
    "end = time.time()\n",
    "print('Run Time: ' +str(end - start))# -*- coding: utf-8 -*-\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.074539,
     "end_time": "2020-10-20T18:28:32.451554",
     "exception": false,
     "start_time": "2020-10-20T18:28:32.377015",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# SVM, C=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-20T18:28:32.633812Z",
     "iopub.status.busy": "2020-10-20T18:28:32.623268Z",
     "iopub.status.idle": "2020-10-20T18:29:43.849001Z",
     "shell.execute_reply": "2020-10-20T18:29:43.848219Z"
    },
    "papermill": {
     "duration": 71.320505,
     "end_time": "2020-10-20T18:29:43.849149",
     "exception": false,
     "start_time": "2020-10-20T18:28:32.528644",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "120\n",
      "140\n",
      "160\n",
      "180\n",
      "[2, 0, 1, 3, 1, 2, 2, 2, 2, 1, 0, 0, 3, 2, 3, 2, 1, 0, 2, 0, 1, 0, 2, 0, 2, 0, 2, 0, 1, 0, 1, 2, 1, 2, 1, 2, 1, 0, 1, 2, 1, 2, 1, 2, 1, 2, 0, 2, 0, 2, 1, 2, 1, 2, 1, 2, 1, 0, 2, 1, 2, 1, 2, 1, 2, 0, 2, 0, 3, 1, 2, 3, 0, 1, 2, 0, 2, 1, 2, 1, 2, 1, 0, 1, 0, 2, 0, 1, 0, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 0, 1, 0, 1, 0, 3, 0, 1, 0, 1, 0, 1, 3, 1, 2, 1, 2, 1, 0, 1, 2, 1, 0, 1, 2, 1, 0, 1, 2, 1, 2, 1, 2, 1, 0, 1, 0, 1, 2, 1, 0, 1, 0, 1, 2, 1, 2, 1, 0, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 0, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 0, 1, 0, 2, 1, 2, 1, 2, 1, 2, 1, 0, 1, 2, 1, 2, 0, 2, 0]\n",
      "[2, 0, 1, 3, 1, 0, 2, 2, 0, 1, 0, 0, 3, 2, 3, 2, 1, 0, 2, 0, 1, 0, 2, 0, 2, 0, 2, 0, 1, 0, 1, 2, 1, 2, 1, 2, 1, 0, 1, 2, 1, 2, 1, 2, 1, 2, 0, 1, 0, 2, 1, 2, 1, 2, 1, 2, 1, 0, 2, 1, 2, 1, 2, 1, 2, 0, 2, 0, 3, 1, 2, 3, 0, 1, 2, 0, 2, 1, 2, 1, 1, 1, 0, 1, 0, 2, 0, 1, 0, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 0, 1, 0, 3, 0, 1, 0, 1, 0, 1, 3, 1, 2, 1, 2, 1, 0, 1, 2, 1, 0, 1, 2, 1, 0, 1, 2, 1, 2, 1, 2, 1, 0, 1, 0, 1, 2, 1, 0, 1, 0, 1, 2, 1, 2, 1, 0, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 0, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 0, 1, 0, 2, 1, 2, 1, 2, 1, 2, 1, 0, 1, 2, 1, 2, 0, 2, 0]\n",
      "Politics Count: 42\n",
      "Sports Count: 80\n",
      "Natural Count: 71\n",
      "Crime Count: 7\n",
      "\n",
      "Politics Predicted: 43\n",
      "Sports Predicted: 82\n",
      "Natural Predicted: 68\n",
      "Crime Predicted: 7\n",
      "\n",
      "Politics Correct: 41\n",
      "Sports Correct: 80\n",
      "Natural Correct: 67\n",
      "Crime Correct: 7\n",
      "\n",
      "Test Data: 200\n",
      "Accuracy: 0.975\n",
      "Run Time: 920.6018414497375\n"
     ]
    }
   ],
   "source": [
    "#classifier = NaiveBayesClassifier.train(train_set)\n",
    "##classifier = SklearnClassifier(LinearSVC(random_state=0), sparse=False).train(train_set) \n",
    "classifier = SklearnClassifier(LinearSVC(tol=1e-6, max_iter=20000, class_weight='balanced', C=1000, multi_class='ovr', random_state=0, loss='hinge'), sparse=False).train(train_set) \n",
    "#classifier = SklearnClassifier(LogisticRegression(), sparse=False).train(train_set)\n",
    "#classifier = SklearnClassifier(DecisionTreeClassifier(), sparse=False).train(train_set)\n",
    "#classifier = SklearnClassifier(RandomForestClassifier(n_estimators=5, max_depth=10, random_state=0, criterion=\"entropy\"), sparse=False).train(train_set) \n",
    "#classifier = SklearnClassifier(KNeighborsClassifier(), sparse=False).train(train_set) \n",
    "\n",
    "cols = ['text']\n",
    "dt = pd.read_csv(\"../input/twitterdataset/testv2.csv\",header=None, names=cols)\n",
    "dtm = read_csv(\"../input/twitterdataset/testmatch2v2.csv\")\n",
    "dtm1 = dtm['tag']\n",
    "dtm2 = dtm['content']\n",
    "#print(classifier)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Predict\n",
    "tok = WordPunctTokenizer()\n",
    "pat1 = r'@[A-Za-z0-9]+'\n",
    "pat2 = r'https?://[A-Za-z0-9./]+'\n",
    "combined_pat = r'|'.join((pat1, pat2))\n",
    "\n",
    "\n",
    "def tweet_cleaner(text):\n",
    "    soup = BeautifulSoup(text, 'lxml')\n",
    "    souped = soup.get_text()\n",
    "    stripped = re.sub(combined_pat, '', souped)\n",
    "    try:\n",
    "        clean = stripped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
    "    except:\n",
    "        clean = stripped\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", clean)\n",
    "    lower_case = letters_only.lower()\n",
    "    words = tok.tokenize(lower_case)\n",
    "    return (\" \".join(words)).strip()\n",
    "\n",
    "\n",
    "testing = dt.text[:]\n",
    "stopWords = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "li=[]\n",
    "test_result = []\n",
    "wordsFiltered = []\n",
    "\n",
    "c=0\n",
    "n=0\n",
    "po=0\n",
    "sp=0\n",
    "cr=0\n",
    "na=0\n",
    "poli=0\n",
    "spor=0\n",
    "natu=0\n",
    "crim=0\n",
    "o=0\n",
    "\n",
    "counterz=0\n",
    "\n",
    "import numpy as np\n",
    "actual = []\n",
    "predicted = []\n",
    "\n",
    "for t in testing:\n",
    "    if counterz%20==0:\n",
    "        print(counterz)\n",
    "    counterz=counterz+1\n",
    "    pol=0\n",
    "    cri=0\n",
    "    nat=0\n",
    "    spo=0\n",
    "    pol2=0\n",
    "    cri2=0\n",
    "    nat2=0\n",
    "    spo2=0\n",
    "    #print(t)\n",
    "    \n",
    "    #print(wd)\n",
    "    b=tweet_cleaner(t)\n",
    "    #print(b)\n",
    "    wd = word_tokenize(b)\n",
    "    test_result.append(b)\n",
    "    words = word_tokenize(tweet_cleaner(t))\n",
    "    #print(words)\n",
    "    \n",
    "    for w in words:\n",
    "        if w not in stopWords:\n",
    "            wordsFiltered.append(w)\n",
    "    #print(wordsFiltered)\n",
    "    for word in wordsFiltered:\n",
    "        l=ps.stem(word)\n",
    "        li.append(l)\n",
    "    #print(li)\n",
    "    \n",
    "    tmp=[]\n",
    "    for i in li:\n",
    "        if i not in tmp:\n",
    "            if i!='b':\n",
    "                if len(i)==1:\n",
    "                    continue\n",
    "                elif len(i)<3:\n",
    "                    if i in wordss:\n",
    "                        tmp.append(correction(i))\n",
    "                else:\n",
    "                    tmp.append(correction(i))\n",
    "    \n",
    "    sentence = tmp\n",
    "    #print(sentence)\n",
    "    #sentence = sentence.lower()\n",
    "    #words = sentence.split(' ')\n",
    "    w=sentence\n",
    "    wordsFiltered=[]\n",
    "    for word in w:\n",
    "        classResult = classifier.classify( word_feats(word))\n",
    "        if classResult == 'pol':\n",
    "            pol = pol + 1\n",
    "        if classResult == 'spo':\n",
    "            spo = spo + 1\n",
    "        if classResult == 'nat':\n",
    "            nat = nat + 1\n",
    "        if classResult == 'cri':\n",
    "            cri = cri + 1\n",
    "    #print(neg)\n",
    "    #print(pos)\n",
    "    #print(li)\n",
    "    #print(len(words))\n",
    "\n",
    "    pol2=float(pol)/(len(wd))\n",
    "    spo2=float(spo)/(len(wd))\n",
    "    nat2=float(nat)/(len(wd))\n",
    "    cri2=float(cri)/(len(wd))\n",
    "\n",
    "    #print('Politics: ' + str(pol2))\n",
    "    #print('Sports: ' + str(spo2))\n",
    "    #print('Natural: ' + str(nat2))\n",
    "    #print('Crime: ' + str(cri2))\n",
    "    \n",
    "    \n",
    "    if dtm1[o]=='politics':\n",
    "        actual.append(0)\n",
    "    elif dtm1[o]=='sports':\n",
    "        actual.append(1)\n",
    "    elif dtm1[o]=='natural':\n",
    "        actual.append(2)\n",
    "    elif dtm1[o]=='crime':\n",
    "        actual.append(3)\n",
    "    \n",
    "    if pol2>=spo2 and pol2>=nat2 and pol2>=cri2:\n",
    "        po=po+1\n",
    "        predicted.append(0)\n",
    "        if dtm1[o]=='politics':\n",
    "            poli=poli+1\n",
    "            #print('politics')\n",
    "    elif spo2>=pol2 and spo2>=nat2 and spo2>=cri2:\n",
    "        sp=sp+1\n",
    "        predicted.append(1)\n",
    "        if dtm1[o]=='sports':\n",
    "            spor=spor+1\n",
    "            #print('sports')\n",
    "    elif nat2>=spo2 and nat2>=pol2 and nat2>=cri2:\n",
    "        na=na+1\n",
    "        predicted.append(2)\n",
    "        if dtm1[o]=='natural':\n",
    "            natu=natu+1\n",
    "            #print('natural')\n",
    "    elif cri2>=spo2 and cri2>=nat2 and cri2>=pol2:\n",
    "        cr=cr+1\n",
    "        predicted.append(3)\n",
    "        if dtm1[o]=='crime':\n",
    "            crim=crim+1\n",
    "            #print('crime')\n",
    "    \n",
    "    #print(li)\n",
    "    while len(li)>0:\n",
    "        li.pop()\n",
    "    \n",
    "    c=c+float(pol2+spo2+nat2+cri2)\n",
    "    n=n+1\n",
    "    o=o+1\n",
    "\n",
    "\n",
    "\n",
    "#np.save(\"actual.npy\", np.array(actual))\n",
    "#np.save(\"SVMpredicted.npy\", np.array(predicted))\n",
    "\n",
    "print(actual)\n",
    "print(predicted)\n",
    "\n",
    "politics_count=0\n",
    "sports_count=0\n",
    "natural_count=0\n",
    "crime_count=0\n",
    "for i in range(0, len(testing)):\n",
    "    if dtm1[i]=='politics':\n",
    "        politics_count=politics_count+1\n",
    "    elif dtm1[i]=='sports':\n",
    "        sports_count=sports_count+1\n",
    "    elif dtm1[i]=='natural':\n",
    "        natural_count=natural_count+1\n",
    "    elif dtm1[i]=='crime':\n",
    "        crime_count=crime_count+1\n",
    "print('Politics Count: ' + str(politics_count))\n",
    "print('Sports Count: ' + str(sports_count))\n",
    "print('Natural Count: ' + str(natural_count))\n",
    "print('Crime Count: ' + str(crime_count))\n",
    "print()\n",
    "\n",
    "\n",
    "\n",
    "ac=poli+spor+crim+natu\n",
    "print('Politics Predicted: ' + str(po))\n",
    "print('Sports Predicted: ' + str(sp))\n",
    "print('Natural Predicted: ' + str(na))\n",
    "print('Crime Predicted: ' + str(cr))\n",
    "\n",
    "print()\n",
    "print('Politics Correct: ' + str(poli))\n",
    "print('Sports Correct: ' + str(spor))\n",
    "print('Natural Correct: ' + str(natu))\n",
    "print('Crime Correct: ' + str(crim))\n",
    "print()\n",
    "\n",
    "print('Test Data: ' + str(n))\n",
    "print('Accuracy: ' + str(ac/n))\n",
    "end = time.time()\n",
    "print('Run Time: ' +str(end - start))# -*- coding: utf-8 -*-\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.080206,
     "end_time": "2020-10-20T18:29:44.011377",
     "exception": false,
     "start_time": "2020-10-20T18:29:43.931171",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# SVM, no loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-20T18:29:44.194196Z",
     "iopub.status.busy": "2020-10-20T18:29:44.177953Z",
     "iopub.status.idle": "2020-10-20T18:31:05.114668Z",
     "shell.execute_reply": "2020-10-20T18:31:05.111125Z"
    },
    "papermill": {
     "duration": 81.024615,
     "end_time": "2020-10-20T18:31:05.114887",
     "exception": false,
     "start_time": "2020-10-20T18:29:44.090272",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "120\n",
      "140\n",
      "160\n",
      "180\n",
      "[2, 0, 1, 3, 1, 2, 2, 2, 2, 1, 0, 0, 3, 2, 3, 2, 1, 0, 2, 0, 1, 0, 2, 0, 2, 0, 2, 0, 1, 0, 1, 2, 1, 2, 1, 2, 1, 0, 1, 2, 1, 2, 1, 2, 1, 2, 0, 2, 0, 2, 1, 2, 1, 2, 1, 2, 1, 0, 2, 1, 2, 1, 2, 1, 2, 0, 2, 0, 3, 1, 2, 3, 0, 1, 2, 0, 2, 1, 2, 1, 2, 1, 0, 1, 0, 2, 0, 1, 0, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 0, 1, 0, 1, 0, 3, 0, 1, 0, 1, 0, 1, 3, 1, 2, 1, 2, 1, 0, 1, 2, 1, 0, 1, 2, 1, 0, 1, 2, 1, 2, 1, 2, 1, 0, 1, 0, 1, 2, 1, 0, 1, 0, 1, 2, 1, 2, 1, 0, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 0, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 0, 1, 0, 2, 1, 2, 1, 2, 1, 2, 1, 0, 1, 2, 1, 2, 0, 2, 0]\n",
      "[2, 0, 1, 3, 1, 0, 2, 2, 0, 1, 0, 0, 3, 2, 3, 2, 1, 0, 2, 0, 1, 0, 2, 0, 2, 0, 2, 0, 1, 0, 1, 2, 1, 2, 1, 2, 1, 0, 1, 2, 1, 2, 1, 2, 1, 2, 0, 1, 0, 2, 1, 2, 1, 2, 1, 2, 1, 0, 2, 1, 2, 1, 2, 1, 2, 0, 2, 0, 3, 1, 2, 3, 0, 1, 2, 0, 2, 1, 2, 1, 1, 1, 0, 1, 0, 2, 0, 1, 0, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 0, 1, 0, 3, 0, 1, 0, 1, 0, 1, 3, 1, 2, 1, 2, 1, 0, 1, 2, 1, 0, 1, 2, 1, 0, 1, 2, 1, 2, 1, 2, 1, 0, 1, 0, 1, 2, 1, 0, 1, 0, 1, 2, 1, 1, 1, 0, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 0, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 0, 1, 0, 2, 1, 2, 1, 2, 1, 2, 1, 0, 1, 2, 1, 2, 0, 2, 0]\n",
      "Politics Count: 42\n",
      "Sports Count: 80\n",
      "Natural Count: 71\n",
      "Crime Count: 7\n",
      "\n",
      "Politics Predicted: 43\n",
      "Sports Predicted: 83\n",
      "Natural Predicted: 67\n",
      "Crime Predicted: 7\n",
      "\n",
      "Politics Correct: 41\n",
      "Sports Correct: 80\n",
      "Natural Correct: 66\n",
      "Crime Correct: 7\n",
      "\n",
      "Test Data: 200\n",
      "Accuracy: 0.97\n",
      "Run Time: 1001.8685710430145\n"
     ]
    }
   ],
   "source": [
    "#classifier = NaiveBayesClassifier.train(train_set)\n",
    "##classifier = SklearnClassifier(LinearSVC(random_state=0), sparse=False).train(train_set) \n",
    "classifier = SklearnClassifier(LinearSVC(tol=1e-6, max_iter=20000, class_weight='balanced', C=100, multi_class='ovr', random_state=0), sparse=False).train(train_set) \n",
    "#classifier = SklearnClassifier(LogisticRegression(), sparse=False).train(train_set)\n",
    "#classifier = SklearnClassifier(DecisionTreeClassifier(), sparse=False).train(train_set)\n",
    "#classifier = SklearnClassifier(RandomForestClassifier(n_estimators=5, max_depth=10, random_state=0, criterion=\"entropy\"), sparse=False).train(train_set) \n",
    "#classifier = SklearnClassifier(KNeighborsClassifier(), sparse=False).train(train_set) \n",
    "\n",
    "cols = ['text']\n",
    "dt = pd.read_csv(\"../input/twitterdataset/testv2.csv\",header=None, names=cols)\n",
    "dtm = read_csv(\"../input/twitterdataset/testmatch2v2.csv\")\n",
    "dtm1 = dtm['tag']\n",
    "dtm2 = dtm['content']\n",
    "#print(classifier)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Predict\n",
    "tok = WordPunctTokenizer()\n",
    "pat1 = r'@[A-Za-z0-9]+'\n",
    "pat2 = r'https?://[A-Za-z0-9./]+'\n",
    "combined_pat = r'|'.join((pat1, pat2))\n",
    "\n",
    "\n",
    "def tweet_cleaner(text):\n",
    "    soup = BeautifulSoup(text, 'lxml')\n",
    "    souped = soup.get_text()\n",
    "    stripped = re.sub(combined_pat, '', souped)\n",
    "    try:\n",
    "        clean = stripped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
    "    except:\n",
    "        clean = stripped\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", clean)\n",
    "    lower_case = letters_only.lower()\n",
    "    words = tok.tokenize(lower_case)\n",
    "    return (\" \".join(words)).strip()\n",
    "\n",
    "\n",
    "testing = dt.text[:]\n",
    "stopWords = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "li=[]\n",
    "test_result = []\n",
    "wordsFiltered = []\n",
    "\n",
    "c=0\n",
    "n=0\n",
    "po=0\n",
    "sp=0\n",
    "cr=0\n",
    "na=0\n",
    "poli=0\n",
    "spor=0\n",
    "natu=0\n",
    "crim=0\n",
    "o=0\n",
    "\n",
    "counterz=0\n",
    "\n",
    "import numpy as np\n",
    "actual = []\n",
    "predicted = []\n",
    "\n",
    "for t in testing:\n",
    "    if counterz%20==0:\n",
    "        print(counterz)\n",
    "    counterz=counterz+1\n",
    "    pol=0\n",
    "    cri=0\n",
    "    nat=0\n",
    "    spo=0\n",
    "    pol2=0\n",
    "    cri2=0\n",
    "    nat2=0\n",
    "    spo2=0\n",
    "    #print(t)\n",
    "    \n",
    "    #print(wd)\n",
    "    b=tweet_cleaner(t)\n",
    "    #print(b)\n",
    "    wd = word_tokenize(b)\n",
    "    test_result.append(b)\n",
    "    words = word_tokenize(tweet_cleaner(t))\n",
    "    #print(words)\n",
    "    \n",
    "    for w in words:\n",
    "        if w not in stopWords:\n",
    "            wordsFiltered.append(w)\n",
    "    #print(wordsFiltered)\n",
    "    for word in wordsFiltered:\n",
    "        l=ps.stem(word)\n",
    "        li.append(l)\n",
    "    #print(li)\n",
    "    \n",
    "    tmp=[]\n",
    "    for i in li:\n",
    "        if i not in tmp:\n",
    "            if i!='b':\n",
    "                if len(i)==1:\n",
    "                    continue\n",
    "                elif len(i)<3:\n",
    "                    if i in wordss:\n",
    "                        tmp.append(correction(i))\n",
    "                else:\n",
    "                    tmp.append(correction(i))\n",
    "    \n",
    "    sentence = tmp\n",
    "    #print(sentence)\n",
    "    #sentence = sentence.lower()\n",
    "    #words = sentence.split(' ')\n",
    "    w=sentence\n",
    "    wordsFiltered=[]\n",
    "    for word in w:\n",
    "        classResult = classifier.classify( word_feats(word))\n",
    "        if classResult == 'pol':\n",
    "            pol = pol + 1\n",
    "        if classResult == 'spo':\n",
    "            spo = spo + 1\n",
    "        if classResult == 'nat':\n",
    "            nat = nat + 1\n",
    "        if classResult == 'cri':\n",
    "            cri = cri + 1\n",
    "    #print(neg)\n",
    "    #print(pos)\n",
    "    #print(li)\n",
    "    #print(len(words))\n",
    "\n",
    "    pol2=float(pol)/(len(wd))\n",
    "    spo2=float(spo)/(len(wd))\n",
    "    nat2=float(nat)/(len(wd))\n",
    "    cri2=float(cri)/(len(wd))\n",
    "\n",
    "    #print('Politics: ' + str(pol2))\n",
    "    #print('Sports: ' + str(spo2))\n",
    "    #print('Natural: ' + str(nat2))\n",
    "    #print('Crime: ' + str(cri2))\n",
    "    \n",
    "    \n",
    "    if dtm1[o]=='politics':\n",
    "        actual.append(0)\n",
    "    elif dtm1[o]=='sports':\n",
    "        actual.append(1)\n",
    "    elif dtm1[o]=='natural':\n",
    "        actual.append(2)\n",
    "    elif dtm1[o]=='crime':\n",
    "        actual.append(3)\n",
    "    \n",
    "    if pol2>=spo2 and pol2>=nat2 and pol2>=cri2:\n",
    "        po=po+1\n",
    "        predicted.append(0)\n",
    "        if dtm1[o]=='politics':\n",
    "            poli=poli+1\n",
    "            #print('politics')\n",
    "    elif spo2>=pol2 and spo2>=nat2 and spo2>=cri2:\n",
    "        sp=sp+1\n",
    "        predicted.append(1)\n",
    "        if dtm1[o]=='sports':\n",
    "            spor=spor+1\n",
    "            #print('sports')\n",
    "    elif nat2>=spo2 and nat2>=pol2 and nat2>=cri2:\n",
    "        na=na+1\n",
    "        predicted.append(2)\n",
    "        if dtm1[o]=='natural':\n",
    "            natu=natu+1\n",
    "            #print('natural')\n",
    "    elif cri2>=spo2 and cri2>=nat2 and cri2>=pol2:\n",
    "        cr=cr+1\n",
    "        predicted.append(3)\n",
    "        if dtm1[o]=='crime':\n",
    "            crim=crim+1\n",
    "            #print('crime')\n",
    "    \n",
    "    #print(li)\n",
    "    while len(li)>0:\n",
    "        li.pop()\n",
    "    \n",
    "    c=c+float(pol2+spo2+nat2+cri2)\n",
    "    n=n+1\n",
    "    o=o+1\n",
    "\n",
    "\n",
    "\n",
    "#np.save(\"actual.npy\", np.array(actual))\n",
    "#np.save(\"SVMpredicted.npy\", np.array(predicted))\n",
    "\n",
    "print(actual)\n",
    "print(predicted)\n",
    "\n",
    "politics_count=0\n",
    "sports_count=0\n",
    "natural_count=0\n",
    "crime_count=0\n",
    "for i in range(0, len(testing)):\n",
    "    if dtm1[i]=='politics':\n",
    "        politics_count=politics_count+1\n",
    "    elif dtm1[i]=='sports':\n",
    "        sports_count=sports_count+1\n",
    "    elif dtm1[i]=='natural':\n",
    "        natural_count=natural_count+1\n",
    "    elif dtm1[i]=='crime':\n",
    "        crime_count=crime_count+1\n",
    "print('Politics Count: ' + str(politics_count))\n",
    "print('Sports Count: ' + str(sports_count))\n",
    "print('Natural Count: ' + str(natural_count))\n",
    "print('Crime Count: ' + str(crime_count))\n",
    "print()\n",
    "\n",
    "\n",
    "\n",
    "ac=poli+spor+crim+natu\n",
    "print('Politics Predicted: ' + str(po))\n",
    "print('Sports Predicted: ' + str(sp))\n",
    "print('Natural Predicted: ' + str(na))\n",
    "print('Crime Predicted: ' + str(cr))\n",
    "\n",
    "print()\n",
    "print('Politics Correct: ' + str(poli))\n",
    "print('Sports Correct: ' + str(spor))\n",
    "print('Natural Correct: ' + str(natu))\n",
    "print('Crime Correct: ' + str(crim))\n",
    "print()\n",
    "\n",
    "print('Test Data: ' + str(n))\n",
    "print('Accuracy: ' + str(ac/n))\n",
    "end = time.time()\n",
    "print('Run Time: ' +str(end - start))# -*- coding: utf-8 -*-\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.082023,
     "end_time": "2020-10-20T18:31:05.296351",
     "exception": false,
     "start_time": "2020-10-20T18:31:05.214328",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-20T18:31:05.509588Z",
     "iopub.status.busy": "2020-10-20T18:31:05.504425Z",
     "iopub.status.idle": "2020-10-20T18:32:10.171395Z",
     "shell.execute_reply": "2020-10-20T18:32:10.172119Z"
    },
    "papermill": {
     "duration": 64.792961,
     "end_time": "2020-10-20T18:32:10.172309",
     "exception": false,
     "start_time": "2020-10-20T18:31:05.379348",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "120\n",
      "140\n",
      "160\n",
      "180\n",
      "[2, 0, 1, 3, 1, 2, 2, 2, 2, 1, 1, 1, 2, 1, 0, 2, 0, 0, 2, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 0, 0, 1, 2, 1, 0, 3, 1, 2, 1, 1, 1, 2, 1, 2, 1, 0, 1, 2, 0, 1, 1, 1, 1, 3, 0, 1, 2, 1, 2, 2, 0, 1, 3, 1, 0, 1, 0, 1, 2, 1, 0, 1, 0, 1, 2, 1, 2, 1, 0, 1, 2, 1, 2, 2, 2, 0, 0, 2, 2, 2, 2, 0, 2, 2, 1, 1, 1, 0, 1, 1, 2, 2, 2, 2, 1, 2, 2, 0, 2, 2, 3, 2, 2, 0, 0, 3, 2, 1, 3, 0, 3, 1, 2, 3, 0, 2, 0, 1, 0, 3, 2, 2, 1, 2, 1, 2, 1, 3, 0, 0, 2, 1, 0, 1, 2, 1, 1, 2, 0, 1, 3, 3, 2, 0, 0, 0, 3, 1, 2, 3, 2, 3, 1, 1, 2, 0, 2, 3, 1, 2, 3, 3, 2, 2, 1, 1, 1, 1, 2, 2, 2, 2, 1, 0, 2, 1, 1, 2, 1, 1, 1, 2, 1, 1, 2, 3, 2, 2, 1, 1]\n",
      "[2, 0, 1, 3, 1, 0, 2, 2, 0, 1, 1, 1, 2, 1, 0, 2, 2, 0, 2, 1, 1, 1, 2, 2, 2, 1, 1, 2, 2, 0, 0, 1, 2, 1, 0, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 0, 1, 2, 0, 1, 1, 1, 1, 3, 0, 1, 2, 1, 2, 2, 0, 1, 3, 1, 0, 1, 0, 1, 2, 1, 0, 1, 0, 1, 2, 1, 1, 1, 0, 1, 2, 1, 2, 2, 2, 0, 0, 2, 2, 2, 2, 0, 2, 2, 1, 1, 2, 0, 1, 1, 2, 2, 2, 2, 1, 2, 2, 0, 2, 2, 2, 2, 2, 0, 2, 2, 2, 1, 2, 0, 2, 1, 2, 2, 0, 2, 0, 1, 0, 1, 3, 2, 1, 2, 1, 2, 1, 2, 2, 0, 2, 1, 0, 1, 1, 2, 1, 2, 0, 1, 3, 0, 2, 0, 2, 0, 2, 1, 1, 2, 2, 2, 1, 1, 2, 0, 2, 0, 1, 2, 0, 0, 2, 2, 1, 1, 1, 1, 2, 2, 2, 1, 1, 0, 2, 1, 1, 2, 2, 1, 1, 2, 1, 1, 2, 2, 2, 2, 1, 1]\n",
      "Politics Count: 36\n",
      "Sports Count: 75\n",
      "Natural Count: 69\n",
      "Crime Count: 20\n",
      "\n",
      "Politics Predicted: 38\n",
      "Sports Predicted: 73\n",
      "Natural Predicted: 84\n",
      "Crime Predicted: 5\n",
      "\n",
      "Politics Correct: 32\n",
      "Sports Correct: 68\n",
      "Natural Correct: 62\n",
      "Crime Correct: 4\n",
      "\n",
      "Test Data: 200\n",
      "Accuracy: 0.83\n",
      "Run Time: 1066.841892004013\n"
     ]
    }
   ],
   "source": [
    "#classifier = NaiveBayesClassifier.train(train_set)\n",
    "##classifier = SklearnClassifier(LinearSVC(random_state=0), sparse=False).train(train_set) \n",
    "#classifier = SklearnClassifier(LinearSVC(tol=1e-6, max_iter=20000, class_weight='balanced', C=100, multi_class='ovr', random_state=0, loss='hinge'), sparse=False).train(train_set) \n",
    "classifier = SklearnClassifier(LogisticRegression(), sparse=False).train(train_set)\n",
    "#classifier = SklearnClassifier(DecisionTreeClassifier(), sparse=False).train(train_set)\n",
    "#classifier = SklearnClassifier(RandomForestClassifier(n_estimators=5, max_depth=10, random_state=0, criterion=\"entropy\"), sparse=False).train(train_set) \n",
    "#classifier = SklearnClassifier(KNeighborsClassifier(), sparse=False).train(train_set) \n",
    "\n",
    "cols = ['text']\n",
    "dt = pd.read_csv(\"../input/twitterdataset/test.csv\",header=None, names=cols)\n",
    "dtm = read_csv(\"../input/twitterdataset/testmatch2.csv\")\n",
    "dtm1 = dtm['tag']\n",
    "dtm2 = dtm['content']\n",
    "#print(classifier)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Predict\n",
    "tok = WordPunctTokenizer()\n",
    "pat1 = r'@[A-Za-z0-9]+'\n",
    "pat2 = r'https?://[A-Za-z0-9./]+'\n",
    "combined_pat = r'|'.join((pat1, pat2))\n",
    "\n",
    "\n",
    "def tweet_cleaner(text):\n",
    "    soup = BeautifulSoup(text, 'lxml')\n",
    "    souped = soup.get_text()\n",
    "    stripped = re.sub(combined_pat, '', souped)\n",
    "    try:\n",
    "        clean = stripped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
    "    except:\n",
    "        clean = stripped\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", clean)\n",
    "    lower_case = letters_only.lower()\n",
    "    words = tok.tokenize(lower_case)\n",
    "    return (\" \".join(words)).strip()\n",
    "\n",
    "\n",
    "testing = dt.text[:]\n",
    "stopWords = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "li=[]\n",
    "test_result = []\n",
    "wordsFiltered = []\n",
    "\n",
    "c=0\n",
    "n=0\n",
    "po=0\n",
    "sp=0\n",
    "cr=0\n",
    "na=0\n",
    "poli=0\n",
    "spor=0\n",
    "natu=0\n",
    "crim=0\n",
    "o=0\n",
    "\n",
    "counterz=0\n",
    "\n",
    "import numpy as np\n",
    "actual = []\n",
    "predicted = []\n",
    "\n",
    "for t in testing:\n",
    "    if counterz%20==0:\n",
    "        print(counterz)\n",
    "    counterz=counterz+1\n",
    "    pol=0\n",
    "    cri=0\n",
    "    nat=0\n",
    "    spo=0\n",
    "    pol2=0\n",
    "    cri2=0\n",
    "    nat2=0\n",
    "    spo2=0\n",
    "    #print(t)\n",
    "    \n",
    "    #print(wd)\n",
    "    b=tweet_cleaner(t)\n",
    "    #print(b)\n",
    "    wd = word_tokenize(b)\n",
    "    test_result.append(b)\n",
    "    words = word_tokenize(tweet_cleaner(t))\n",
    "    #print(words)\n",
    "    \n",
    "    for w in words:\n",
    "        if w not in stopWords:\n",
    "            wordsFiltered.append(w)\n",
    "    #print(wordsFiltered)\n",
    "    for word in wordsFiltered:\n",
    "        l=ps.stem(word)\n",
    "        li.append(l)\n",
    "    #print(li)\n",
    "    \n",
    "    tmp=[]\n",
    "    for i in li:\n",
    "        if i not in tmp:\n",
    "            if i!='b':\n",
    "                if len(i)==1:\n",
    "                    continue\n",
    "                elif len(i)<3:\n",
    "                    if i in wordss:\n",
    "                        tmp.append(correction(i))\n",
    "                else:\n",
    "                    tmp.append(correction(i))\n",
    "    \n",
    "    sentence = tmp\n",
    "    #print(sentence)\n",
    "    #sentence = sentence.lower()\n",
    "    #words = sentence.split(' ')\n",
    "    w=sentence\n",
    "    wordsFiltered=[]\n",
    "    for word in w:\n",
    "        classResult = classifier.classify( word_feats(word))\n",
    "        if classResult == 'pol':\n",
    "            pol = pol + 1\n",
    "        if classResult == 'spo':\n",
    "            spo = spo + 1\n",
    "        if classResult == 'nat':\n",
    "            nat = nat + 1\n",
    "        if classResult == 'cri':\n",
    "            cri = cri + 1\n",
    "    #print(neg)\n",
    "    #print(pos)\n",
    "    #print(li)\n",
    "    #print(len(words))\n",
    "\n",
    "    pol2=float(pol)/(len(wd))\n",
    "    spo2=float(spo)/(len(wd))\n",
    "    nat2=float(nat)/(len(wd))\n",
    "    cri2=float(cri)/(len(wd))\n",
    "\n",
    "    #print('Politics: ' + str(pol2))\n",
    "    #print('Sports: ' + str(spo2))\n",
    "    #print('Natural: ' + str(nat2))\n",
    "    #print('Crime: ' + str(cri2))\n",
    "    \n",
    "    \n",
    "    if dtm1[o]=='politics':\n",
    "        actual.append(0)\n",
    "    elif dtm1[o]=='sports':\n",
    "        actual.append(1)\n",
    "    elif dtm1[o]=='natural':\n",
    "        actual.append(2)\n",
    "    elif dtm1[o]=='crime':\n",
    "        actual.append(3)\n",
    "    \n",
    "    if pol2>=spo2 and pol2>=nat2 and pol2>=cri2:\n",
    "        po=po+1\n",
    "        predicted.append(0)\n",
    "        if dtm1[o]=='politics':\n",
    "            poli=poli+1\n",
    "            #print('politics')\n",
    "    elif spo2>=pol2 and spo2>=nat2 and spo2>=cri2:\n",
    "        sp=sp+1\n",
    "        predicted.append(1)\n",
    "        if dtm1[o]=='sports':\n",
    "            spor=spor+1\n",
    "            #print('sports')\n",
    "    elif nat2>=spo2 and nat2>=pol2 and nat2>=cri2:\n",
    "        na=na+1\n",
    "        predicted.append(2)\n",
    "        if dtm1[o]=='natural':\n",
    "            natu=natu+1\n",
    "            #print('natural')\n",
    "    elif cri2>=spo2 and cri2>=nat2 and cri2>=pol2:\n",
    "        cr=cr+1\n",
    "        predicted.append(3)\n",
    "        if dtm1[o]=='crime':\n",
    "            crim=crim+1\n",
    "            #print('crime')\n",
    "    \n",
    "    #print(li)\n",
    "    while len(li)>0:\n",
    "        li.pop()\n",
    "    \n",
    "    c=c+float(pol2+spo2+nat2+cri2)\n",
    "    n=n+1\n",
    "    o=o+1\n",
    "\n",
    "\n",
    "\n",
    "#np.save(\"actual.npy\", np.array(actual))\n",
    "#np.save(\"SVMpredicted.npy\", np.array(predicted))\n",
    "\n",
    "print(actual)\n",
    "print(predicted)\n",
    "\n",
    "politics_count=0\n",
    "sports_count=0\n",
    "natural_count=0\n",
    "crime_count=0\n",
    "for i in range(0, len(testing)):\n",
    "    if dtm1[i]=='politics':\n",
    "        politics_count=politics_count+1\n",
    "    elif dtm1[i]=='sports':\n",
    "        sports_count=sports_count+1\n",
    "    elif dtm1[i]=='natural':\n",
    "        natural_count=natural_count+1\n",
    "    elif dtm1[i]=='crime':\n",
    "        crime_count=crime_count+1\n",
    "print('Politics Count: ' + str(politics_count))\n",
    "print('Sports Count: ' + str(sports_count))\n",
    "print('Natural Count: ' + str(natural_count))\n",
    "print('Crime Count: ' + str(crime_count))\n",
    "print()\n",
    "\n",
    "\n",
    "\n",
    "ac=poli+spor+crim+natu\n",
    "print('Politics Predicted: ' + str(po))\n",
    "print('Sports Predicted: ' + str(sp))\n",
    "print('Natural Predicted: ' + str(na))\n",
    "print('Crime Predicted: ' + str(cr))\n",
    "\n",
    "print()\n",
    "print('Politics Correct: ' + str(poli))\n",
    "print('Sports Correct: ' + str(spor))\n",
    "print('Natural Correct: ' + str(natu))\n",
    "print('Crime Correct: ' + str(crim))\n",
    "print()\n",
    "\n",
    "print('Test Data: ' + str(n))\n",
    "print('Accuracy: ' + str(ac/n))\n",
    "end = time.time()\n",
    "print('Run Time: ' +str(end - start))# -*- coding: utf-8 -*-\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.088443,
     "end_time": "2020-10-20T18:32:10.351675",
     "exception": false,
     "start_time": "2020-10-20T18:32:10.263232",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-20T18:32:10.575594Z",
     "iopub.status.busy": "2020-10-20T18:32:10.570247Z",
     "iopub.status.idle": "2020-10-20T18:33:52.633210Z",
     "shell.execute_reply": "2020-10-20T18:33:52.634008Z"
    },
    "papermill": {
     "duration": 102.194348,
     "end_time": "2020-10-20T18:33:52.634189",
     "exception": false,
     "start_time": "2020-10-20T18:32:10.439841",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "120\n",
      "140\n",
      "160\n",
      "180\n",
      "[2, 0, 1, 3, 1, 2, 2, 2, 2, 1, 0, 0, 3, 2, 3, 2, 1, 0, 2, 0, 1, 0, 2, 0, 2, 0, 2, 0, 1, 0, 1, 2, 1, 2, 1, 2, 1, 0, 1, 2, 1, 2, 1, 2, 1, 2, 0, 2, 0, 2, 1, 2, 1, 2, 1, 2, 1, 0, 2, 1, 2, 1, 2, 1, 2, 0, 2, 0, 3, 1, 2, 3, 0, 1, 2, 0, 2, 1, 2, 1, 2, 1, 0, 1, 0, 2, 0, 1, 0, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 0, 1, 0, 1, 0, 3, 0, 1, 0, 1, 0, 1, 3, 1, 2, 1, 2, 1, 0, 1, 2, 1, 0, 1, 2, 1, 0, 1, 2, 1, 2, 1, 2, 1, 0, 1, 0, 1, 2, 1, 0, 1, 0, 1, 2, 1, 2, 1, 0, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 0, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 0, 1, 0, 2, 1, 2, 1, 2, 1, 2, 1, 0, 1, 2, 1, 2, 0, 2, 0]\n",
      "[2, 0, 1, 3, 1, 0, 2, 2, 0, 1, 0, 0, 3, 2, 3, 2, 1, 0, 2, 0, 1, 0, 2, 0, 2, 0, 2, 0, 1, 0, 1, 2, 1, 2, 1, 2, 1, 0, 1, 2, 1, 2, 1, 2, 1, 2, 0, 1, 0, 2, 1, 2, 1, 2, 1, 2, 1, 0, 2, 1, 2, 1, 2, 1, 2, 0, 2, 0, 3, 1, 2, 3, 0, 1, 2, 0, 2, 1, 2, 1, 1, 1, 0, 1, 0, 2, 0, 1, 0, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 0, 1, 0, 3, 0, 1, 0, 1, 0, 1, 3, 1, 2, 1, 2, 1, 0, 1, 2, 1, 0, 1, 2, 1, 0, 1, 2, 1, 2, 1, 2, 1, 0, 1, 0, 1, 2, 1, 0, 1, 0, 1, 2, 1, 1, 1, 0, 1, 2, 2, 2, 1, 2, 1, 2, 1, 2, 1, 0, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 0, 1, 0, 2, 1, 2, 1, 2, 1, 2, 1, 0, 1, 2, 1, 2, 0, 2, 0]\n",
      "Politics Count: 42\n",
      "Sports Count: 80\n",
      "Natural Count: 71\n",
      "Crime Count: 7\n",
      "\n",
      "Politics Predicted: 43\n",
      "Sports Predicted: 82\n",
      "Natural Predicted: 68\n",
      "Crime Predicted: 7\n",
      "\n",
      "Politics Correct: 41\n",
      "Sports Correct: 79\n",
      "Natural Correct: 66\n",
      "Crime Correct: 7\n",
      "\n",
      "Test Data: 200\n",
      "Accuracy: 0.965\n",
      "Run Time: 1169.2993245124817\n"
     ]
    }
   ],
   "source": [
    "#classifier = NaiveBayesClassifier.train(train_set)\n",
    "##classifier = SklearnClassifier(LinearSVC(random_state=0), sparse=False).train(train_set) \n",
    "#classifier = SklearnClassifier(LinearSVC(tol=1e-6, max_iter=20000, class_weight='balanced', C=100, multi_class='ovr', random_state=0, loss='hinge'), sparse=False).train(train_set) \n",
    "#classifier = SklearnClassifier(LogisticRegression(), sparse=False).train(train_set)\n",
    "classifier = SklearnClassifier(DecisionTreeClassifier(), sparse=False).train(train_set)\n",
    "#classifier = SklearnClassifier(RandomForestClassifier(n_estimators=5, max_depth=10, random_state=0, criterion=\"entropy\"), sparse=False).train(train_set) \n",
    "#classifier = SklearnClassifier(KNeighborsClassifier(), sparse=False).train(train_set) \n",
    "\n",
    "cols = ['text']\n",
    "dt = pd.read_csv(\"../input/twitterdataset/testv2.csv\",header=None, names=cols)\n",
    "dtm = read_csv(\"../input/twitterdataset/testmatch2v2.csv\")\n",
    "dtm1 = dtm['tag']\n",
    "dtm2 = dtm['content']\n",
    "#print(classifier)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Predict\n",
    "tok = WordPunctTokenizer()\n",
    "pat1 = r'@[A-Za-z0-9]+'\n",
    "pat2 = r'https?://[A-Za-z0-9./]+'\n",
    "combined_pat = r'|'.join((pat1, pat2))\n",
    "\n",
    "\n",
    "def tweet_cleaner(text):\n",
    "    soup = BeautifulSoup(text, 'lxml')\n",
    "    souped = soup.get_text()\n",
    "    stripped = re.sub(combined_pat, '', souped)\n",
    "    try:\n",
    "        clean = stripped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
    "    except:\n",
    "        clean = stripped\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", clean)\n",
    "    lower_case = letters_only.lower()\n",
    "    words = tok.tokenize(lower_case)\n",
    "    return (\" \".join(words)).strip()\n",
    "\n",
    "\n",
    "testing = dt.text[:]\n",
    "stopWords = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "li=[]\n",
    "test_result = []\n",
    "wordsFiltered = []\n",
    "\n",
    "c=0\n",
    "n=0\n",
    "po=0\n",
    "sp=0\n",
    "cr=0\n",
    "na=0\n",
    "poli=0\n",
    "spor=0\n",
    "natu=0\n",
    "crim=0\n",
    "o=0\n",
    "\n",
    "counterz=0\n",
    "\n",
    "import numpy as np\n",
    "actual = []\n",
    "predicted = []\n",
    "\n",
    "for t in testing:\n",
    "    if counterz%20==0:\n",
    "        print(counterz)\n",
    "    counterz=counterz+1\n",
    "    pol=0\n",
    "    cri=0\n",
    "    nat=0\n",
    "    spo=0\n",
    "    pol2=0\n",
    "    cri2=0\n",
    "    nat2=0\n",
    "    spo2=0\n",
    "    #print(t)\n",
    "    \n",
    "    #print(wd)\n",
    "    b=tweet_cleaner(t)\n",
    "    #print(b)\n",
    "    wd = word_tokenize(b)\n",
    "    test_result.append(b)\n",
    "    words = word_tokenize(tweet_cleaner(t))\n",
    "    #print(words)\n",
    "    \n",
    "    for w in words:\n",
    "        if w not in stopWords:\n",
    "            wordsFiltered.append(w)\n",
    "    #print(wordsFiltered)\n",
    "    for word in wordsFiltered:\n",
    "        l=ps.stem(word)\n",
    "        li.append(l)\n",
    "    #print(li)\n",
    "    \n",
    "    tmp=[]\n",
    "    for i in li:\n",
    "        if i not in tmp:\n",
    "            if i!='b':\n",
    "                if len(i)==1:\n",
    "                    continue\n",
    "                elif len(i)<3:\n",
    "                    if i in wordss:\n",
    "                        tmp.append(correction(i))\n",
    "                else:\n",
    "                    tmp.append(correction(i))\n",
    "    \n",
    "    sentence = tmp\n",
    "    #print(sentence)\n",
    "    #sentence = sentence.lower()\n",
    "    #words = sentence.split(' ')\n",
    "    w=sentence\n",
    "    wordsFiltered=[]\n",
    "    for word in w:\n",
    "        classResult = classifier.classify( word_feats(word))\n",
    "        if classResult == 'pol':\n",
    "            pol = pol + 1\n",
    "        if classResult == 'spo':\n",
    "            spo = spo + 1\n",
    "        if classResult == 'nat':\n",
    "            nat = nat + 1\n",
    "        if classResult == 'cri':\n",
    "            cri = cri + 1\n",
    "    #print(neg)\n",
    "    #print(pos)\n",
    "    #print(li)\n",
    "    #print(len(words))\n",
    "\n",
    "    pol2=float(pol)/(len(wd))\n",
    "    spo2=float(spo)/(len(wd))\n",
    "    nat2=float(nat)/(len(wd))\n",
    "    cri2=float(cri)/(len(wd))\n",
    "\n",
    "    #print('Politics: ' + str(pol2))\n",
    "    #print('Sports: ' + str(spo2))\n",
    "    #print('Natural: ' + str(nat2))\n",
    "    #print('Crime: ' + str(cri2))\n",
    "    \n",
    "    \n",
    "    if dtm1[o]=='politics':\n",
    "        actual.append(0)\n",
    "    elif dtm1[o]=='sports':\n",
    "        actual.append(1)\n",
    "    elif dtm1[o]=='natural':\n",
    "        actual.append(2)\n",
    "    elif dtm1[o]=='crime':\n",
    "        actual.append(3)\n",
    "    \n",
    "    if pol2>=spo2 and pol2>=nat2 and pol2>=cri2:\n",
    "        po=po+1\n",
    "        predicted.append(0)\n",
    "        if dtm1[o]=='politics':\n",
    "            poli=poli+1\n",
    "            #print('politics')\n",
    "    elif spo2>=pol2 and spo2>=nat2 and spo2>=cri2:\n",
    "        sp=sp+1\n",
    "        predicted.append(1)\n",
    "        if dtm1[o]=='sports':\n",
    "            spor=spor+1\n",
    "            #print('sports')\n",
    "    elif nat2>=spo2 and nat2>=pol2 and nat2>=cri2:\n",
    "        na=na+1\n",
    "        predicted.append(2)\n",
    "        if dtm1[o]=='natural':\n",
    "            natu=natu+1\n",
    "            #print('natural')\n",
    "    elif cri2>=spo2 and cri2>=nat2 and cri2>=pol2:\n",
    "        cr=cr+1\n",
    "        predicted.append(3)\n",
    "        if dtm1[o]=='crime':\n",
    "            crim=crim+1\n",
    "            #print('crime')\n",
    "    \n",
    "    #print(li)\n",
    "    while len(li)>0:\n",
    "        li.pop()\n",
    "    \n",
    "    c=c+float(pol2+spo2+nat2+cri2)\n",
    "    n=n+1\n",
    "    o=o+1\n",
    "\n",
    "\n",
    "\n",
    "#np.save(\"actual.npy\", np.array(actual))\n",
    "#np.save(\"SVMpredicted.npy\", np.array(predicted))\n",
    "\n",
    "print(actual)\n",
    "print(predicted)\n",
    "\n",
    "politics_count=0\n",
    "sports_count=0\n",
    "natural_count=0\n",
    "crime_count=0\n",
    "for i in range(0, len(testing)):\n",
    "    if dtm1[i]=='politics':\n",
    "        politics_count=politics_count+1\n",
    "    elif dtm1[i]=='sports':\n",
    "        sports_count=sports_count+1\n",
    "    elif dtm1[i]=='natural':\n",
    "        natural_count=natural_count+1\n",
    "    elif dtm1[i]=='crime':\n",
    "        crime_count=crime_count+1\n",
    "print('Politics Count: ' + str(politics_count))\n",
    "print('Sports Count: ' + str(sports_count))\n",
    "print('Natural Count: ' + str(natural_count))\n",
    "print('Crime Count: ' + str(crime_count))\n",
    "print()\n",
    "\n",
    "\n",
    "\n",
    "ac=poli+spor+crim+natu\n",
    "print('Politics Predicted: ' + str(po))\n",
    "print('Sports Predicted: ' + str(sp))\n",
    "print('Natural Predicted: ' + str(na))\n",
    "print('Crime Predicted: ' + str(cr))\n",
    "\n",
    "print()\n",
    "print('Politics Correct: ' + str(poli))\n",
    "print('Sports Correct: ' + str(spor))\n",
    "print('Natural Correct: ' + str(natu))\n",
    "print('Crime Correct: ' + str(crim))\n",
    "print()\n",
    "\n",
    "print('Test Data: ' + str(n))\n",
    "print('Accuracy: ' + str(ac/n))\n",
    "end = time.time()\n",
    "print('Run Time: ' +str(end - start))# -*- coding: utf-8 -*-\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.092566,
     "end_time": "2020-10-20T18:33:52.820948",
     "exception": false,
     "start_time": "2020-10-20T18:33:52.728382",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-20T18:33:53.038960Z",
     "iopub.status.busy": "2020-10-20T18:33:53.018131Z",
     "iopub.status.idle": "2020-10-20T18:34:57.364953Z",
     "shell.execute_reply": "2020-10-20T18:34:57.365880Z"
    },
    "papermill": {
     "duration": 64.452002,
     "end_time": "2020-10-20T18:34:57.366110",
     "exception": false,
     "start_time": "2020-10-20T18:33:52.914108",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "120\n",
      "140\n",
      "160\n",
      "180\n",
      "[2, 0, 1, 3, 1, 2, 2, 2, 2, 1, 1, 1, 2, 1, 0, 2, 0, 0, 2, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 0, 0, 1, 2, 1, 0, 3, 1, 2, 1, 1, 1, 2, 1, 2, 1, 0, 1, 2, 0, 1, 1, 1, 1, 3, 0, 1, 2, 1, 2, 2, 0, 1, 3, 1, 0, 1, 0, 1, 2, 1, 0, 1, 0, 1, 2, 1, 2, 1, 0, 1, 2, 1, 2, 2, 2, 0, 0, 2, 2, 2, 2, 0, 2, 2, 1, 1, 1, 0, 1, 1, 2, 2, 2, 2, 1, 2, 2, 0, 2, 2, 3, 2, 2, 0, 0, 3, 2, 1, 3, 0, 3, 1, 2, 3, 0, 2, 0, 1, 0, 3, 2, 2, 1, 2, 1, 2, 1, 3, 0, 0, 2, 1, 0, 1, 2, 1, 1, 2, 0, 1, 3, 3, 2, 0, 0, 0, 3, 1, 2, 3, 2, 3, 1, 1, 2, 0, 2, 3, 1, 2, 3, 3, 2, 2, 1, 1, 1, 1, 2, 2, 2, 2, 1, 0, 2, 1, 1, 2, 1, 1, 1, 2, 1, 1, 2, 3, 2, 2, 1, 1]\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 1, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2]\n",
      "Politics Count: 36\n",
      "Sports Count: 75\n",
      "Natural Count: 69\n",
      "Crime Count: 20\n",
      "\n",
      "Politics Predicted: 1\n",
      "Sports Predicted: 12\n",
      "Natural Predicted: 187\n",
      "Crime Predicted: 0\n",
      "\n",
      "Politics Correct: 1\n",
      "Sports Correct: 12\n",
      "Natural Correct: 69\n",
      "Crime Correct: 0\n",
      "\n",
      "Test Data: 200\n",
      "Accuracy: 0.41\n",
      "Run Time: 1234.0278437137604\n"
     ]
    }
   ],
   "source": [
    "#classifier = NaiveBayesClassifier.train(train_set)\n",
    "##classifier = SklearnClassifier(LinearSVC(random_state=0), sparse=False).train(train_set) \n",
    "#classifier = SklearnClassifier(LinearSVC(tol=1e-6, max_iter=20000, class_weight='balanced', C=100, multi_class='ovr', random_state=0, loss='hinge'), sparse=False).train(train_set) \n",
    "#classifier = SklearnClassifier(LogisticRegression(), sparse=False).train(train_set)\n",
    "#classifier = SklearnClassifier(DecisionTreeClassifier(), sparse=False).train(train_set)\n",
    "classifier = SklearnClassifier(RandomForestClassifier(n_estimators=5, max_depth=10, random_state=0, criterion=\"entropy\"), sparse=False).train(train_set) \n",
    "#classifier = SklearnClassifier(KNeighborsClassifier(), sparse=False).train(train_set) \n",
    "\n",
    "cols = ['text']\n",
    "dt = pd.read_csv(\"../input/twitterdataset/test.csv\",header=None, names=cols)\n",
    "dtm = read_csv(\"../input/twitterdataset/testmatch2.csv\")\n",
    "dtm1 = dtm['tag']\n",
    "dtm2 = dtm['content']\n",
    "#print(classifier)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Predict\n",
    "tok = WordPunctTokenizer()\n",
    "pat1 = r'@[A-Za-z0-9]+'\n",
    "pat2 = r'https?://[A-Za-z0-9./]+'\n",
    "combined_pat = r'|'.join((pat1, pat2))\n",
    "\n",
    "\n",
    "def tweet_cleaner(text):\n",
    "    soup = BeautifulSoup(text, 'lxml')\n",
    "    souped = soup.get_text()\n",
    "    stripped = re.sub(combined_pat, '', souped)\n",
    "    try:\n",
    "        clean = stripped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
    "    except:\n",
    "        clean = stripped\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", clean)\n",
    "    lower_case = letters_only.lower()\n",
    "    words = tok.tokenize(lower_case)\n",
    "    return (\" \".join(words)).strip()\n",
    "\n",
    "\n",
    "testing = dt.text[:]\n",
    "stopWords = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "li=[]\n",
    "test_result = []\n",
    "wordsFiltered = []\n",
    "\n",
    "c=0\n",
    "n=0\n",
    "po=0\n",
    "sp=0\n",
    "cr=0\n",
    "na=0\n",
    "poli=0\n",
    "spor=0\n",
    "natu=0\n",
    "crim=0\n",
    "o=0\n",
    "\n",
    "counterz=0\n",
    "\n",
    "import numpy as np\n",
    "actual = []\n",
    "predicted = []\n",
    "\n",
    "for t in testing:\n",
    "    if counterz%20==0:\n",
    "        print(counterz)\n",
    "    counterz=counterz+1\n",
    "    pol=0\n",
    "    cri=0\n",
    "    nat=0\n",
    "    spo=0\n",
    "    pol2=0\n",
    "    cri2=0\n",
    "    nat2=0\n",
    "    spo2=0\n",
    "    #print(t)\n",
    "    \n",
    "    #print(wd)\n",
    "    b=tweet_cleaner(t)\n",
    "    #print(b)\n",
    "    wd = word_tokenize(b)\n",
    "    test_result.append(b)\n",
    "    words = word_tokenize(tweet_cleaner(t))\n",
    "    #print(words)\n",
    "    \n",
    "    for w in words:\n",
    "        if w not in stopWords:\n",
    "            wordsFiltered.append(w)\n",
    "    #print(wordsFiltered)\n",
    "    for word in wordsFiltered:\n",
    "        l=ps.stem(word)\n",
    "        li.append(l)\n",
    "    #print(li)\n",
    "    \n",
    "    tmp=[]\n",
    "    for i in li:\n",
    "        if i not in tmp:\n",
    "            if i!='b':\n",
    "                if len(i)==1:\n",
    "                    continue\n",
    "                elif len(i)<3:\n",
    "                    if i in wordss:\n",
    "                        tmp.append(correction(i))\n",
    "                else:\n",
    "                    tmp.append(correction(i))\n",
    "    \n",
    "    sentence = tmp\n",
    "    #print(sentence)\n",
    "    #sentence = sentence.lower()\n",
    "    #words = sentence.split(' ')\n",
    "    w=sentence\n",
    "    wordsFiltered=[]\n",
    "    for word in w:\n",
    "        classResult = classifier.classify( word_feats(word))\n",
    "        if classResult == 'pol':\n",
    "            pol = pol + 1\n",
    "        if classResult == 'spo':\n",
    "            spo = spo + 1\n",
    "        if classResult == 'nat':\n",
    "            nat = nat + 1\n",
    "        if classResult == 'cri':\n",
    "            cri = cri + 1\n",
    "    #print(neg)\n",
    "    #print(pos)\n",
    "    #print(li)\n",
    "    #print(len(words))\n",
    "\n",
    "    pol2=float(pol)/(len(wd))\n",
    "    spo2=float(spo)/(len(wd))\n",
    "    nat2=float(nat)/(len(wd))\n",
    "    cri2=float(cri)/(len(wd))\n",
    "\n",
    "    #print('Politics: ' + str(pol2))\n",
    "    #print('Sports: ' + str(spo2))\n",
    "    #print('Natural: ' + str(nat2))\n",
    "    #print('Crime: ' + str(cri2))\n",
    "    \n",
    "    \n",
    "    if dtm1[o]=='politics':\n",
    "        actual.append(0)\n",
    "    elif dtm1[o]=='sports':\n",
    "        actual.append(1)\n",
    "    elif dtm1[o]=='natural':\n",
    "        actual.append(2)\n",
    "    elif dtm1[o]=='crime':\n",
    "        actual.append(3)\n",
    "    \n",
    "    if pol2>=spo2 and pol2>=nat2 and pol2>=cri2:\n",
    "        po=po+1\n",
    "        predicted.append(0)\n",
    "        if dtm1[o]=='politics':\n",
    "            poli=poli+1\n",
    "            #print('politics')\n",
    "    elif spo2>=pol2 and spo2>=nat2 and spo2>=cri2:\n",
    "        sp=sp+1\n",
    "        predicted.append(1)\n",
    "        if dtm1[o]=='sports':\n",
    "            spor=spor+1\n",
    "            #print('sports')\n",
    "    elif nat2>=spo2 and nat2>=pol2 and nat2>=cri2:\n",
    "        na=na+1\n",
    "        predicted.append(2)\n",
    "        if dtm1[o]=='natural':\n",
    "            natu=natu+1\n",
    "            #print('natural')\n",
    "    elif cri2>=spo2 and cri2>=nat2 and cri2>=pol2:\n",
    "        cr=cr+1\n",
    "        predicted.append(3)\n",
    "        if dtm1[o]=='crime':\n",
    "            crim=crim+1\n",
    "            #print('crime')\n",
    "    \n",
    "    #print(li)\n",
    "    while len(li)>0:\n",
    "        li.pop()\n",
    "    \n",
    "    c=c+float(pol2+spo2+nat2+cri2)\n",
    "    n=n+1\n",
    "    o=o+1\n",
    "\n",
    "\n",
    "\n",
    "#np.save(\"actual.npy\", np.array(actual))\n",
    "#np.save(\"SVMpredicted.npy\", np.array(predicted))\n",
    "\n",
    "print(actual)\n",
    "print(predicted)\n",
    "\n",
    "politics_count=0\n",
    "sports_count=0\n",
    "natural_count=0\n",
    "crime_count=0\n",
    "for i in range(0, len(testing)):\n",
    "    if dtm1[i]=='politics':\n",
    "        politics_count=politics_count+1\n",
    "    elif dtm1[i]=='sports':\n",
    "        sports_count=sports_count+1\n",
    "    elif dtm1[i]=='natural':\n",
    "        natural_count=natural_count+1\n",
    "    elif dtm1[i]=='crime':\n",
    "        crime_count=crime_count+1\n",
    "print('Politics Count: ' + str(politics_count))\n",
    "print('Sports Count: ' + str(sports_count))\n",
    "print('Natural Count: ' + str(natural_count))\n",
    "print('Crime Count: ' + str(crime_count))\n",
    "print()\n",
    "\n",
    "\n",
    "\n",
    "ac=poli+spor+crim+natu\n",
    "print('Politics Predicted: ' + str(po))\n",
    "print('Sports Predicted: ' + str(sp))\n",
    "print('Natural Predicted: ' + str(na))\n",
    "print('Crime Predicted: ' + str(cr))\n",
    "\n",
    "print()\n",
    "print('Politics Correct: ' + str(poli))\n",
    "print('Sports Correct: ' + str(spor))\n",
    "print('Natural Correct: ' + str(natu))\n",
    "print('Crime Correct: ' + str(crim))\n",
    "print()\n",
    "\n",
    "print('Test Data: ' + str(n))\n",
    "print('Accuracy: ' + str(ac/n))\n",
    "end = time.time()\n",
    "print('Run Time: ' +str(end - start))# -*- coding: utf-8 -*-\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.097527,
     "end_time": "2020-10-20T18:34:57.562445",
     "exception": false,
     "start_time": "2020-10-20T18:34:57.464918",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Random Forest, n_estimator=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-20T18:34:57.779078Z",
     "iopub.status.busy": "2020-10-20T18:34:57.772268Z",
     "iopub.status.idle": "2020-10-20T18:36:01.465354Z",
     "shell.execute_reply": "2020-10-20T18:36:01.466315Z"
    },
    "papermill": {
     "duration": 63.807339,
     "end_time": "2020-10-20T18:36:01.466617",
     "exception": false,
     "start_time": "2020-10-20T18:34:57.659278",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "120\n",
      "140\n",
      "160\n",
      "180\n",
      "[2, 0, 1, 3, 1, 2, 2, 2, 2, 1, 1, 1, 2, 1, 0, 2, 0, 0, 2, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 0, 0, 1, 2, 1, 0, 3, 1, 2, 1, 1, 1, 2, 1, 2, 1, 0, 1, 2, 0, 1, 1, 1, 1, 3, 0, 1, 2, 1, 2, 2, 0, 1, 3, 1, 0, 1, 0, 1, 2, 1, 0, 1, 0, 1, 2, 1, 2, 1, 0, 1, 2, 1, 2, 2, 2, 0, 0, 2, 2, 2, 2, 0, 2, 2, 1, 1, 1, 0, 1, 1, 2, 2, 2, 2, 1, 2, 2, 0, 2, 2, 3, 2, 2, 0, 0, 3, 2, 1, 3, 0, 3, 1, 2, 3, 0, 2, 0, 1, 0, 3, 2, 2, 1, 2, 1, 2, 1, 3, 0, 0, 2, 1, 0, 1, 2, 1, 1, 2, 0, 1, 3, 3, 2, 0, 0, 0, 3, 1, 2, 3, 2, 3, 1, 1, 2, 0, 2, 3, 1, 2, 3, 3, 2, 2, 1, 1, 1, 1, 2, 2, 2, 2, 1, 0, 2, 1, 1, 2, 1, 1, 1, 2, 1, 1, 2, 3, 2, 2, 1, 1]\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2]\n",
      "Politics Count: 36\n",
      "Sports Count: 75\n",
      "Natural Count: 69\n",
      "Crime Count: 20\n",
      "\n",
      "Politics Predicted: 1\n",
      "Sports Predicted: 7\n",
      "Natural Predicted: 192\n",
      "Crime Predicted: 0\n",
      "\n",
      "Politics Correct: 1\n",
      "Sports Correct: 7\n",
      "Natural Correct: 69\n",
      "Crime Correct: 0\n",
      "\n",
      "Test Data: 200\n",
      "Accuracy: 0.385\n",
      "Run Time: 1298.1228816509247\n"
     ]
    }
   ],
   "source": [
    "#classifier = NaiveBayesClassifier.train(train_set)\n",
    "##classifier = SklearnClassifier(LinearSVC(random_state=0), sparse=False).train(train_set) \n",
    "#classifier = SklearnClassifier(LinearSVC(tol=1e-6, max_iter=20000, class_weight='balanced', C=100, multi_class='ovr', random_state=0, loss='hinge'), sparse=False).train(train_set) \n",
    "#classifier = SklearnClassifier(LogisticRegression(), sparse=False).train(train_set)\n",
    "#classifier = SklearnClassifier(DecisionTreeClassifier(), sparse=False).train(train_set)\n",
    "classifier = SklearnClassifier(RandomForestClassifier(n_estimators=3, max_depth=10, random_state=0, criterion=\"entropy\"), sparse=False).train(train_set) \n",
    "#classifier = SklearnClassifier(KNeighborsClassifier(), sparse=False).train(train_set) \n",
    "\n",
    "cols = ['text']\n",
    "dt = pd.read_csv(\"../input/twitterdataset/test.csv\",header=None, names=cols)\n",
    "dtm = read_csv(\"../input/twitterdataset/testmatch2.csv\")\n",
    "dtm1 = dtm['tag']\n",
    "dtm2 = dtm['content']\n",
    "#print(classifier)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Predict\n",
    "tok = WordPunctTokenizer()\n",
    "pat1 = r'@[A-Za-z0-9]+'\n",
    "pat2 = r'https?://[A-Za-z0-9./]+'\n",
    "combined_pat = r'|'.join((pat1, pat2))\n",
    "\n",
    "\n",
    "def tweet_cleaner(text):\n",
    "    soup = BeautifulSoup(text, 'lxml')\n",
    "    souped = soup.get_text()\n",
    "    stripped = re.sub(combined_pat, '', souped)\n",
    "    try:\n",
    "        clean = stripped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
    "    except:\n",
    "        clean = stripped\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", clean)\n",
    "    lower_case = letters_only.lower()\n",
    "    words = tok.tokenize(lower_case)\n",
    "    return (\" \".join(words)).strip()\n",
    "\n",
    "\n",
    "testing = dt.text[:]\n",
    "stopWords = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "li=[]\n",
    "test_result = []\n",
    "wordsFiltered = []\n",
    "\n",
    "c=0\n",
    "n=0\n",
    "po=0\n",
    "sp=0\n",
    "cr=0\n",
    "na=0\n",
    "poli=0\n",
    "spor=0\n",
    "natu=0\n",
    "crim=0\n",
    "o=0\n",
    "\n",
    "counterz=0\n",
    "\n",
    "import numpy as np\n",
    "actual = []\n",
    "predicted = []\n",
    "\n",
    "for t in testing:\n",
    "    if counterz%20==0:\n",
    "        print(counterz)\n",
    "    counterz=counterz+1\n",
    "    pol=0\n",
    "    cri=0\n",
    "    nat=0\n",
    "    spo=0\n",
    "    pol2=0\n",
    "    cri2=0\n",
    "    nat2=0\n",
    "    spo2=0\n",
    "    #print(t)\n",
    "    \n",
    "    #print(wd)\n",
    "    b=tweet_cleaner(t)\n",
    "    #print(b)\n",
    "    wd = word_tokenize(b)\n",
    "    test_result.append(b)\n",
    "    words = word_tokenize(tweet_cleaner(t))\n",
    "    #print(words)\n",
    "    \n",
    "    for w in words:\n",
    "        if w not in stopWords:\n",
    "            wordsFiltered.append(w)\n",
    "    #print(wordsFiltered)\n",
    "    for word in wordsFiltered:\n",
    "        l=ps.stem(word)\n",
    "        li.append(l)\n",
    "    #print(li)\n",
    "    \n",
    "    tmp=[]\n",
    "    for i in li:\n",
    "        if i not in tmp:\n",
    "            if i!='b':\n",
    "                if len(i)==1:\n",
    "                    continue\n",
    "                elif len(i)<3:\n",
    "                    if i in wordss:\n",
    "                        tmp.append(correction(i))\n",
    "                else:\n",
    "                    tmp.append(correction(i))\n",
    "    \n",
    "    sentence = tmp\n",
    "    #print(sentence)\n",
    "    #sentence = sentence.lower()\n",
    "    #words = sentence.split(' ')\n",
    "    w=sentence\n",
    "    wordsFiltered=[]\n",
    "    for word in w:\n",
    "        classResult = classifier.classify( word_feats(word))\n",
    "        if classResult == 'pol':\n",
    "            pol = pol + 1\n",
    "        if classResult == 'spo':\n",
    "            spo = spo + 1\n",
    "        if classResult == 'nat':\n",
    "            nat = nat + 1\n",
    "        if classResult == 'cri':\n",
    "            cri = cri + 1\n",
    "    #print(neg)\n",
    "    #print(pos)\n",
    "    #print(li)\n",
    "    #print(len(words))\n",
    "\n",
    "    pol2=float(pol)/(len(wd))\n",
    "    spo2=float(spo)/(len(wd))\n",
    "    nat2=float(nat)/(len(wd))\n",
    "    cri2=float(cri)/(len(wd))\n",
    "\n",
    "    #print('Politics: ' + str(pol2))\n",
    "    #print('Sports: ' + str(spo2))\n",
    "    #print('Natural: ' + str(nat2))\n",
    "    #print('Crime: ' + str(cri2))\n",
    "    \n",
    "    \n",
    "    if dtm1[o]=='politics':\n",
    "        actual.append(0)\n",
    "    elif dtm1[o]=='sports':\n",
    "        actual.append(1)\n",
    "    elif dtm1[o]=='natural':\n",
    "        actual.append(2)\n",
    "    elif dtm1[o]=='crime':\n",
    "        actual.append(3)\n",
    "    \n",
    "    if pol2>=spo2 and pol2>=nat2 and pol2>=cri2:\n",
    "        po=po+1\n",
    "        predicted.append(0)\n",
    "        if dtm1[o]=='politics':\n",
    "            poli=poli+1\n",
    "            #print('politics')\n",
    "    elif spo2>=pol2 and spo2>=nat2 and spo2>=cri2:\n",
    "        sp=sp+1\n",
    "        predicted.append(1)\n",
    "        if dtm1[o]=='sports':\n",
    "            spor=spor+1\n",
    "            #print('sports')\n",
    "    elif nat2>=spo2 and nat2>=pol2 and nat2>=cri2:\n",
    "        na=na+1\n",
    "        predicted.append(2)\n",
    "        if dtm1[o]=='natural':\n",
    "            natu=natu+1\n",
    "            #print('natural')\n",
    "    elif cri2>=spo2 and cri2>=nat2 and cri2>=pol2:\n",
    "        cr=cr+1\n",
    "        predicted.append(3)\n",
    "        if dtm1[o]=='crime':\n",
    "            crim=crim+1\n",
    "            #print('crime')\n",
    "    \n",
    "    #print(li)\n",
    "    while len(li)>0:\n",
    "        li.pop()\n",
    "    \n",
    "    c=c+float(pol2+spo2+nat2+cri2)\n",
    "    n=n+1\n",
    "    o=o+1\n",
    "\n",
    "\n",
    "\n",
    "#np.save(\"actual.npy\", np.array(actual))\n",
    "#np.save(\"SVMpredicted.npy\", np.array(predicted))\n",
    "\n",
    "print(actual)\n",
    "print(predicted)\n",
    "\n",
    "politics_count=0\n",
    "sports_count=0\n",
    "natural_count=0\n",
    "crime_count=0\n",
    "for i in range(0, len(testing)):\n",
    "    if dtm1[i]=='politics':\n",
    "        politics_count=politics_count+1\n",
    "    elif dtm1[i]=='sports':\n",
    "        sports_count=sports_count+1\n",
    "    elif dtm1[i]=='natural':\n",
    "        natural_count=natural_count+1\n",
    "    elif dtm1[i]=='crime':\n",
    "        crime_count=crime_count+1\n",
    "print('Politics Count: ' + str(politics_count))\n",
    "print('Sports Count: ' + str(sports_count))\n",
    "print('Natural Count: ' + str(natural_count))\n",
    "print('Crime Count: ' + str(crime_count))\n",
    "print()\n",
    "\n",
    "\n",
    "\n",
    "ac=poli+spor+crim+natu\n",
    "print('Politics Predicted: ' + str(po))\n",
    "print('Sports Predicted: ' + str(sp))\n",
    "print('Natural Predicted: ' + str(na))\n",
    "print('Crime Predicted: ' + str(cr))\n",
    "\n",
    "print()\n",
    "print('Politics Correct: ' + str(poli))\n",
    "print('Sports Correct: ' + str(spor))\n",
    "print('Natural Correct: ' + str(natu))\n",
    "print('Crime Correct: ' + str(crim))\n",
    "print()\n",
    "\n",
    "print('Test Data: ' + str(n))\n",
    "print('Accuracy: ' + str(ac/n))\n",
    "end = time.time()\n",
    "print('Run Time: ' +str(end - start))# -*- coding: utf-8 -*-\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.101407,
     "end_time": "2020-10-20T18:36:01.671966",
     "exception": false,
     "start_time": "2020-10-20T18:36:01.570559",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Random Forest, n_estimators=7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-20T18:36:01.925185Z",
     "iopub.status.busy": "2020-10-20T18:36:01.903419Z",
     "iopub.status.idle": "2020-10-20T18:37:07.197304Z",
     "shell.execute_reply": "2020-10-20T18:37:07.198229Z"
    },
    "papermill": {
     "duration": 65.424134,
     "end_time": "2020-10-20T18:37:07.198476",
     "exception": false,
     "start_time": "2020-10-20T18:36:01.774342",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "120\n",
      "140\n",
      "160\n",
      "180\n",
      "[2, 0, 1, 3, 1, 2, 2, 2, 2, 1, 1, 1, 2, 1, 0, 2, 0, 0, 2, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 0, 0, 1, 2, 1, 0, 3, 1, 2, 1, 1, 1, 2, 1, 2, 1, 0, 1, 2, 0, 1, 1, 1, 1, 3, 0, 1, 2, 1, 2, 2, 0, 1, 3, 1, 0, 1, 0, 1, 2, 1, 0, 1, 0, 1, 2, 1, 2, 1, 0, 1, 2, 1, 2, 2, 2, 0, 0, 2, 2, 2, 2, 0, 2, 2, 1, 1, 1, 0, 1, 1, 2, 2, 2, 2, 1, 2, 2, 0, 2, 2, 3, 2, 2, 0, 0, 3, 2, 1, 3, 0, 3, 1, 2, 3, 0, 2, 0, 1, 0, 3, 2, 2, 1, 2, 1, 2, 1, 3, 0, 0, 2, 1, 0, 1, 2, 1, 1, 2, 0, 1, 3, 3, 2, 0, 0, 0, 3, 1, 2, 3, 2, 3, 1, 1, 2, 0, 2, 3, 1, 2, 3, 3, 2, 2, 1, 1, 1, 1, 2, 2, 2, 2, 1, 0, 2, 1, 1, 2, 1, 1, 1, 2, 1, 1, 2, 3, 2, 2, 1, 1]\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 1, 2, 1, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 1, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 1, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 2, 2, 2, 2, 1, 2]\n",
      "Politics Count: 36\n",
      "Sports Count: 75\n",
      "Natural Count: 69\n",
      "Crime Count: 20\n",
      "\n",
      "Politics Predicted: 1\n",
      "Sports Predicted: 21\n",
      "Natural Predicted: 178\n",
      "Crime Predicted: 0\n",
      "\n",
      "Politics Correct: 1\n",
      "Sports Correct: 21\n",
      "Natural Correct: 69\n",
      "Crime Correct: 0\n",
      "\n",
      "Test Data: 200\n",
      "Accuracy: 0.455\n",
      "Run Time: 1363.8473262786865\n"
     ]
    }
   ],
   "source": [
    "#classifier = NaiveBayesClassifier.train(train_set)\n",
    "##classifier = SklearnClassifier(LinearSVC(random_state=0), sparse=False).train(train_set) \n",
    "#classifier = SklearnClassifier(LinearSVC(tol=1e-6, max_iter=20000, class_weight='balanced', C=100, multi_class='ovr', random_state=0, loss='hinge'), sparse=False).train(train_set) \n",
    "#classifier = SklearnClassifier(LogisticRegression(), sparse=False).train(train_set)\n",
    "#classifier = SklearnClassifier(DecisionTreeClassifier(), sparse=False).train(train_set)\n",
    "classifier = SklearnClassifier(RandomForestClassifier(n_estimators=7, max_depth=10, random_state=0, criterion=\"entropy\"), sparse=False).train(train_set) \n",
    "#classifier = SklearnClassifier(KNeighborsClassifier(), sparse=False).train(train_set) \n",
    "\n",
    "cols = ['text']\n",
    "dt = pd.read_csv(\"../input/twitterdataset/test.csv\",header=None, names=cols)\n",
    "dtm = read_csv(\"../input/twitterdataset/testmatch2.csv\")\n",
    "dtm1 = dtm['tag']\n",
    "dtm2 = dtm['content']\n",
    "#print(classifier)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Predict\n",
    "tok = WordPunctTokenizer()\n",
    "pat1 = r'@[A-Za-z0-9]+'\n",
    "pat2 = r'https?://[A-Za-z0-9./]+'\n",
    "combined_pat = r'|'.join((pat1, pat2))\n",
    "\n",
    "\n",
    "def tweet_cleaner(text):\n",
    "    soup = BeautifulSoup(text, 'lxml')\n",
    "    souped = soup.get_text()\n",
    "    stripped = re.sub(combined_pat, '', souped)\n",
    "    try:\n",
    "        clean = stripped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
    "    except:\n",
    "        clean = stripped\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", clean)\n",
    "    lower_case = letters_only.lower()\n",
    "    words = tok.tokenize(lower_case)\n",
    "    return (\" \".join(words)).strip()\n",
    "\n",
    "\n",
    "testing = dt.text[:]\n",
    "stopWords = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "li=[]\n",
    "test_result = []\n",
    "wordsFiltered = []\n",
    "\n",
    "c=0\n",
    "n=0\n",
    "po=0\n",
    "sp=0\n",
    "cr=0\n",
    "na=0\n",
    "poli=0\n",
    "spor=0\n",
    "natu=0\n",
    "crim=0\n",
    "o=0\n",
    "\n",
    "counterz=0\n",
    "\n",
    "import numpy as np\n",
    "actual = []\n",
    "predicted = []\n",
    "\n",
    "for t in testing:\n",
    "    if counterz%20==0:\n",
    "        print(counterz)\n",
    "    counterz=counterz+1\n",
    "    pol=0\n",
    "    cri=0\n",
    "    nat=0\n",
    "    spo=0\n",
    "    pol2=0\n",
    "    cri2=0\n",
    "    nat2=0\n",
    "    spo2=0\n",
    "    #print(t)\n",
    "    \n",
    "    #print(wd)\n",
    "    b=tweet_cleaner(t)\n",
    "    #print(b)\n",
    "    wd = word_tokenize(b)\n",
    "    test_result.append(b)\n",
    "    words = word_tokenize(tweet_cleaner(t))\n",
    "    #print(words)\n",
    "    \n",
    "    for w in words:\n",
    "        if w not in stopWords:\n",
    "            wordsFiltered.append(w)\n",
    "    #print(wordsFiltered)\n",
    "    for word in wordsFiltered:\n",
    "        l=ps.stem(word)\n",
    "        li.append(l)\n",
    "    #print(li)\n",
    "    \n",
    "    tmp=[]\n",
    "    for i in li:\n",
    "        if i not in tmp:\n",
    "            if i!='b':\n",
    "                if len(i)==1:\n",
    "                    continue\n",
    "                elif len(i)<3:\n",
    "                    if i in wordss:\n",
    "                        tmp.append(correction(i))\n",
    "                else:\n",
    "                    tmp.append(correction(i))\n",
    "    \n",
    "    sentence = tmp\n",
    "    #print(sentence)\n",
    "    #sentence = sentence.lower()\n",
    "    #words = sentence.split(' ')\n",
    "    w=sentence\n",
    "    wordsFiltered=[]\n",
    "    for word in w:\n",
    "        classResult = classifier.classify( word_feats(word))\n",
    "        if classResult == 'pol':\n",
    "            pol = pol + 1\n",
    "        if classResult == 'spo':\n",
    "            spo = spo + 1\n",
    "        if classResult == 'nat':\n",
    "            nat = nat + 1\n",
    "        if classResult == 'cri':\n",
    "            cri = cri + 1\n",
    "    #print(neg)\n",
    "    #print(pos)\n",
    "    #print(li)\n",
    "    #print(len(words))\n",
    "\n",
    "    pol2=float(pol)/(len(wd))\n",
    "    spo2=float(spo)/(len(wd))\n",
    "    nat2=float(nat)/(len(wd))\n",
    "    cri2=float(cri)/(len(wd))\n",
    "\n",
    "    #print('Politics: ' + str(pol2))\n",
    "    #print('Sports: ' + str(spo2))\n",
    "    #print('Natural: ' + str(nat2))\n",
    "    #print('Crime: ' + str(cri2))\n",
    "    \n",
    "    \n",
    "    if dtm1[o]=='politics':\n",
    "        actual.append(0)\n",
    "    elif dtm1[o]=='sports':\n",
    "        actual.append(1)\n",
    "    elif dtm1[o]=='natural':\n",
    "        actual.append(2)\n",
    "    elif dtm1[o]=='crime':\n",
    "        actual.append(3)\n",
    "    \n",
    "    if pol2>=spo2 and pol2>=nat2 and pol2>=cri2:\n",
    "        po=po+1\n",
    "        predicted.append(0)\n",
    "        if dtm1[o]=='politics':\n",
    "            poli=poli+1\n",
    "            #print('politics')\n",
    "    elif spo2>=pol2 and spo2>=nat2 and spo2>=cri2:\n",
    "        sp=sp+1\n",
    "        predicted.append(1)\n",
    "        if dtm1[o]=='sports':\n",
    "            spor=spor+1\n",
    "            #print('sports')\n",
    "    elif nat2>=spo2 and nat2>=pol2 and nat2>=cri2:\n",
    "        na=na+1\n",
    "        predicted.append(2)\n",
    "        if dtm1[o]=='natural':\n",
    "            natu=natu+1\n",
    "            #print('natural')\n",
    "    elif cri2>=spo2 and cri2>=nat2 and cri2>=pol2:\n",
    "        cr=cr+1\n",
    "        predicted.append(3)\n",
    "        if dtm1[o]=='crime':\n",
    "            crim=crim+1\n",
    "            #print('crime')\n",
    "    \n",
    "    #print(li)\n",
    "    while len(li)>0:\n",
    "        li.pop()\n",
    "    \n",
    "    c=c+float(pol2+spo2+nat2+cri2)\n",
    "    n=n+1\n",
    "    o=o+1\n",
    "\n",
    "\n",
    "\n",
    "#np.save(\"actual.npy\", np.array(actual))\n",
    "#np.save(\"SVMpredicted.npy\", np.array(predicted))\n",
    "\n",
    "print(actual)\n",
    "print(predicted)\n",
    "\n",
    "politics_count=0\n",
    "sports_count=0\n",
    "natural_count=0\n",
    "crime_count=0\n",
    "for i in range(0, len(testing)):\n",
    "    if dtm1[i]=='politics':\n",
    "        politics_count=politics_count+1\n",
    "    elif dtm1[i]=='sports':\n",
    "        sports_count=sports_count+1\n",
    "    elif dtm1[i]=='natural':\n",
    "        natural_count=natural_count+1\n",
    "    elif dtm1[i]=='crime':\n",
    "        crime_count=crime_count+1\n",
    "print('Politics Count: ' + str(politics_count))\n",
    "print('Sports Count: ' + str(sports_count))\n",
    "print('Natural Count: ' + str(natural_count))\n",
    "print('Crime Count: ' + str(crime_count))\n",
    "print()\n",
    "\n",
    "\n",
    "\n",
    "ac=poli+spor+crim+natu\n",
    "print('Politics Predicted: ' + str(po))\n",
    "print('Sports Predicted: ' + str(sp))\n",
    "print('Natural Predicted: ' + str(na))\n",
    "print('Crime Predicted: ' + str(cr))\n",
    "\n",
    "print()\n",
    "print('Politics Correct: ' + str(poli))\n",
    "print('Sports Correct: ' + str(spor))\n",
    "print('Natural Correct: ' + str(natu))\n",
    "print('Crime Correct: ' + str(crim))\n",
    "print()\n",
    "\n",
    "print('Test Data: ' + str(n))\n",
    "print('Accuracy: ' + str(ac/n))\n",
    "end = time.time()\n",
    "print('Run Time: ' +str(end - start))# -*- coding: utf-8 -*-\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.105871,
     "end_time": "2020-10-20T18:37:07.412917",
     "exception": false,
     "start_time": "2020-10-20T18:37:07.307046",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Random Forest, max_depth=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-20T18:37:07.693234Z",
     "iopub.status.busy": "2020-10-20T18:37:07.662297Z",
     "iopub.status.idle": "2020-10-20T18:38:12.554189Z",
     "shell.execute_reply": "2020-10-20T18:38:12.554921Z"
    },
    "papermill": {
     "duration": 65.036595,
     "end_time": "2020-10-20T18:38:12.555116",
     "exception": false,
     "start_time": "2020-10-20T18:37:07.518521",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "120\n",
      "140\n",
      "160\n",
      "180\n",
      "[2, 0, 1, 3, 1, 2, 2, 2, 2, 1, 1, 1, 2, 1, 0, 2, 0, 0, 2, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 0, 0, 1, 2, 1, 0, 3, 1, 2, 1, 1, 1, 2, 1, 2, 1, 0, 1, 2, 0, 1, 1, 1, 1, 3, 0, 1, 2, 1, 2, 2, 0, 1, 3, 1, 0, 1, 0, 1, 2, 1, 0, 1, 0, 1, 2, 1, 2, 1, 0, 1, 2, 1, 2, 2, 2, 0, 0, 2, 2, 2, 2, 0, 2, 2, 1, 1, 1, 0, 1, 1, 2, 2, 2, 2, 1, 2, 2, 0, 2, 2, 3, 2, 2, 0, 0, 3, 2, 1, 3, 0, 3, 1, 2, 3, 0, 2, 0, 1, 0, 3, 2, 2, 1, 2, 1, 2, 1, 3, 0, 0, 2, 1, 0, 1, 2, 1, 1, 2, 0, 1, 3, 3, 2, 0, 0, 0, 3, 1, 2, 3, 2, 3, 1, 1, 2, 0, 2, 3, 1, 2, 3, 3, 2, 2, 1, 1, 1, 1, 2, 2, 2, 2, 1, 0, 2, 1, 1, 2, 1, 1, 1, 2, 1, 1, 2, 3, 2, 2, 1, 1]\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "Politics Count: 36\n",
      "Sports Count: 75\n",
      "Natural Count: 69\n",
      "Crime Count: 20\n",
      "\n",
      "Politics Predicted: 0\n",
      "Sports Predicted: 0\n",
      "Natural Predicted: 200\n",
      "Crime Predicted: 0\n",
      "\n",
      "Politics Correct: 0\n",
      "Sports Correct: 0\n",
      "Natural Correct: 69\n",
      "Crime Correct: 0\n",
      "\n",
      "Test Data: 200\n",
      "Accuracy: 0.345\n",
      "Run Time: 1429.203549861908\n"
     ]
    }
   ],
   "source": [
    "#classifier = NaiveBayesClassifier.train(train_set)\n",
    "##classifier = SklearnClassifier(LinearSVC(random_state=0), sparse=False).train(train_set) \n",
    "#classifier = SklearnClassifier(LinearSVC(tol=1e-6, max_iter=20000, class_weight='balanced', C=100, multi_class='ovr', random_state=0, loss='hinge'), sparse=False).train(train_set) \n",
    "#classifier = SklearnClassifier(LogisticRegression(), sparse=False).train(train_set)\n",
    "#classifier = SklearnClassifier(DecisionTreeClassifier(), sparse=False).train(train_set)\n",
    "classifier = SklearnClassifier(RandomForestClassifier(n_estimators=5, max_depth=5, random_state=0, criterion=\"entropy\"), sparse=False).train(train_set) \n",
    "#classifier = SklearnClassifier(KNeighborsClassifier(), sparse=False).train(train_set) \n",
    "\n",
    "cols = ['text']\n",
    "dt = pd.read_csv(\"../input/twitterdataset/test.csv\",header=None, names=cols)\n",
    "dtm = read_csv(\"../input/twitterdataset/testmatch2.csv\")\n",
    "dtm1 = dtm['tag']\n",
    "dtm2 = dtm['content']\n",
    "#print(classifier)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Predict\n",
    "tok = WordPunctTokenizer()\n",
    "pat1 = r'@[A-Za-z0-9]+'\n",
    "pat2 = r'https?://[A-Za-z0-9./]+'\n",
    "combined_pat = r'|'.join((pat1, pat2))\n",
    "\n",
    "\n",
    "def tweet_cleaner(text):\n",
    "    soup = BeautifulSoup(text, 'lxml')\n",
    "    souped = soup.get_text()\n",
    "    stripped = re.sub(combined_pat, '', souped)\n",
    "    try:\n",
    "        clean = stripped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
    "    except:\n",
    "        clean = stripped\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", clean)\n",
    "    lower_case = letters_only.lower()\n",
    "    words = tok.tokenize(lower_case)\n",
    "    return (\" \".join(words)).strip()\n",
    "\n",
    "\n",
    "testing = dt.text[:]\n",
    "stopWords = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "li=[]\n",
    "test_result = []\n",
    "wordsFiltered = []\n",
    "\n",
    "c=0\n",
    "n=0\n",
    "po=0\n",
    "sp=0\n",
    "cr=0\n",
    "na=0\n",
    "poli=0\n",
    "spor=0\n",
    "natu=0\n",
    "crim=0\n",
    "o=0\n",
    "\n",
    "counterz=0\n",
    "\n",
    "import numpy as np\n",
    "actual = []\n",
    "predicted = []\n",
    "\n",
    "for t in testing:\n",
    "    if counterz%20==0:\n",
    "        print(counterz)\n",
    "    counterz=counterz+1\n",
    "    pol=0\n",
    "    cri=0\n",
    "    nat=0\n",
    "    spo=0\n",
    "    pol2=0\n",
    "    cri2=0\n",
    "    nat2=0\n",
    "    spo2=0\n",
    "    #print(t)\n",
    "    \n",
    "    #print(wd)\n",
    "    b=tweet_cleaner(t)\n",
    "    #print(b)\n",
    "    wd = word_tokenize(b)\n",
    "    test_result.append(b)\n",
    "    words = word_tokenize(tweet_cleaner(t))\n",
    "    #print(words)\n",
    "    \n",
    "    for w in words:\n",
    "        if w not in stopWords:\n",
    "            wordsFiltered.append(w)\n",
    "    #print(wordsFiltered)\n",
    "    for word in wordsFiltered:\n",
    "        l=ps.stem(word)\n",
    "        li.append(l)\n",
    "    #print(li)\n",
    "    \n",
    "    tmp=[]\n",
    "    for i in li:\n",
    "        if i not in tmp:\n",
    "            if i!='b':\n",
    "                if len(i)==1:\n",
    "                    continue\n",
    "                elif len(i)<3:\n",
    "                    if i in wordss:\n",
    "                        tmp.append(correction(i))\n",
    "                else:\n",
    "                    tmp.append(correction(i))\n",
    "    \n",
    "    sentence = tmp\n",
    "    #print(sentence)\n",
    "    #sentence = sentence.lower()\n",
    "    #words = sentence.split(' ')\n",
    "    w=sentence\n",
    "    wordsFiltered=[]\n",
    "    for word in w:\n",
    "        classResult = classifier.classify( word_feats(word))\n",
    "        if classResult == 'pol':\n",
    "            pol = pol + 1\n",
    "        if classResult == 'spo':\n",
    "            spo = spo + 1\n",
    "        if classResult == 'nat':\n",
    "            nat = nat + 1\n",
    "        if classResult == 'cri':\n",
    "            cri = cri + 1\n",
    "    #print(neg)\n",
    "    #print(pos)\n",
    "    #print(li)\n",
    "    #print(len(words))\n",
    "\n",
    "    pol2=float(pol)/(len(wd))\n",
    "    spo2=float(spo)/(len(wd))\n",
    "    nat2=float(nat)/(len(wd))\n",
    "    cri2=float(cri)/(len(wd))\n",
    "\n",
    "    #print('Politics: ' + str(pol2))\n",
    "    #print('Sports: ' + str(spo2))\n",
    "    #print('Natural: ' + str(nat2))\n",
    "    #print('Crime: ' + str(cri2))\n",
    "    \n",
    "    \n",
    "    if dtm1[o]=='politics':\n",
    "        actual.append(0)\n",
    "    elif dtm1[o]=='sports':\n",
    "        actual.append(1)\n",
    "    elif dtm1[o]=='natural':\n",
    "        actual.append(2)\n",
    "    elif dtm1[o]=='crime':\n",
    "        actual.append(3)\n",
    "    \n",
    "    if pol2>=spo2 and pol2>=nat2 and pol2>=cri2:\n",
    "        po=po+1\n",
    "        predicted.append(0)\n",
    "        if dtm1[o]=='politics':\n",
    "            poli=poli+1\n",
    "            #print('politics')\n",
    "    elif spo2>=pol2 and spo2>=nat2 and spo2>=cri2:\n",
    "        sp=sp+1\n",
    "        predicted.append(1)\n",
    "        if dtm1[o]=='sports':\n",
    "            spor=spor+1\n",
    "            #print('sports')\n",
    "    elif nat2>=spo2 and nat2>=pol2 and nat2>=cri2:\n",
    "        na=na+1\n",
    "        predicted.append(2)\n",
    "        if dtm1[o]=='natural':\n",
    "            natu=natu+1\n",
    "            #print('natural')\n",
    "    elif cri2>=spo2 and cri2>=nat2 and cri2>=pol2:\n",
    "        cr=cr+1\n",
    "        predicted.append(3)\n",
    "        if dtm1[o]=='crime':\n",
    "            crim=crim+1\n",
    "            #print('crime')\n",
    "    \n",
    "    #print(li)\n",
    "    while len(li)>0:\n",
    "        li.pop()\n",
    "    \n",
    "    c=c+float(pol2+spo2+nat2+cri2)\n",
    "    n=n+1\n",
    "    o=o+1\n",
    "\n",
    "\n",
    "\n",
    "#np.save(\"actual.npy\", np.array(actual))\n",
    "#np.save(\"SVMpredicted.npy\", np.array(predicted))\n",
    "\n",
    "print(actual)\n",
    "print(predicted)\n",
    "\n",
    "politics_count=0\n",
    "sports_count=0\n",
    "natural_count=0\n",
    "crime_count=0\n",
    "for i in range(0, len(testing)):\n",
    "    if dtm1[i]=='politics':\n",
    "        politics_count=politics_count+1\n",
    "    elif dtm1[i]=='sports':\n",
    "        sports_count=sports_count+1\n",
    "    elif dtm1[i]=='natural':\n",
    "        natural_count=natural_count+1\n",
    "    elif dtm1[i]=='crime':\n",
    "        crime_count=crime_count+1\n",
    "print('Politics Count: ' + str(politics_count))\n",
    "print('Sports Count: ' + str(sports_count))\n",
    "print('Natural Count: ' + str(natural_count))\n",
    "print('Crime Count: ' + str(crime_count))\n",
    "print()\n",
    "\n",
    "\n",
    "\n",
    "ac=poli+spor+crim+natu\n",
    "print('Politics Predicted: ' + str(po))\n",
    "print('Sports Predicted: ' + str(sp))\n",
    "print('Natural Predicted: ' + str(na))\n",
    "print('Crime Predicted: ' + str(cr))\n",
    "\n",
    "print()\n",
    "print('Politics Correct: ' + str(poli))\n",
    "print('Sports Correct: ' + str(spor))\n",
    "print('Natural Correct: ' + str(natu))\n",
    "print('Crime Correct: ' + str(crim))\n",
    "print()\n",
    "\n",
    "print('Test Data: ' + str(n))\n",
    "print('Accuracy: ' + str(ac/n))\n",
    "end = time.time()\n",
    "print('Run Time: ' +str(end - start))# -*- coding: utf-8 -*-\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.110569,
     "end_time": "2020-10-20T18:38:12.778603",
     "exception": false,
     "start_time": "2020-10-20T18:38:12.668034",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Random Forest, max_depth=15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-20T18:38:13.022753Z",
     "iopub.status.busy": "2020-10-20T18:38:13.017192Z",
     "iopub.status.idle": "2020-10-20T18:39:17.622116Z",
     "shell.execute_reply": "2020-10-20T18:39:17.622814Z"
    },
    "papermill": {
     "duration": 64.733213,
     "end_time": "2020-10-20T18:39:17.623002",
     "exception": false,
     "start_time": "2020-10-20T18:38:12.889789",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "120\n",
      "140\n",
      "160\n",
      "180\n",
      "[2, 0, 1, 3, 1, 2, 2, 2, 2, 1, 1, 1, 2, 1, 0, 2, 0, 0, 2, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 0, 0, 1, 2, 1, 0, 3, 1, 2, 1, 1, 1, 2, 1, 2, 1, 0, 1, 2, 0, 1, 1, 1, 1, 3, 0, 1, 2, 1, 2, 2, 0, 1, 3, 1, 0, 1, 0, 1, 2, 1, 0, 1, 0, 1, 2, 1, 2, 1, 0, 1, 2, 1, 2, 2, 2, 0, 0, 2, 2, 2, 2, 0, 2, 2, 1, 1, 1, 0, 1, 1, 2, 2, 2, 2, 1, 2, 2, 0, 2, 2, 3, 2, 2, 0, 0, 3, 2, 1, 3, 0, 3, 1, 2, 3, 0, 2, 0, 1, 0, 3, 2, 2, 1, 2, 1, 2, 1, 3, 0, 0, 2, 1, 0, 1, 2, 1, 1, 2, 0, 1, 3, 3, 2, 0, 0, 0, 3, 1, 2, 3, 2, 3, 1, 1, 2, 0, 2, 3, 1, 2, 3, 3, 2, 2, 1, 1, 1, 1, 2, 2, 2, 2, 1, 0, 2, 1, 1, 2, 1, 1, 1, 2, 1, 1, 2, 3, 2, 2, 1, 1]\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 1, 2, 2, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 1, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 1, 2, 1, 2, 2, 1, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 2, 2, 2, 2, 1, 2]\n",
      "Politics Count: 36\n",
      "Sports Count: 75\n",
      "Natural Count: 69\n",
      "Crime Count: 20\n",
      "\n",
      "Politics Predicted: 1\n",
      "Sports Predicted: 23\n",
      "Natural Predicted: 176\n",
      "Crime Predicted: 0\n",
      "\n",
      "Politics Correct: 1\n",
      "Sports Correct: 23\n",
      "Natural Correct: 69\n",
      "Crime Correct: 0\n",
      "\n",
      "Test Data: 200\n",
      "Accuracy: 0.465\n",
      "Run Time: 1494.2668235301971\n"
     ]
    }
   ],
   "source": [
    "#classifier = NaiveBayesClassifier.train(train_set)\n",
    "##classifier = SklearnClassifier(LinearSVC(random_state=0), sparse=False).train(train_set) \n",
    "#classifier = SklearnClassifier(LinearSVC(tol=1e-6, max_iter=20000, class_weight='balanced', C=100, multi_class='ovr', random_state=0, loss='hinge'), sparse=False).train(train_set) \n",
    "#classifier = SklearnClassifier(LogisticRegression(), sparse=False).train(train_set)\n",
    "#classifier = SklearnClassifier(DecisionTreeClassifier(), sparse=False).train(train_set)\n",
    "classifier = SklearnClassifier(RandomForestClassifier(n_estimators=5, max_depth=15, random_state=0, criterion=\"entropy\"), sparse=False).train(train_set) \n",
    "#classifier = SklearnClassifier(KNeighborsClassifier(), sparse=False).train(train_set) \n",
    "\n",
    "cols = ['text']\n",
    "dt = pd.read_csv(\"../input/twitterdataset/test.csv\",header=None, names=cols)\n",
    "dtm = read_csv(\"../input/twitterdataset/testmatch2.csv\")\n",
    "dtm1 = dtm['tag']\n",
    "dtm2 = dtm['content']\n",
    "#print(classifier)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Predict\n",
    "tok = WordPunctTokenizer()\n",
    "pat1 = r'@[A-Za-z0-9]+'\n",
    "pat2 = r'https?://[A-Za-z0-9./]+'\n",
    "combined_pat = r'|'.join((pat1, pat2))\n",
    "\n",
    "\n",
    "def tweet_cleaner(text):\n",
    "    soup = BeautifulSoup(text, 'lxml')\n",
    "    souped = soup.get_text()\n",
    "    stripped = re.sub(combined_pat, '', souped)\n",
    "    try:\n",
    "        clean = stripped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
    "    except:\n",
    "        clean = stripped\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", clean)\n",
    "    lower_case = letters_only.lower()\n",
    "    words = tok.tokenize(lower_case)\n",
    "    return (\" \".join(words)).strip()\n",
    "\n",
    "\n",
    "testing = dt.text[:]\n",
    "stopWords = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "li=[]\n",
    "test_result = []\n",
    "wordsFiltered = []\n",
    "\n",
    "c=0\n",
    "n=0\n",
    "po=0\n",
    "sp=0\n",
    "cr=0\n",
    "na=0\n",
    "poli=0\n",
    "spor=0\n",
    "natu=0\n",
    "crim=0\n",
    "o=0\n",
    "\n",
    "counterz=0\n",
    "\n",
    "import numpy as np\n",
    "actual = []\n",
    "predicted = []\n",
    "\n",
    "for t in testing:\n",
    "    if counterz%20==0:\n",
    "        print(counterz)\n",
    "    counterz=counterz+1\n",
    "    pol=0\n",
    "    cri=0\n",
    "    nat=0\n",
    "    spo=0\n",
    "    pol2=0\n",
    "    cri2=0\n",
    "    nat2=0\n",
    "    spo2=0\n",
    "    #print(t)\n",
    "    \n",
    "    #print(wd)\n",
    "    b=tweet_cleaner(t)\n",
    "    #print(b)\n",
    "    wd = word_tokenize(b)\n",
    "    test_result.append(b)\n",
    "    words = word_tokenize(tweet_cleaner(t))\n",
    "    #print(words)\n",
    "    \n",
    "    for w in words:\n",
    "        if w not in stopWords:\n",
    "            wordsFiltered.append(w)\n",
    "    #print(wordsFiltered)\n",
    "    for word in wordsFiltered:\n",
    "        l=ps.stem(word)\n",
    "        li.append(l)\n",
    "    #print(li)\n",
    "    \n",
    "    tmp=[]\n",
    "    for i in li:\n",
    "        if i not in tmp:\n",
    "            if i!='b':\n",
    "                if len(i)==1:\n",
    "                    continue\n",
    "                elif len(i)<3:\n",
    "                    if i in wordss:\n",
    "                        tmp.append(correction(i))\n",
    "                else:\n",
    "                    tmp.append(correction(i))\n",
    "    \n",
    "    sentence = tmp\n",
    "    #print(sentence)\n",
    "    #sentence = sentence.lower()\n",
    "    #words = sentence.split(' ')\n",
    "    w=sentence\n",
    "    wordsFiltered=[]\n",
    "    for word in w:\n",
    "        classResult = classifier.classify( word_feats(word))\n",
    "        if classResult == 'pol':\n",
    "            pol = pol + 1\n",
    "        if classResult == 'spo':\n",
    "            spo = spo + 1\n",
    "        if classResult == 'nat':\n",
    "            nat = nat + 1\n",
    "        if classResult == 'cri':\n",
    "            cri = cri + 1\n",
    "    #print(neg)\n",
    "    #print(pos)\n",
    "    #print(li)\n",
    "    #print(len(words))\n",
    "\n",
    "    pol2=float(pol)/(len(wd))\n",
    "    spo2=float(spo)/(len(wd))\n",
    "    nat2=float(nat)/(len(wd))\n",
    "    cri2=float(cri)/(len(wd))\n",
    "\n",
    "    #print('Politics: ' + str(pol2))\n",
    "    #print('Sports: ' + str(spo2))\n",
    "    #print('Natural: ' + str(nat2))\n",
    "    #print('Crime: ' + str(cri2))\n",
    "    \n",
    "    \n",
    "    if dtm1[o]=='politics':\n",
    "        actual.append(0)\n",
    "    elif dtm1[o]=='sports':\n",
    "        actual.append(1)\n",
    "    elif dtm1[o]=='natural':\n",
    "        actual.append(2)\n",
    "    elif dtm1[o]=='crime':\n",
    "        actual.append(3)\n",
    "    \n",
    "    if pol2>=spo2 and pol2>=nat2 and pol2>=cri2:\n",
    "        po=po+1\n",
    "        predicted.append(0)\n",
    "        if dtm1[o]=='politics':\n",
    "            poli=poli+1\n",
    "            #print('politics')\n",
    "    elif spo2>=pol2 and spo2>=nat2 and spo2>=cri2:\n",
    "        sp=sp+1\n",
    "        predicted.append(1)\n",
    "        if dtm1[o]=='sports':\n",
    "            spor=spor+1\n",
    "            #print('sports')\n",
    "    elif nat2>=spo2 and nat2>=pol2 and nat2>=cri2:\n",
    "        na=na+1\n",
    "        predicted.append(2)\n",
    "        if dtm1[o]=='natural':\n",
    "            natu=natu+1\n",
    "            #print('natural')\n",
    "    elif cri2>=spo2 and cri2>=nat2 and cri2>=pol2:\n",
    "        cr=cr+1\n",
    "        predicted.append(3)\n",
    "        if dtm1[o]=='crime':\n",
    "            crim=crim+1\n",
    "            #print('crime')\n",
    "    \n",
    "    #print(li)\n",
    "    while len(li)>0:\n",
    "        li.pop()\n",
    "    \n",
    "    c=c+float(pol2+spo2+nat2+cri2)\n",
    "    n=n+1\n",
    "    o=o+1\n",
    "\n",
    "\n",
    "\n",
    "#np.save(\"actual.npy\", np.array(actual))\n",
    "#np.save(\"SVMpredicted.npy\", np.array(predicted))\n",
    "\n",
    "print(actual)\n",
    "print(predicted)\n",
    "\n",
    "politics_count=0\n",
    "sports_count=0\n",
    "natural_count=0\n",
    "crime_count=0\n",
    "for i in range(0, len(testing)):\n",
    "    if dtm1[i]=='politics':\n",
    "        politics_count=politics_count+1\n",
    "    elif dtm1[i]=='sports':\n",
    "        sports_count=sports_count+1\n",
    "    elif dtm1[i]=='natural':\n",
    "        natural_count=natural_count+1\n",
    "    elif dtm1[i]=='crime':\n",
    "        crime_count=crime_count+1\n",
    "print('Politics Count: ' + str(politics_count))\n",
    "print('Sports Count: ' + str(sports_count))\n",
    "print('Natural Count: ' + str(natural_count))\n",
    "print('Crime Count: ' + str(crime_count))\n",
    "print()\n",
    "\n",
    "\n",
    "\n",
    "ac=poli+spor+crim+natu\n",
    "print('Politics Predicted: ' + str(po))\n",
    "print('Sports Predicted: ' + str(sp))\n",
    "print('Natural Predicted: ' + str(na))\n",
    "print('Crime Predicted: ' + str(cr))\n",
    "\n",
    "print()\n",
    "print('Politics Correct: ' + str(poli))\n",
    "print('Sports Correct: ' + str(spor))\n",
    "print('Natural Correct: ' + str(natu))\n",
    "print('Crime Correct: ' + str(crim))\n",
    "print()\n",
    "\n",
    "print('Test Data: ' + str(n))\n",
    "print('Accuracy: ' + str(ac/n))\n",
    "end = time.time()\n",
    "print('Run Time: ' +str(end - start))# -*- coding: utf-8 -*-\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.115341,
     "end_time": "2020-10-20T18:39:17.854220",
     "exception": false,
     "start_time": "2020-10-20T18:39:17.738879",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-20T18:39:18.103351Z",
     "iopub.status.busy": "2020-10-20T18:39:18.097975Z",
     "iopub.status.idle": "2020-10-20T18:40:53.445059Z",
     "shell.execute_reply": "2020-10-20T18:40:53.444371Z"
    },
    "papermill": {
     "duration": 95.474312,
     "end_time": "2020-10-20T18:40:53.445189",
     "exception": false,
     "start_time": "2020-10-20T18:39:17.970877",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "120\n",
      "140\n",
      "160\n",
      "180\n",
      "[2, 0, 1, 3, 1, 2, 2, 2, 2, 1, 0, 0, 3, 2, 3, 2, 1, 0, 2, 0, 1, 0, 2, 0, 2, 0, 2, 0, 1, 0, 1, 2, 1, 2, 1, 2, 1, 0, 1, 2, 1, 2, 1, 2, 1, 2, 0, 2, 0, 2, 1, 2, 1, 2, 1, 2, 1, 0, 2, 1, 2, 1, 2, 1, 2, 0, 2, 0, 3, 1, 2, 3, 0, 1, 2, 0, 2, 1, 2, 1, 2, 1, 0, 1, 0, 2, 0, 1, 0, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 0, 1, 0, 1, 0, 3, 0, 1, 0, 1, 0, 1, 3, 1, 2, 1, 2, 1, 0, 1, 2, 1, 0, 1, 2, 1, 0, 1, 2, 1, 2, 1, 2, 1, 0, 1, 0, 1, 2, 1, 0, 1, 0, 1, 2, 1, 2, 1, 0, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 0, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 0, 1, 0, 2, 1, 2, 1, 2, 1, 2, 1, 0, 1, 2, 1, 2, 0, 2, 0]\n",
      "[2, 0, 1, 3, 1, 0, 2, 0, 0, 1, 0, 0, 3, 2, 3, 2, 1, 0, 2, 0, 1, 0, 2, 0, 2, 0, 2, 0, 1, 0, 1, 2, 1, 2, 1, 2, 1, 0, 1, 2, 1, 2, 1, 2, 1, 2, 0, 1, 0, 2, 1, 2, 1, 2, 1, 2, 1, 0, 2, 1, 2, 1, 2, 1, 2, 0, 2, 0, 0, 1, 2, 3, 0, 1, 2, 0, 2, 1, 2, 1, 1, 1, 0, 1, 0, 2, 0, 1, 0, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 0, 1, 0, 1, 0, 3, 0, 1, 0, 1, 0, 1, 3, 1, 2, 1, 2, 1, 0, 1, 2, 1, 0, 1, 2, 1, 0, 1, 2, 1, 2, 1, 2, 1, 0, 1, 0, 1, 2, 1, 0, 1, 0, 1, 2, 1, 1, 1, 0, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 0, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 0, 1, 0, 2, 1, 2, 1, 2, 1, 2, 1, 0, 1, 2, 1, 2, 0, 2, 0]\n",
      "Politics Count: 42\n",
      "Sports Count: 80\n",
      "Natural Count: 71\n",
      "Crime Count: 7\n",
      "\n",
      "Politics Predicted: 46\n",
      "Sports Predicted: 83\n",
      "Natural Predicted: 65\n",
      "Crime Predicted: 6\n",
      "\n",
      "Politics Correct: 42\n",
      "Sports Correct: 80\n",
      "Natural Correct: 65\n",
      "Crime Correct: 6\n",
      "\n",
      "Test Data: 200\n",
      "Accuracy: 0.965\n",
      "Run Time: 1590.2026579380035\n"
     ]
    }
   ],
   "source": [
    "#classifier = NaiveBayesClassifier.train(train_set)\n",
    "##classifier = SklearnClassifier(LinearSVC(random_state=0), sparse=False).train(train_set) \n",
    "#classifier = SklearnClassifier(LinearSVC(tol=1e-6, max_iter=20000, class_weight='balanced', C=100, multi_class='ovr', random_state=0, loss='hinge'), sparse=False).train(train_set) \n",
    "#classifier = SklearnClassifier(LogisticRegression(), sparse=False).train(train_set)\n",
    "#classifier = SklearnClassifier(DecisionTreeClassifier(), sparse=False).train(train_set)\n",
    "#classifier = SklearnClassifier(RandomForestClassifier(n_estimators=5, max_depth=10, random_state=0, criterion=\"entropy\"), sparse=False).train(train_set) \n",
    "classifier = SklearnClassifier(KNeighborsClassifier(), sparse=False).train(train_set) \n",
    "\n",
    "cols = ['text']\n",
    "dt = pd.read_csv(\"../input/twitterdataset/testv2.csv\",header=None, names=cols)\n",
    "dtm = read_csv(\"../input/twitterdataset/testmatch2v2.csv\")\n",
    "dtm1 = dtm['tag']\n",
    "dtm2 = dtm['content']\n",
    "#print(classifier)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Predict\n",
    "tok = WordPunctTokenizer()\n",
    "pat1 = r'@[A-Za-z0-9]+'\n",
    "pat2 = r'https?://[A-Za-z0-9./]+'\n",
    "combined_pat = r'|'.join((pat1, pat2))\n",
    "\n",
    "\n",
    "def tweet_cleaner(text):\n",
    "    soup = BeautifulSoup(text, 'lxml')\n",
    "    souped = soup.get_text()\n",
    "    stripped = re.sub(combined_pat, '', souped)\n",
    "    try:\n",
    "        clean = stripped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
    "    except:\n",
    "        clean = stripped\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", clean)\n",
    "    lower_case = letters_only.lower()\n",
    "    words = tok.tokenize(lower_case)\n",
    "    return (\" \".join(words)).strip()\n",
    "\n",
    "\n",
    "testing = dt.text[:]\n",
    "stopWords = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "li=[]\n",
    "test_result = []\n",
    "wordsFiltered = []\n",
    "\n",
    "c=0\n",
    "n=0\n",
    "po=0\n",
    "sp=0\n",
    "cr=0\n",
    "na=0\n",
    "poli=0\n",
    "spor=0\n",
    "natu=0\n",
    "crim=0\n",
    "o=0\n",
    "\n",
    "counterz=0\n",
    "\n",
    "import numpy as np\n",
    "actual = []\n",
    "predicted = []\n",
    "\n",
    "for t in testing:\n",
    "    if counterz%20==0:\n",
    "        print(counterz)\n",
    "    counterz=counterz+1\n",
    "    pol=0\n",
    "    cri=0\n",
    "    nat=0\n",
    "    spo=0\n",
    "    pol2=0\n",
    "    cri2=0\n",
    "    nat2=0\n",
    "    spo2=0\n",
    "    #print(t)\n",
    "    \n",
    "    #print(wd)\n",
    "    b=tweet_cleaner(t)\n",
    "    #print(b)\n",
    "    wd = word_tokenize(b)\n",
    "    test_result.append(b)\n",
    "    words = word_tokenize(tweet_cleaner(t))\n",
    "    #print(words)\n",
    "    \n",
    "    for w in words:\n",
    "        if w not in stopWords:\n",
    "            wordsFiltered.append(w)\n",
    "    #print(wordsFiltered)\n",
    "    for word in wordsFiltered:\n",
    "        l=ps.stem(word)\n",
    "        li.append(l)\n",
    "    #print(li)\n",
    "    \n",
    "    tmp=[]\n",
    "    for i in li:\n",
    "        if i not in tmp:\n",
    "            if i!='b':\n",
    "                if len(i)==1:\n",
    "                    continue\n",
    "                elif len(i)<3:\n",
    "                    if i in wordss:\n",
    "                        tmp.append(correction(i))\n",
    "                else:\n",
    "                    tmp.append(correction(i))\n",
    "    \n",
    "    sentence = tmp\n",
    "    #print(sentence)\n",
    "    #sentence = sentence.lower()\n",
    "    #words = sentence.split(' ')\n",
    "    w=sentence\n",
    "    wordsFiltered=[]\n",
    "    for word in w:\n",
    "        classResult = classifier.classify( word_feats(word))\n",
    "        if classResult == 'pol':\n",
    "            pol = pol + 1\n",
    "        if classResult == 'spo':\n",
    "            spo = spo + 1\n",
    "        if classResult == 'nat':\n",
    "            nat = nat + 1\n",
    "        if classResult == 'cri':\n",
    "            cri = cri + 1\n",
    "    #print(neg)\n",
    "    #print(pos)\n",
    "    #print(li)\n",
    "    #print(len(words))\n",
    "\n",
    "    pol2=float(pol)/(len(wd))\n",
    "    spo2=float(spo)/(len(wd))\n",
    "    nat2=float(nat)/(len(wd))\n",
    "    cri2=float(cri)/(len(wd))\n",
    "\n",
    "    #print('Politics: ' + str(pol2))\n",
    "    #print('Sports: ' + str(spo2))\n",
    "    #print('Natural: ' + str(nat2))\n",
    "    #print('Crime: ' + str(cri2))\n",
    "    \n",
    "    \n",
    "    if dtm1[o]=='politics':\n",
    "        actual.append(0)\n",
    "    elif dtm1[o]=='sports':\n",
    "        actual.append(1)\n",
    "    elif dtm1[o]=='natural':\n",
    "        actual.append(2)\n",
    "    elif dtm1[o]=='crime':\n",
    "        actual.append(3)\n",
    "    \n",
    "    if pol2>=spo2 and pol2>=nat2 and pol2>=cri2:\n",
    "        po=po+1\n",
    "        predicted.append(0)\n",
    "        if dtm1[o]=='politics':\n",
    "            poli=poli+1\n",
    "            #print('politics')\n",
    "    elif spo2>=pol2 and spo2>=nat2 and spo2>=cri2:\n",
    "        sp=sp+1\n",
    "        predicted.append(1)\n",
    "        if dtm1[o]=='sports':\n",
    "            spor=spor+1\n",
    "            #print('sports')\n",
    "    elif nat2>=spo2 and nat2>=pol2 and nat2>=cri2:\n",
    "        na=na+1\n",
    "        predicted.append(2)\n",
    "        if dtm1[o]=='natural':\n",
    "            natu=natu+1\n",
    "            #print('natural')\n",
    "    elif cri2>=spo2 and cri2>=nat2 and cri2>=pol2:\n",
    "        cr=cr+1\n",
    "        predicted.append(3)\n",
    "        if dtm1[o]=='crime':\n",
    "            crim=crim+1\n",
    "            #print('crime')\n",
    "    \n",
    "    #print(li)\n",
    "    while len(li)>0:\n",
    "        li.pop()\n",
    "    \n",
    "    c=c+float(pol2+spo2+nat2+cri2)\n",
    "    n=n+1\n",
    "    o=o+1\n",
    "\n",
    "\n",
    "\n",
    "#np.save(\"actual.npy\", np.array(actual))\n",
    "#np.save(\"SVMpredicted.npy\", np.array(predicted))\n",
    "\n",
    "print(actual)\n",
    "print(predicted)\n",
    "\n",
    "politics_count=0\n",
    "sports_count=0\n",
    "natural_count=0\n",
    "crime_count=0\n",
    "for i in range(0, len(testing)):\n",
    "    if dtm1[i]=='politics':\n",
    "        politics_count=politics_count+1\n",
    "    elif dtm1[i]=='sports':\n",
    "        sports_count=sports_count+1\n",
    "    elif dtm1[i]=='natural':\n",
    "        natural_count=natural_count+1\n",
    "    elif dtm1[i]=='crime':\n",
    "        crime_count=crime_count+1\n",
    "print('Politics Count: ' + str(politics_count))\n",
    "print('Sports Count: ' + str(sports_count))\n",
    "print('Natural Count: ' + str(natural_count))\n",
    "print('Crime Count: ' + str(crime_count))\n",
    "print()\n",
    "\n",
    "\n",
    "\n",
    "ac=poli+spor+crim+natu\n",
    "print('Politics Predicted: ' + str(po))\n",
    "print('Sports Predicted: ' + str(sp))\n",
    "print('Natural Predicted: ' + str(na))\n",
    "print('Crime Predicted: ' + str(cr))\n",
    "\n",
    "print()\n",
    "print('Politics Correct: ' + str(poli))\n",
    "print('Sports Correct: ' + str(spor))\n",
    "print('Natural Correct: ' + str(natu))\n",
    "print('Crime Correct: ' + str(crim))\n",
    "print()\n",
    "\n",
    "print('Test Data: ' + str(n))\n",
    "print('Accuracy: ' + str(ac/n))\n",
    "end = time.time()\n",
    "print('Run Time: ' +str(end - start))# -*- coding: utf-8 -*-\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.11977,
     "end_time": "2020-10-20T18:40:53.684876",
     "exception": false,
     "start_time": "2020-10-20T18:40:53.565106",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# SVM [18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-20T18:40:53.940632Z",
     "iopub.status.busy": "2020-10-20T18:40:53.935091Z",
     "iopub.status.idle": "2020-10-20T18:40:58.004951Z",
     "shell.execute_reply": "2020-10-20T18:40:58.006079Z"
    },
    "papermill": {
     "duration": 4.201528,
     "end_time": "2020-10-20T18:40:58.006265",
     "exception": false,
     "start_time": "2020-10-20T18:40:53.804737",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "120\n",
      "140\n",
      "160\n",
      "180\n",
      "[2, 0, 1, 3, 1, 2, 2, 2, 2, 1, 1, 1, 2, 1, 0, 2, 0, 0, 2, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 0, 0, 1, 2, 1, 0, 3, 1, 2, 1, 1, 1, 2, 1, 2, 1, 0, 1, 2, 0, 1, 1, 1, 1, 3, 0, 1, 2, 1, 2, 2, 0, 1, 3, 1, 0, 1, 0, 1, 2, 1, 0, 1, 0, 1, 2, 1, 2, 1, 0, 1, 2, 1, 2, 2, 2, 0, 0, 2, 2, 2, 2, 0, 2, 2, 1, 1, 1, 0, 1, 1, 2, 2, 2, 2, 1, 2, 2, 0, 2, 2, 3, 2, 2, 0, 0, 3, 2, 1, 3, 0, 3, 1, 2, 3, 0, 2, 0, 1, 0, 3, 2, 2, 1, 2, 1, 2, 1, 3, 0, 0, 2, 1, 0, 1, 2, 1, 1, 2, 0, 1, 3, 3, 2, 0, 0, 0, 3, 1, 2, 3, 2, 3, 1, 1, 2, 0, 2, 3, 1, 2, 3, 3, 2, 2, 1, 1, 1, 1, 2, 2, 2, 2, 1, 0, 2, 1, 1, 2, 1, 1, 1, 2, 1, 1, 2, 3, 2, 2, 1, 1]\n",
      "[2, 2, 1, 2, 1, 2, 2, 2, 2, 1, 1, 1, 2, 1, 0, 2, 2, 0, 2, 1, 1, 1, 2, 2, 2, 2, 1, 2, 2, 2, 2, 1, 2, 2, 0, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 0, 1, 2, 0, 2, 1, 1, 1, 1, 2, 1, 2, 1, 2, 2, 2, 1, 2, 1, 0, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 0, 1, 2, 1, 2, 2, 2, 2, 0, 2, 2, 2, 2, 0, 2, 2, 1, 1, 2, 2, 1, 1, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 1, 2, 2, 2, 1, 2, 2, 2, 2, 2, 1, 2, 1, 2, 2, 1, 2, 1, 2, 1, 2, 2, 2, 2, 1, 0, 1, 2, 2, 1, 2, 0, 1, 1, 2, 2, 0, 2, 0, 2, 1, 2, 2, 2, 2, 1, 1, 2, 0, 2, 0, 1, 2, 2, 2, 2, 2, 1, 1, 2, 1, 2, 2, 2, 2, 1, 0, 2, 2, 2, 2, 2, 1, 2, 2, 1, 2, 2, 2, 2, 2, 1, 1]\n",
      "Politics Count: 36\n",
      "Sports Count: 75\n",
      "Natural Count: 69\n",
      "Crime Count: 20\n",
      "\n",
      "Politics Predicted: 17\n",
      "Sports Predicted: 63\n",
      "Natural Predicted: 120\n",
      "Crime Predicted: 0\n",
      "\n",
      "Politics Correct: 16\n",
      "Sports Correct: 60\n",
      "Natural Correct: 69\n",
      "Crime Correct: 0\n",
      "\n",
      "Test Data: 200\n",
      "Accuracy: 0.725\n",
      "Run Time: 3.643584966659546\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def words(text): return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "WORDS = Counter(words(open('../input/twitterdataset2/big.txt').read()))\n",
    "\n",
    "def P(word, N=sum(WORDS.values())): \n",
    "    \"Probability of `word`.\"\n",
    "    return WORDS[word] / N\n",
    "\n",
    "def correction(word): \n",
    "    \"Most probable spelling correction for word.\"\n",
    "    return max(candidates(word), key=P)\n",
    "\n",
    "def candidates(word): \n",
    "    \"Generate possible spelling corrections for word.\"\n",
    "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "\n",
    "def known(words): \n",
    "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "    return set(w for w in words if w in WORDS)\n",
    "\n",
    "def edits1(word):\n",
    "    \"All edits that are one edit away from `word`.\"\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def edits2(word): \n",
    "    \"All edits that are two edits away from `word`.\"\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Naive Bayes\n",
    "from nltk.corpus import words as wsss\n",
    "import pandas as pd \n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from nltk.corpus import stopwords\n",
    "import nltk.classify.util\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from nltk.classify import SklearnClassifier\n",
    "from nltk.corpus import names\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import *\n",
    "\n",
    "wordss=[]\n",
    "for i in wsss.words():\n",
    "    wordss.append(i)\n",
    "\n",
    "start = time.time()\n",
    "num=0\n",
    "df = read_csv(\"../input/twitterdataset/traindata.csv\")\n",
    "df1 = df['tag']\n",
    "df2 = df['content']\n",
    "tok = WordPunctTokenizer()\n",
    "pat1 = r'@[A-Za-z0-9]+'\n",
    "pat2 = r'https?://[A-Za-z0-9./]+'\n",
    "combined_pat = r'|'.join((pat1, pat2))\n",
    "\n",
    "f= open(\"processed_dataset.csv\",\"w\")\n",
    "\n",
    "def clean(s):\n",
    "    def tweet_cleaner(text):\n",
    "        soup = BeautifulSoup(text, 'lxml')\n",
    "        souped = soup.get_text()\n",
    "        stripped = re.sub(combined_pat, '', souped)\n",
    "        try:\n",
    "            clean = stripped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
    "        except:\n",
    "            clean = stripped\n",
    "        letters_only = re.sub(\"[^a-zA-Z]\", \" \", clean)\n",
    "        lower_case = letters_only.lower()\n",
    "        words = tok.tokenize(lower_case)\n",
    "        return (\" \".join(words)).strip()\n",
    "    \n",
    "    stopWords = set(stopwords.words('english'))\n",
    "    ps = PorterStemmer()\n",
    "    li=[]\n",
    "    test_result = []\n",
    "    wordsFiltered = []\n",
    "    b=tweet_cleaner(s)\n",
    "    test_result.append(b)\n",
    "    words = word_tokenize(tweet_cleaner(s))\n",
    "    #print(words)\n",
    "    \n",
    "    \n",
    "    return words\n",
    "\n",
    "\n",
    "p=[]\n",
    "s=[]\n",
    "n=[]\n",
    "c=[]\n",
    "counterz = 0\n",
    "for t in range(0,len(df2)):\n",
    "    counterz=counterz+1\n",
    "    if df1[t]=='politics':\n",
    "        sen=df2[t]\n",
    "        f.write('0')\n",
    "        p=p+clean(sen)\n",
    "    elif df1[t]=='sports':\n",
    "        sen=df2[t]\n",
    "        f.write('1')\n",
    "        s=s+clean(sen)\n",
    "    elif df1[t]=='natural':\n",
    "        sen=df2[t]\n",
    "        f.write('2')\n",
    "        n=n+clean(sen)\n",
    "    elif df1[t]=='crime':\n",
    "        sen=df2[t]\n",
    "        f.write('3')\n",
    "        c=c+clean(sen)\n",
    "        \n",
    "f.close()\n",
    "        \n",
    "#print(p)\n",
    "#print(s)\n",
    "#print(n)\n",
    "#print(c)\n",
    "\n",
    "normal_vocab=[]\n",
    "def word_feats(words):\n",
    "    return dict([(words, True)]) \n",
    "\n",
    "\n",
    "for a in p:\n",
    "    if a in p and a in s and a in n and a in c:\n",
    "        normal_vocab=normal_vocab+list(a);\n",
    "        p.remove(a)\n",
    "        s.remove(a)\n",
    "        n.remove(a)\n",
    "        c.remove(a)\n",
    "    if a in p and a in s:\n",
    "        normal_vocab=normal_vocab+list(a);\n",
    "        p.remove(a)\n",
    "        s.remove(a)\n",
    "    if a in s and a in n:\n",
    "        normal_vocab=normal_vocab+list(a);\n",
    "        n.remove(a)\n",
    "        s.remove(a)\n",
    "    if a in n and a in c:\n",
    "        normal_vocab=normal_vocab+list(a);\n",
    "        n.remove(a)\n",
    "        c.remove(a)\n",
    "    if a in p and a in c:\n",
    "        normal_vocab=normal_vocab+list(a);\n",
    "        p.remove(a)\n",
    "        c.remove(a)\n",
    "    if a in s and a in c:\n",
    "        normal_vocab=normal_vocab+list(a);\n",
    "        c.remove(a)\n",
    "        s.remove(a)\n",
    "    if a in p and a in n:\n",
    "        normal_vocab=normal_vocab+list(a);\n",
    "        p.remove(a)\n",
    "        n.remove(a)\n",
    "    if a in p and a in s and a in n:\n",
    "        normal_vocab=normal_vocab+list(a);\n",
    "        p.remove(a)\n",
    "        s.remove(a)\n",
    "        n.remove(a)\n",
    "    if a in s and a in n and a in c:\n",
    "        normal_vocab=normal_vocab+list(a);\n",
    "        c.remove(a)\n",
    "        s.remove(a)\n",
    "        n.remove(a)\n",
    "    if a in p and a in n and a in c:\n",
    "        normal_vocab=normal_vocab+list(a);\n",
    "        p.remove(a)\n",
    "        c.remove(a)\n",
    "        n.remove(a)\n",
    "    if a in p and a in s and a in c:\n",
    "        normal_vocab=normal_vocab+list(a);\n",
    "        p.remove(a)\n",
    "        s.remove(a)\n",
    "        c.remove(a)\n",
    "\n",
    "        \n",
    "politics_vocab=p\n",
    "#print(politics_vocab)\n",
    "sports_vocab=s\n",
    "natural_vocab=n\n",
    "crime_vocab=c\n",
    " \n",
    "politics_features = [(word_feats(pol), 'pol') for pol in politics_vocab]\n",
    "sports_features = [(word_feats(spo), 'spo') for spo in sports_vocab]\n",
    "natural_features = [(word_feats(nat), 'nat') for nat in natural_vocab]\n",
    "crime_features = [(word_feats(cri), 'cri') for cri in crime_vocab]\n",
    "normal_features = [(word_feats(nor), 'nor') for nor in normal_vocab]\n",
    "train_set = politics_features + sports_features + natural_features+crime_features\n",
    "#print(train_set)\n",
    "\n",
    "#classifier = NaiveBayesClassifier.train(train_set)\n",
    "classifier = SklearnClassifier(LinearSVC(max_iter=20000), sparse=False).train(train_set) \n",
    "#classifier = SklearnClassifier(LinearSVC(tol=1e-6, max_iter=20000, class_weight='balanced', C=100, multi_class='ovr', random_state=0, loss='hinge'), sparse=False).train(train_set) \n",
    "#classifier = SklearnClassifier(LogisticRegression(), sparse=False).train(train_set)\n",
    "#classifier = SklearnClassifier(DecisionTreeClassifier(), sparse=False).train(train_set)\n",
    "#classifier = SklearnClassifier(RandomForestClassifier(n_estimators=5, max_depth=10, random_state=0, criterion=\"entropy\"), sparse=False).train(train_set) \n",
    "#classifier = SklearnClassifier(KNeighborsClassifier(), sparse=False).train(train_set) \n",
    "\n",
    "cols = ['text']\n",
    "dt = pd.read_csv(\"../input/twitterdataset/test.csv\",header=None, names=cols)\n",
    "dtm = read_csv(\"../input/twitterdataset/testmatch2.csv\")\n",
    "dtm1 = dtm['tag']\n",
    "dtm2 = dtm['content']\n",
    "#print(classifier)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Predict\n",
    "tok = WordPunctTokenizer()\n",
    "pat1 = r'@[A-Za-z0-9]+'\n",
    "pat2 = r'https?://[A-Za-z0-9./]+'\n",
    "combined_pat = r'|'.join((pat1, pat2))\n",
    "\n",
    "\n",
    "def tweet_cleaner(text):\n",
    "    soup = BeautifulSoup(text, 'lxml')\n",
    "    souped = soup.get_text()\n",
    "    stripped = re.sub(combined_pat, '', souped)\n",
    "    try:\n",
    "        clean = stripped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
    "    except:\n",
    "        clean = stripped\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", clean)\n",
    "    lower_case = letters_only.lower()\n",
    "    words = tok.tokenize(lower_case)\n",
    "    return (\" \".join(words)).strip()\n",
    "\n",
    "\n",
    "testing = dt.text[:]\n",
    "stopWords = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "li=[]\n",
    "test_result = []\n",
    "wordsFiltered = []\n",
    "\n",
    "c=0\n",
    "n=0\n",
    "po=0\n",
    "sp=0\n",
    "cr=0\n",
    "na=0\n",
    "poli=0\n",
    "spor=0\n",
    "natu=0\n",
    "crim=0\n",
    "o=0\n",
    "\n",
    "counterz=0\n",
    "\n",
    "import numpy as np\n",
    "actual = []\n",
    "predicted = []\n",
    "\n",
    "for t in testing:\n",
    "    if counterz%20==0:\n",
    "        print(counterz)\n",
    "    counterz=counterz+1\n",
    "    pol=0\n",
    "    cri=0\n",
    "    nat=0\n",
    "    spo=0\n",
    "    pol2=0\n",
    "    cri2=0\n",
    "    nat2=0\n",
    "    spo2=0\n",
    "    #print(t)\n",
    "    \n",
    "    #print(wd)\n",
    "    b=tweet_cleaner(t)\n",
    "    #print(b)\n",
    "    wd = word_tokenize(b)\n",
    "    test_result.append(b)\n",
    "    words = word_tokenize(tweet_cleaner(t))\n",
    "    \n",
    "    sentence = words\n",
    "    #print(sentence)\n",
    "    #sentence = sentence.lower()\n",
    "    #words = sentence.split(' ')\n",
    "    w=sentence\n",
    "    wordsFiltered=[]\n",
    "    for word in w:\n",
    "        classResult = classifier.classify( word_feats(word))\n",
    "        if classResult == 'pol':\n",
    "            pol = pol + 1\n",
    "        if classResult == 'spo':\n",
    "            spo = spo + 1\n",
    "        if classResult == 'nat':\n",
    "            nat = nat + 1\n",
    "        if classResult == 'cri':\n",
    "            cri = cri + 1\n",
    "    #print(neg)\n",
    "    #print(pos)\n",
    "    #print(li)\n",
    "    #print(len(words))\n",
    "\n",
    "    pol2=float(pol)/(len(wd))\n",
    "    spo2=float(spo)/(len(wd))\n",
    "    nat2=float(nat)/(len(wd))\n",
    "    cri2=float(cri)/(len(wd))\n",
    "\n",
    "    #print('Politics: ' + str(pol2))\n",
    "    #print('Sports: ' + str(spo2))\n",
    "    #print('Natural: ' + str(nat2))\n",
    "    #print('Crime: ' + str(cri2))\n",
    "    \n",
    "    \n",
    "    if dtm1[o]=='politics':\n",
    "        actual.append(0)\n",
    "    elif dtm1[o]=='sports':\n",
    "        actual.append(1)\n",
    "    elif dtm1[o]=='natural':\n",
    "        actual.append(2)\n",
    "    elif dtm1[o]=='crime':\n",
    "        actual.append(3)\n",
    "    \n",
    "    if pol2>=spo2 and pol2>=nat2 and pol2>=cri2:\n",
    "        po=po+1\n",
    "        predicted.append(0)\n",
    "        if dtm1[o]=='politics':\n",
    "            poli=poli+1\n",
    "            #print('politics')\n",
    "    elif spo2>=pol2 and spo2>=nat2 and spo2>=cri2:\n",
    "        sp=sp+1\n",
    "        predicted.append(1)\n",
    "        if dtm1[o]=='sports':\n",
    "            spor=spor+1\n",
    "            #print('sports')\n",
    "    elif nat2>=spo2 and nat2>=pol2 and nat2>=cri2:\n",
    "        na=na+1\n",
    "        predicted.append(2)\n",
    "        if dtm1[o]=='natural':\n",
    "            natu=natu+1\n",
    "            #print('natural')\n",
    "    elif cri2>=spo2 and cri2>=nat2 and cri2>=pol2:\n",
    "        cr=cr+1\n",
    "        predicted.append(3)\n",
    "        if dtm1[o]=='crime':\n",
    "            crim=crim+1\n",
    "            #print('crime')\n",
    "    \n",
    "    #print(li)\n",
    "    while len(li)>0:\n",
    "        li.pop()\n",
    "    \n",
    "    c=c+float(pol2+spo2+nat2+cri2)\n",
    "    n=n+1\n",
    "    o=o+1\n",
    "\n",
    "\n",
    "\n",
    "#np.save(\"actual.npy\", np.array(actual))\n",
    "#np.save(\"SVMpredicted.npy\", np.array(predicted))\n",
    "\n",
    "print(actual)\n",
    "print(predicted)\n",
    "\n",
    "politics_count=0\n",
    "sports_count=0\n",
    "natural_count=0\n",
    "crime_count=0\n",
    "for i in range(0, len(testing)):\n",
    "    if dtm1[i]=='politics':\n",
    "        politics_count=politics_count+1\n",
    "    elif dtm1[i]=='sports':\n",
    "        sports_count=sports_count+1\n",
    "    elif dtm1[i]=='natural':\n",
    "        natural_count=natural_count+1\n",
    "    elif dtm1[i]=='crime':\n",
    "        crime_count=crime_count+1\n",
    "print('Politics Count: ' + str(politics_count))\n",
    "print('Sports Count: ' + str(sports_count))\n",
    "print('Natural Count: ' + str(natural_count))\n",
    "print('Crime Count: ' + str(crime_count))\n",
    "print()\n",
    "\n",
    "\n",
    "\n",
    "ac=poli+spor+crim+natu\n",
    "print('Politics Predicted: ' + str(po))\n",
    "print('Sports Predicted: ' + str(sp))\n",
    "print('Natural Predicted: ' + str(na))\n",
    "print('Crime Predicted: ' + str(cr))\n",
    "\n",
    "print()\n",
    "print('Politics Correct: ' + str(poli))\n",
    "print('Sports Correct: ' + str(spor))\n",
    "print('Natural Correct: ' + str(natu))\n",
    "print('Crime Correct: ' + str(crim))\n",
    "print()\n",
    "\n",
    "print('Test Data: ' + str(n))\n",
    "print('Accuracy: ' + str(ac/n))\n",
    "end = time.time()\n",
    "print('Run Time: ' +str(end - start))# -*- coding: utf-8 -*-\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 1603.458534,
   "end_time": "2020-10-20T18:40:59.231521",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-10-20T18:14:15.772987",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
